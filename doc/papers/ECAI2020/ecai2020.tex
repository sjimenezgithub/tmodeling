\documentclass{ecai}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{array}

\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}    % precondition
\newcommand{\eff}{\mathsf{eff}}    % effect
\newcommand{\cond}{\mathsf{cond}}  % condition
\newcommand{\dur}{\mathsf{dur}}    % duration
\newcommand{\iscond}{\mathsf{is\_cond}}    % is_cond
\newcommand{\iseff}{\mathsf{is\_eff}}    % is_eff
\newcommand{\obs}{\mathsf{obs}}    % observation
\newcommand{\start}{\mathsf{start}}% start
\newcommand{\en}{\mathsf{end}}     % end
\newcommand{\til}{\mathsf{til}}    % TIL
\newcommand{\supp}{\mathsf{sup}}   % sup
\newcommand{\tim}{\mathsf{time}}   % time
\newcommand{\reqs}{\mathsf{req\_{start}}} % req_start
\newcommand{\reqe}{\mathsf{req\_{end}}}   % req_end
\newcommand{\ini}{\mathsf{init}}   % init
\newcommand{\goal}{\mathsf{goal}}  % goal


%TamaÃ±o 7+1 pages

\begin{document}
\title{One-Shot Learning of Concurrent Durative [o Temporal??] Actions Models [via Constraint Programming???]}
\author{Antonio Garrido \and Sergio Jim\'enez}
 
%\author{Name1 Surname1 \and Name2 Surname2 \and Name3 Surname3\institute{University, Country, email: somename@university.edu} }
 
\maketitle

\begin{abstract}
  We present a {\em Constraint Programming} (CP) formulation for learning models of {\em durative actions} from the observation of a single plan execution. Inspired by the CSP approach to {\em temporal planning}, our CP formulation models {\em time-stamps} for actions, {\em causal-link} relationships, {\em threats} and effect {\em interferences} and evidences the connection between the tasks of {\em plan synthesis}, {\em plan validation} and {\em action model learning}. The CP formulation is solver-independent so off-the-shelf CSP solvers can be used for the resolution of any of these tasks. The performance of our CP formulation is evaluated learning and validating action models of several temporal domains specified in PDDL2.1. The paper also shows that the CP formulation is flexible to accommodate a different range of expressiveness, subsuming the PDDL2.1 temporal semantics.
\end{abstract}



\section{INTRODUCTION}

{\em Temporal planning} is an expressive planning model that relaxes the assumption of instantaneous actions of {\em classical planning}~\cite{geffner2013concise}. Actions in temporal planning are called {\em durative}, because each action has an associated duration and hence, the conditions/effects of an action may hold/happen at different times~\cite{fox2003pddl2}. This means that {\em durative actions} can be executed in parallel and overlap in several different ways~\cite{cushing2007temporal}, and that valid solutions for temporal planning instances must indicate the precise time-stamp when durative actions start and end~\cite{howey2004val}.

Despite the potential of state-of-the-art planners, their application to real world problems is still somewhat limited mainly because of the difficulty of specifying correct and complete (temporal) planning models~\cite{kambhampati2007model}. The more expressive the planning model, the more evident becomes this knowledge acquisition bottleneck that jeopardizes the usability of AI planning technology. There is however a growing interest in the planning community for the machine learning of action models~\cite{kuvcera2018louga,MouraoZPS12,yang2007learning,zhuo2013action} with a wide range of different approaches for learning classical action models from sequential plans~\cite{arora2018review}. Since pioneering learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or states~\cite{MouraoZPS12,zhuo2013action}, from null state information~\cite{cresswell2013}, or from incomplete domain models~\cite{ZhuoK17,ZhuoNK13}.

As far as we know this work is the first approach for learning action models for temporal planning. While learning an action model for classical planning means computing the actions' conditions and effects that are consistent with the input observations, learning temporal action models requires additionally: i) identifying how conditions and effects are temporally distributed within the action, and ii) estimate the action duration. Further, most of the cited approaches for model learning are purely inductive and require large input datasets, e.g. hundreds of plan observations, to compute statistically significant models and focus on learning models from sequential plans for classical planning.

With the aim of understanding better the connection between the learning of durative action models, {\em temporal planning}  and the validation of temporal plans, this paper follows a radically different approach and studies the singular learning scenario where just the observation of a single plan execution (one-shot) is available. The contributions of this work are two-fold:

\begin{enumerate}  
\item We show how to learn action models from observations of plans with overlapping actions. This feature makes our approach appealing for learning action models in multi-agent environments~\cite{furelos2018carpool}. 
\item A CP formulation that connnects the {\em learning} of planning action models with the {\em synthesis} and the {\em validation} of plans. The paper shows that of-the-shelf CSP solvers can be used for any of these tasks. Further, we show that the plan validation ability of our CP formulation is beyond the functionality of VAL (the standard plan validation tool~\cite{howey2004val}) since it can address plan validation of partial, or even empty, action models and with partially observed plan traces (VAL requires both a full plan and a full action model for plan validation). 
\end{enumerate}



\section{BACKGROUND}

This section formalizes the {\em temporal planning} and {\em Constraint Satisfaction} models that we follow in this work.

\subsection{Temporal Planning}
\label{sec:temporalplanning}

We assume that {\em states} are factored into a set $F$ of Boolean variables. A state $s$ is a time-stamped assignment of values to all the variables in $F$. A {\em temporal planning problem} is a tuple $P=\tup{F,I,G,A}$ where the {\em initial state} $I$ is a fully observed state (i.e. $|I|=|F|$) that is time-stamped with $t=0$; $G \subseteq F$ is a conjunction of {\em goal conditions} over the variables in $F$ that defines the set of goal states; and $A$ represents the set of {\em durative actions}. A {\em durative action} has an associated duration and may have conditions/effects on $F$ at different times~\cite{garrido2009constraint,vidal2006branching}. In this work we assume that durative actions in $A$ are fully grounded from {\em action schemes} (aka {\em operators}) to compactly represent temporal planning problems. 

PDDL2.1 is the input language for the temporal track of the International Planning Competition (IPC)~\cite{fox2003pddl2,ghallab2004automated}. According to PDDL2.1, a durative action $a\in A$ is defined with the following elements:

\begin{enumerate}
\item $\dur(a)$, a positive value indicating the {\em duration} of the action.

\item $\cond_s(a), \cond_o(a), \cond_e(a)$ representing the three types of action {\em conditions}. Unlike the \emph{pre}conditions of classical actions, action conditions in PDDL2.1 must hold: before $a$ is executed ({\em at start}), during the entire execution of $a$ ({\em over all}) or when $a$ finishes ({\em at end}), respectively. 

\item $\eff_s(a)$ and $\eff_e(a)$ represent the two types of action effects. In PDDL2.1, effects can happen {\em at start} or {\em at end} of action $a$ respectively, and can be either positive or negative (i.e. asserting or retracting variables). 

\end{enumerate}

PDDL2.1 is a restricted temporal planning model since the semantics of a PDDL2.1 durative action $a$ can always be defined in terms of just two discrete events, $\start(a)$ and $\en(a)=\start(a)+\dur(a)$. This means that if $a$ starts on state $s$ with time-stamp $\start(a)$, then $\cond_s(a)$ must hold in $s$. Ending action $a$ in state $s'$, with time-stamp $\en(a)$, means $\cond_e(a)$ must hold in $s'$. {\em Over all} conditions must hold at any state between $s$ and $s'$ or, in other words, throughout the closed interval $[\start(a)..\en(a)]$. Likewise, {\em at start} and {\em at end} effects are instantaneously applied at states $s$ and $s'$, respectively (continuous effects are not considered in this work). 

A {\em temporal plan} is a set of pairs $\pi=\{(a_1,t_1),(a_2,t_2)\ldots (a_n,t_n)\}$. Each pair $(a_i,t_i)$ contains a durative action $a_i$ and a time-stamp $t_i=\start(a_i)$. The execution of a temporal plan starting from a given initial state $I$ induces a state sequence formed by the union of all states $\{s_{t_i}, s_{t_i+\dur(a_i)}\}$, where there exists an initial state $s_{0}=I$, and a state $s_{end}$ that is the last state induced by the execution of the plan. Note then that sequential plans can be expressed as temporal plans but not the opposite. A {\em solution} to a given temporal planning problem $P$ is a temporal plan $\pi$ such that its execution, starting from the corresponding initial state, eventually reaches a state that meets the goal conditions, $G\subseteq s_{end}$.

PDDL2.2 is an extension of the language PDDL2.1 that includes the notion of {\em Timed Initial Literal}~\cite{hoffmann2005}, denoted as $\til(f,t)$, and representing that a variable $f\in F$ becomes true (or false) at a certain time $t>0$, independently of the actions in the plan~\cite{Edelkamp04}. Traditionally, TILs are useful to model {\em exogenous happenings}; for instance, a time window when a warehouse is open in a logistics scenario, $\til(open,8)$ and $\til($\textit{not-}$open,20)$.

\subsection{Constraint Satisfaction}
A {\em Constraint Satisfaction Problem} (CSP) is a tuple $\tup{X,D,C}$, where $X$ is a set of finite variables, $D$ represents the finite domain for each of these variables and $C$ is a set of constraints among the variables in $X$ that bound their possible values in $D$.

A solution to a CSP as an assignment of values to all variables in $X$ that satisfy all the constraints in $C$, that is, those values are {\em consistent} with all the constraints.

Given a CSP there may be many different solutions to that problem. A {\em cost-function} can be defined to specify user preferences about the space of possible solutions. Given a CSP and cost-function, then an {\em optimal solution} is a total assignment of the variables that is consistent with the constraints of the CSP and minimizes the value of the input cost-function.




\section{One-Shot learning of temporal action models}
\label{section:learningTemporalModels}

This section formalizes the learning task addressed in the paper. The aim of this learning task is to specify the set of {\em conditions} and {\em effects} that correspond to the action schemes of a given temporal planning domain represented in the PDDL2.1 language. As is common in previous approaches for learning planning action models we assume that the {\em headers} (the name and parameters) of each action schemes are known. 

Figure~\ref{fig:exampleactions2} shows an example of two schemes for PDDL2.1 durative actions taken from the {\em driverlog} domain. The schema \texttt{board-truck} has a fixed duration while the duration of \texttt{drive-truck} depends on the driving time associated to the two given locations.

\begin{figure}
%\begin{tabular}{p{\textwidth}}
\begin{scriptsize}    
\begin{verbatim}
(:durative-action board-truck
 :parameters (?d - driver ?t - truck ?l - location)
 :duration (= ?duration 2)
 :condition (and (at start (at ?d ?l)) 
                 (at start (empty ?t))
                 (over all (at ?t ?l)))
 :effect (and (at start (not (at ?d ?l))) 
              (at start (not (empty ?t)))
              (at end (driving ?d ?t))))

(:durative-action drive-truck
 :parameters (?t - truck ?l1 - location ?l2 - location 
              ?d - driver)
 :duration (= ?duration (driving-time ?l1 ?l2))
 :condition (and (at start (at ?t ?l1)) 
                 (at start (link ?l1 ?l2))
                 (over all (driving ?d ?t)))
 :effect (and (at start (not (at ?t ?l1))) 
              (at end (at ?t ?l2))))
\end{verbatim}
\end{scriptsize}    
%\end{tabular}
\caption{Two action schemes for PDDL2.1 durative actions.}
\label{fig:exampleactions2}
\end{figure}

\subsection{The hypothesis space}
\label{sec:action-space}



Like in PDDL, we assume that the set $F$ of state variables is given by the instantiation of a given set of predicates $\Psi$. We denote as ${\mathcal I}_{\xi,\Psi}$ the {\em vocabulary} (set of symbols) that can appear in the conditions and effects of a given durative action schema $\xi$, with parameters $pars(\xi)$. This set is formally defined as the FOL interpretations of predicates $\Psi$, over the action parameters $pars(\xi)$. 

For a durative action schema $\xi$, the size of its space of possible action models is then $\mathcal{D}\times 2^{5\times|{\mathcal I}_{\xi,\Psi}}|$ where $\mathcal{D}$ is the number of different possible durations for any action shaped by the $\xi$ schema. Note that this space is significantly larger than for learning classical STRIPS actions~\cite{yang2007learning}, where this number is bound to $2^{2\times|{\mathcal I}_{\xi,\Psi}}|$ forcing that negative effects must also be preconditions of the same action and cannot be positive effects of that action.

Provided the vocabulary, then the {\em conditions} and {\em effects} of a given durative action schema can be compactly coded by 5 bit-vectors, each of length $|{\mathcal I}_{\xi,\Psi}|$. A 0-bit in the vector represents that the corresponding {\em condition}/{\em effect} is not part of the schema while a 1-bit represents that is part of the schema. This also means that the {\em Hamming distance} can be used straightforward as a sintactic similarity metric for durative schemes. For instance, we can use the {\em Hamming distance} to compare a learned action model with respect to a given reference model that serves as baseline. In this case, the number of wrong 1-bits in the learned schema provide us a measure of the {\em incorrectness} of the learned model (number of {\em conditions} and {\em effects} that should not be in the learned model) and the number of wrong 0-bits in the learned schema provide us a measure of the {\em incompleteness} of that model (number of {\em conditions} and {\em effects} that are missing in the learned model). 

Available prior knowledge can be used to bound this vocabulary for certain action schemes. For instance in a given domain we may known in advance several preconditions and/effects of a particular action. In this case the value of the corresponding bits is a priori specified. The remaining bits are then regular Boolean Variables whose value will be specify solving a CSP.










\subsection{One-shot learning of concurrent action models}
\label{subsec:ones}

We define the task of the {\em one-shot learning of temporal action models} as a tuple $\mathcal{L}=\tup{F,I,G,A?,O,C}$, where:

\begin{itemize}
	
\item $\tup{F,I,G,A?}$ is a temporal planning problem where actions in $A?$ are partially specified: the exact conditions/effects, their temporal annotation and/or the duration of actions are unknown. In the worst case, we only know the vocabulary of the symbols that can appear in the conditions/effects of the actions, which means having only the set of candidate conditions/effects. 

\item $O$ is the set of observations over a plan execution. Al least it contains a full observation of the initial state (time-stamped with $t=0$) and a final state observation that equals the goals $G$ of the temporal planning problem (time-stamped with $t_{end}$, the makespan of the observed plan). Additionally, it can contain time-stamped observations of traversed intermediate partial states\footnote{In this work, not all variables can be observed at any time; that is, we deal with partial observations (e.g. just a subset of variables is observable by associated sensors). But the observations are noiseless, which means that if a value is observed, that is the actual value of that variable.} and the times when actions start and/or end their execution. For instance, a partial state can be given by the set of observations $\tup{\obs(f_1,t_1),\obs(f_2,t_2)\ldots \obs(f_n,t_n)}$, where each $\obs(f_i,t_i)$ denotes the value observed for fact $f_i \in F$ at time $t_i$. To simulate the observations of the execution of actions in the plan, we can observe $\tup{\obs(is\_start(a_i),t_1),\obs(is\_start(a_j),t_2)\ldots \obs(is\_end(a_k),t_n)}$, representing the fact that action $a_i$ starts at $t_1$, $a_j$ starts at $t_2$, and $a_k$ ends at $t_n$. Figure~\ref{fig:exampleObservations} shows an example of the observation of a plan execution that is taken from the {\em driverlog} domain.

\begin{figure}
\begin{tabular}{p{5cm}}
\begin{scriptsize}    
\begin{verbatim}
??
\end{verbatim}
\end{scriptsize}    
\end{tabular}
\caption{Example of the observation of a plan execution.}
\label{fig:exampleObservations}
\end{figure}


\item $C$ is a set of {\em mutex-constraints} that reflects domain-specific expert knowledge. These constraints allow us to deduce new observations to prune inconsistent action models because of mutex (mutual exclusion) information. Figure~\ref{fig:example-statecs} show an example of a set of state-constraints for the {\em driverlog} domain. 
\end{itemize}


CREO QUE HABRIA QUE PONER UN EJEMPLO DE LO QUE SON LAS CONDICIONES/EFECTOS CANDIDATOS, POR EJEMPLO A PARTIR DE LAS ACCIONES DE LA FIGURA 1!!! Y LUEGO A CONTINUACION PONER EL EJEMPLO DE LAS STATE-CONSTRAINTS PARA DICHO EJEMPLO QUE INCLUYE AT, DRIVING Y EMPTY


\begin{figure}
  \begin{scriptsize}
  %\begin{tabular}{l}
Habria que poner algo en plan $\forall d_1 - driver, y_1,y_2 - location: ...$ \\
$\forall x_1,y_1,y_2: \neg at(x_1,y_1)\vee\neg at(x_1,y_2), \neq (y_1,y_2)$\\
$\forall x_1,y_1,y_2: \neg in(x_1,y_1)\vee\neg in(x_1,y_2), \neq (y_1,y_2)$  NO EXPLICADA PORQUE in ES PARA PAQUETE! \\
$\forall x_1,y_1,y_2: \neg driving(x_1,y_1)\vee\neg driving(x_1,y_2), \neq (y_1,y_2)$ NOMBRAR MEJOR PARA QUE SE VEA QUE FIJAMOS EL DRIVER\\
$\forall x_1,y_1,y_2: \neg driving(y_1,x_1)\vee\neg driving(y_2,x_1), \neq (y_1,y_2)$  NOMBRAR MEJOR LOS PARAMS PARA QUE SE VEA QUE AHORA FIJAMOS EL TRUCK?? \\
$\forall x_1,y_1,y_2: \neg at(x_1,y_1)\vee\neg driving(x_1,y_2)$\\
%$\forall x_1,y_1,y_2: \neg in(x_1,y_1)\vee\neg driving(x_1,y_2)$\\ MAL: in ES PARA PAQUETE NO PARA DRIVER!!! QUITAR!!!
$\forall x_1,y_1,y_2: \neg at(x_1,y_1)\vee\neg in(x_1,y_2)$ QUITAR PORQUE ES PARA PAQUETE! \\
$\forall x_1,y_1: \neg empty(x_1)\vee\neg driving(y_1,x_1)$\\
%$\forall x_1,y_1: \neg empty(x_1)\vee\neg driving(y_1,x_1)$\\ ES IDENTICA A LA DE ARRIBA. QUITADA!!!
$\forall x_1: \neg link(x_1,x_1)$ [REALMENTE ESTO NO SIRVE COMO MUTEX SINO PARA LA INSTANCIACION] \\
$\forall x_1: \neg path(x_1,x_1)$\\
  %\end{tabular}
\end{scriptsize}      
\caption{Examples of state-constraints for the {\em driverlog} domain. HACE FALTA PONERLOS TODOS CUANDO NO SE HAN DEFINIDO TODOS EN EL EJEMPLO??? YO PONDRIA NOMBRES MAS SIGNIFICATIVOS A LOS PARAMS}
\label{fig:example-statecs}
\end{figure}


A {\em solution} for the learning task $\mathcal{L}$ is a fully specified model of durative actions $\mathcal{A}$ such that the conditions, effects and duration of its actions are: i) completely specified, i.e. there is no uncertainty about them; and ii) 
{consistent} with $\mathcal{L}=\tup{F,I,G,A?,O,C}$. [NO SE SI LO DE consistent SE ENTIENDE BIEN AQUI??] Intuitively, we can build on top of the actions in $\mathcal{A}$ a valid plan whose execution starts in $I$, produces the observations in $O$, satisfies all constraints in $C$, and reaches a final state that satisfies $G$. 



\section{Learning action models as constraint satisfaction}
\label{subsec:CPformulation}
Given the one-shot learning task $\mathcal{L}$, as defined in subsection~\ref{subsec:ones}, we automatically create a CSP whose solution induces an action model that solves $\mathcal{L}$. Our CP formulation is solver-independent 
%(this means that any off-the-shelf CSP solver that supports the expressiveness of our CP formulation can be used) 
and it is inspired by previous work on temporal planning as CP~\cite{garrido2009constraint,vidal2006branching}\footnote{Contrarily to those works, we now address the inverse task: rather than deducing a plan from a fully known temporal action model, we want to learn a temporal action model from a partial action model and a set of observations.} .


\subsection{The CSP variables}

For each action $a$ in $A?$ and candidate condition/effect $f$ of $a$, we create the variables described in Table~\ref{table:variables}. For simplicity, we model time in $\mathbb{Z}^+$ and bound all maximum times to the makespan $t_{end}$ observed in $O$. If the observation of $t_{end}$ is unavailable, we consider a long enough domain for time.

\begin{table}
\begin{center}
\caption{The CSP variables, domains and semantics.}
%\setlength\extrarowheight{2pt}  
\begin{scriptsize}
\begin{tabular}{lllp{4.4cm}}
\hline	
{\bf ID} & {\bf Variable} & {\bf Domain} & {\bf Description} \\
\hline
X1 &$\start(a)$ & $[0..t_{end}]$ & Start time of $a$ (observed or derived value) \\
X2 &$\en(a)$ & $[0..t_{end}]$ & End time of $a$ (observed or derived value) \\
X3 &$\dur(a)$ & $[0..t_{end}]$ & Duration of $a$ \\

X4 &$\iscond(f,a)$ & \textit{\{0,1\}} & 1 if $f$ is a condition of $a$; 0 otherwise \\
X5 &$\iseff(f,a)$ & \textit{\{0,1\}} & 1 if $f$ is an effect of $a$; 0 otherwise \\

X6.1 &$\reqs(f,a)$, & $[0..t_{end}]$ & Interval when action $a$ requires $f$\\ 
X6.2 &$\reqe(f,a)$  &  & \\

X7 &$\supp(f,a)$ & $\{b_i\} \cup \emptyset $&  Supporters for causal link $\tup{b_i,f,a}$ \\ %where $b_i$ is an action in the action space \\
%There is a causal link $\tup{b_i,f,a}$ s.t.\\
%&& $b_i$ is an action in the action space. 
X8 &$\tim(f,a)$ & $[0..t_{end}]$ & Time when the effect $f$ of $a$ happens\\

\end{tabular}
\end{scriptsize}
\label{table:variables}
\end{center}
\end{table}

The three first variables are self-explanatory and their value is either observed in $O$ or derived by the expression $\en(a)=\start(a)+\dur(a)$.
Four and five are decision variables (for readability in comparisons, 1=\textit{true} and 0=\textit{false}): $\iscond(f,a)$ models whether $f$ is a condition of $a$, whereas $\iseff(f,a)$ models whether $f$ is an effect of $a$. The two sixth variables model the $[\reqs(f,a)..\reqe(f,a)]$ interval throughout condition $f$ must hold, provided $\iscond(f,a)$=\textit{true}.
The seventh variable, $\supp(f,a)$, represents the causal link relationship and models the actions $b_i$ that can support $f$ of $a$. If $f$ is not a condition of $a$ ($\iscond(f,a)$=\textit{false}) then $\supp(f,a)$=$\emptyset$, thus representing an empty supporter.
The eighth variable, $\tim(f,a)$, models when effect $f$ happens in $a$, provided $\iseff(f,a)$=\textit{true}.

Our formulation accommodates a level of expressiveness beyond PDDL2.1 and allows conditions/effects to be at any time, even outside the execution of the action. For example, we allow a condition $f$ to hold in $\start(a)\pm$2: $\reqs(f,a)=\start(a)-2$ and $\reqe(f,a)=\start(a)+2$. An effect $f$ might also happen after the action ends e.g., $\tim(f,a)=\en(a)+2$.


Besides the actions of the given planning problem, we create two dummy actions:

\begin{itemize}
	
\item $\ini$, which represents the {\em initial state} ($\start(\ini)=0$ and $\dur(\ini)=0$). This dummy action has no conditions so it has no associated variables $\iscond, \reqs, \reqe$ and $\supp$. It has as many $\iseff(f_i,\ini)$=\textit{true} and $\tim(f_i,\ini)=0$ as $f_i$ in $I$.

\item $\goal$, which represents the {\em goal conditions} ($\start(\goal)=t_{end}$ and $\dur(\goal)=0$). This dummy action has no effects so it has no $\iseff$ and $\tim$ variables. It has as many $\iscond(f_i,a)$=\textit{true}, $\supp(f_i,\goal)\neq \emptyset$ and $\reqs(f_i,\goal)=\reqe(f_i,\goal)=t_{end}$ as $f_i$ in $G$. 
\end{itemize}  

This formulation can also model both TILs and {\em observations}. On the one hand $\til(f,t)$ are modeled as the dummy action ($\start(\til(f,t))=t$ and $\dur(\til(f,t))=0$) with no conditions and the single effect $f$ that happens at time $t$ ($\iseff(f,\til(f,t))$=\textit{true} and $\tim(f,\til(f,t))=t$). On the other hand, $\obs(f,t)$ is modeled as another dummy action ($\start(\obs(f,t))=t$ and $\dur(\obs(f,t))=0$) with only one condition $f$, which is the value observed for fact $f$ ($\iscond(f,\obs(f,t))$=\textit{true}, $\supp(f,\obs(f,t))\neq \emptyset$ and $\reqs(f,\obs(f,t))=\reqe(f,\obs(f,t))=t$), and no effects at all. 
As can be seen, $\til$ is analogous to $\ini$, as they both represent information that is given at a particular time, but externally to the execution of the plan. Alternatively, $\obs$ is analogous to $\goal$, as they both represent conditions that must be satisfied in the execution of the plan at a particular time.



\subsection{The CSP constraints}
\label{section:CSPconstraints}


Table~\ref{table:constraints} shows the constraints defined among the CSP variables of Table~\ref{table:variables}. The first two constraints are explicit enough. The third constraint is a double implication that means that when $\iscond(f,a)$=\textit{false} it will require no supporter (alternatively, $f$ in $a$ needs a valid supporter only when $\iscond(f,a)$=\textit{true}). Constraint four forces to have valid values for the $\reqs$ and $\reqe$ variables, i.e. the interval condition.
The fifth constraint models the causal link $\tup{b,f,a}$. Intuitively, the time when $b$ supports $f$ must be before $a$ requires $f$. Note that in this causal link, $\tim(f,b) < \reqs(f,a)$ and not $\leq$ because, like in PDDL2.1~\cite{fox2003pddl2}, our temporal planning model assumes an $\epsilon > 0$. The value of $\epsilon$ denotes a small tolerance that implies no collision between the time when an effect $f$ is supported and when it is required. When time is modeled in 
%$\mathbb{R}^+$, epsilon is usually 0.001 but when it is modeled in 
$\mathbb{Z}^+$, $\epsilon=1$ and $\leq$ becomes $<$.
Given a causal link $\tup{b,f,a}$, constraint six avoids the threat of action $c$, which deletes $f$. It is solved via {\em promotion} or {\em demotion}~\cite{ghallab2004automated}, which means bringing $\tim($\textit{not-}$f,c)$ backward or forward, respectively, in time.
The seventh constraint avoids action $a$ from being a supporter of $f$ when $\iseff(f,a)$=\textit{false}.
Constraint eight models the fact that the same action requires and deletes $f$; then the effect cannot happen before the condition. 
Note the $\geq$ inequality here: if one condition and one effect of the same action happen at the same time, the underlying semantics in planning considers the condition is checked instantly before the effect~\cite{fox2003pddl2}.
The ninth constraint solves the fact that two arbitrary actions have contradictory effects.
These nine constraints apply to any type of action, including the dummy actions ($\ini$, $\goal$, $\til$ and $\obs$).
The tenth constraint, however, only applies to non-dummy actions and forces any action to have at least one condition and one effect.
We include this constraint to make the learning task more rational, as an action without conditions can be arbitrarily annotated at many times. Alternatively, an action without effects is unnecessary in any plan.

As can be noticed, some conditions of Table~\ref{table:constraints} are redundant. See for instance constraints five and six: $\supp(f,a)=b$ means obligatorily $\iseff(f,b)=$ \textit{true}. 
%if $\supp(f,a)=b$ then $\iseff(f,b)$=\textit{true} obligatorily. 
We include them here to define an homogeneous formulation but they are not included in our implementation.
For simplicity, the value of some unnecessary variables is not bounded in the table. For instance, if $\iscond(f,a)$=\textit{false}, variables $\reqs(f,a)$ and $\reqe(f,a)$ become useless.
%and their values are indifferent.


\begin{table*}
\begin{center}
\caption{The CSP constraints and semantics.}	
%	\setlength\extrarowheight{2pt}    
\begin{scriptsize}
\begin{tabular}{lp{10.4cm}p{6.6cm}} %7.9cm
%\begin{tabular}{ll}
\hline
{\bf ID}&{\bf Constraint}&{\bf Description}\\\hline
			
C1& $\en(a)=\start(a)+\dur(a)$ & Relationship among start, end and duration of $a$ \\

C2& $\en(a) \leq \start(\goal)$ & Always $\goal$ is the last action of the plan \\

C3& \textbf{iff} ($\iscond(f,a)$=\textit{false}) \textbf{then} $\supp(f,a) = \emptyset$ & $f$ is not a condition of $a \iff $ the supporter of $f$ in $a$ is $\emptyset$ \\

C4& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{then} $\reqs(f,a) \leq \reqe(f,a)$ & [$\reqs(f,a)..\reqe(f,a)$] is a valid interval\\

C5& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$))  & Modeling the causal link $\tup{b,f,a}$: supporting $f$ before it is \\
&\hspace{0.5cm}\textbf{then} $\tim(f,b) < \reqs(f,a)$ & required (obviously $b \neq \emptyset$) \\

C6& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$) & Solving threat of $c$ to causal link $\tup{b,f,a}$ by promotion or \\
&\hspace{0.5cm}\textbf{AND} ($c \neq a$) \textbf{then} ($\tim($\textit{not-f}$,c) < \tim(f,b)$) \textbf{OR} ($\tim($\textit{not-f}$,c) > \reqe(f,a)$) & demotion (obviously $b \neq \emptyset$) \\

C7& \textbf{if} ($\iseff(f,a)$=\textit{false}) \textbf{then} $\forall b$ that requires $f$: $\supp(f,b) \neq a$ & $a$ cannot be a supporter of $f$ for any other action $b$\\

C8& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,a)$=\textit{true}) \textbf{then} $\tim($\textit{not-f}$,a) \geq \reqe(f,a)$ & $a$ requires and deletes $f$: the condition holds before the effect \\ 

C9& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{then} $\tim(f,b) \neq \tim($\textit{not-f}$,c)$ & Solving effect interference at the same time ($f$ and \textit{not-}$f$) \\

C10& $\sum \iscond(f_i,a) \geq 1$ \textbf{AND} $\sum \iseff(f_j,a) \geq 1$ \textbf{forall} condition $f_i$ and effect $f_j$ of $a$ & Every non-dummy action has at least one condition/effect \\

\end{tabular}
\end{scriptsize}	
\label{table:constraints}
\end{center}	
\end{table*}


\subsubsection{Mutex constraints}
The mutex relationships defined in $C$ for a learning task $\mathcal{L}$ allows us to infer new information in form of dynamic observations that improve the temporal action model.

More specifically, if two variables $\tup{f_i, f_j}$ are mutex they cannot hold simultaneously. But it is important to note that, in a rich temporal model, $f_i$ does not necessarily imply \textit{not-}$f_j$.
See action \texttt{drive-truck} of Figure~\ref{fig:exampleactions2}, where \texttt{(at ?t ?l1)} and \texttt{(at ?t ?l2)} are mutex as defined in Figure~\ref{fig:example-statecs}. But effects \texttt{(not (at ?t ?l1))} and \texttt{(at ?t ?l2)} happen \textit{at start} and \textit{at end}, respectively. In other words, clearly the same truck cannot be in two locations simultaneously, but \textit{being} in \texttt{?l2} is possible some time after \textit{not being} in \texttt{?l1}. Note that this situation does not happen in simple temporal models, or in STRIPS, where all effects happen at the same time and if $\tup{f_i, f_j}$ are mutex, $f_i$ implies \textit{not-}$f_j$ and vice versa.

In order to model the mutex constraint between two variables $\tup{f_i, f_j}$ we need to create dynamic observations. Roughly speaking, immediately after $a$ asserts $f_i$, we need to ensure the observation of \textit{not-}$f_j$.
This is done while performing the search, and if $\iseff(f_i,a)$ takes the value \textit{true}, then the next observation
%(with its subjacent variables and constraints) 
is added:
%by dynamically posting the following constraint: \textbf{if} ($\iseff(f_i,a)$=\textit{true}) \textbf{then}
$\obs($\textit{not-}$f_j,\tim(f_i,a)+\epsilon)$. Note that the time for the observation cannot be just $\tim(f_i,a)$, as we first need to assert $f_i$ and one $\epsilon$ later observe \textit{not-}$f_j$.
Adding the variables and constraints for this new observation is trivial when using a Dynamic CSP (DCSP), in which the original formulation can be altered.
Otherwise, we need to statically define a new type of observation $\obs(f_i,a,$\textit{not-}$f_j)$, where $a$ supports $f_i$ which is mutex with $f_j$ and, consequently, we will need to observe \textit{not-}$f_j$. The difference \textit{w.r.t.} an original $\obs$ is twofold: i) the observation time is now initially unknown, and ii) the observation will be activated or not according to the following constraints:
\newline

{\scriptsize 

\textbf{if} ($\iseff(f_i,a)$=\textit{true}) \textbf{then} ($\start(\obs(f_i,a,$\textit{not-}$f_j))=\tim(f_i,a)+\epsilon$) \textbf{AND}

\hspace{2.83cm}($\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{true})

\textbf{else} $\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{false}
\newline
%\hspace{0.4cm} ($\supp($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))=\emptyset$)  // unnecessary by constraint 3
}



Modeling the mutex information increases the CP model size, specially in non-DCSPs, but it can be automated together with the creation of the constraints of Table~\ref{table:constraints}.
In practice, the mutex information becomes very useful for learning negative effects. The learning task satisfies the causal links of positive variables, but in the absence of many observations there is no real need to learn negative effects.
Mutex information helps to fill this void by inferring the observation of negated variables, which forces to satisfy the causal link of negative variables. 




% VERSION DE LA TABLA PARA FORMATO DE 2 COLUMNAS: QUEDA MUY DIFICIL DE LEER
%\begin{table}
%	\begin{center}
%		\caption{The CSP constraints and their semantics.}	
%		%	\setlength\extrarowheight{2pt}    
%		\begin{scriptsize}
%			\begin{tabular}{p{3.6cm}p{4.3cm}} %7.9cm
%				%\begin{tabular}{ll}
%				\hline
%				{\bf Constraint}&{\bf Description}\\\hline
%				
%				$\en(a)=\start(a)+\dur(a)$ & End time of $a$ \\
%				
%				$\en(a) \leq \start(\goal)$ & Always $\goal$ is the last action of the plan \\
%				
%				iff $\iscond(f,a)$=\textit{false} then & If $f$ is condition of $a$, $f$ needs a supporter\\
%				\hspace{0.2cm} $\supp(f,a) \neq \emptyset$ \\
%				
%				if $\iscond(f,a)$=\textit{true} then & [$\reqs(f,a)..\reqe(f,a)$] is a \\
%				\hspace{0.2cm}$\reqs(f,a) \leq \reqe(f,a)$ & valid interval\\
%				
%				if $\iseff(f,b)$=\textit{true} AND & Modeling causal links $\tup{b,f,a}$ ($b \neq \emptyset$) \\
%				\hspace{0.2cm}$\iscond(f,a)$=\textit{true} AND &  \\
%				\hspace{0.2cm}$\supp(f,a)=b$ then & \\
%				\hspace{0.4cm}$\tim(f,b) < \reqs(f,a)$ & \\								
%				
%				$\forall c \neq a$ that deletes $f$ at time $t$: & Solving threat of $c$ to causal link $\tup{b,f,a}$\\
%				\hspace{0.2cm}if $\iseff(f,b)$=true AND & by promotion or demotion ($b \neq \emptyset$)\\
%				\hspace{0.42cm}$\iscond(f,a)$=true AND \\
%				\hspace{0.42cm}$\iseff($\textit{not-f}$,c)$=true AND \\ 
%				\hspace{0.42cm}$\supp(f,a)=b$ then \\
%				\hspace{0.6cm}$t < \tim(f,b)$ OR \\
%				\hspace{0.6cm}$t > \reqe(f,a)$ \\
%				
%				if $\iseff(f,a)$=\textit{false} then & $a$ cannot be a supporter of $f$ for any other\\
%				\hspace{0.2cm}$\forall b$ that requires $f$: & action $b$ \\
%				\hspace{0.4cm}$\supp(f,b) \neq a$ \\
%				
%				
%				if $a$ requires and deletes $f$: & When $a$ requires and deletes $f$, the effect \\
%				\hspace{0.2cm}if $\iscond(f,a)$=true AND & cannot happen before the condition\\ 
%				\hspace{0.42cm}$\iseff($\textit{not-f}$,a)$=true \\			
%				\hspace{0.6cm}$\tim($\textit{not-f}$,a) \geq$ \\
%				\hspace{0.8cm}$\reqe(f,a)$ & \\
%				
%				$\forall b,c \mid b$ supports $f$ and & Solving effect interference ($f$ and \textit{not-}$f$):\\
%				\hspace{0.8cm}$c$ deletes $f$: & they cannot happen at the same time\\
%				\hspace{0.2cm}if $\iseff(f,b)$=true AND \\
%				\hspace{0.42cm}$\iseff($\textit{not-f}$,c)$=true then \\
%				\hspace{0.6cm}$\tim(f,b) \neq \tim($\textit{not-f}$,c)$ &
%			\end{tabular}
%		\end{scriptsize}	
%		\label{table:constraints}
%	\end{center}	
%\end{table}



\subsubsection{Constraints for the PDDL2.1 model}
\label{sec:PDDL21constraints}

Our temporal planning model is more expressive than PDDL2.1, but we can make it PDDL2.1-compliant by adding the simple constraints of Table~\ref{table:21constraints} for all non-dummy actions.
The first constraint limits conditions to be only at \emph{at start}, \emph{over all} or \emph{at end}, whereas the second one limits effects to happen \emph{at start} or \emph{at end}.
The third constraint makes the duration of all occurrences of the same action equals.
The structure of conditions/effects of all actions $\{a_j\}$ grounded from a particular operator are fixed, so the fourth constraint makes the conditions of all $\{a_j\}$ equal. The fifth constraint is analogous for the effects.
Finally, the sixth constraint forces all actions to have at least one of its \textit{n}-effects \textit{at end}. Actions with only \textit{at start} effects would turn the value of the duration irrelevant and they could exceed the plan makespan. Although this constraint is not specific of PDDL2.1, we include it to learn more rational durative actions.



\begin{table}
%\setlength\extrarowheight{2pt} 
\begin{center}   
\caption{The simple constraints to fulfill a PDDL2.1 model of actions.}	
\begin{scriptsize}
\begin{tabular}{ll}
\hline	
{\bf ID} &{\bf Constraint} \\ %& {\bf Description} \\
\hline
C11.1& ($\reqs(f,a) = \start(a)$) \textbf{OR} ($\reqs(f,a) = \en(a)$) \\% & Conditions at start\\
C11.2& ($\reqe(f,a) = \start(a)$) \textbf{OR} ($\reqe(f,a) = \en(a)$) \\% & Conditions at end\\
C12& ($\tim(f,a) = \start(a)$) \textbf{OR} ($\tim(f,a) = \en(a)$) \\ %& Effects at start or at end\\
C13& $\forall a_i,a_j$ occurrences of the same action: $\dur(a_i) = \dur(a_j)$ \\ %& Duration of the schema instantiations\\

C14.1& $\forall f_i: (\forall a_j: \reqs(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%& Conditions of the schema instantiations\\
&\hspace{1.1cm}$(\forall a_j: \reqs(f_i,a_j) = \en(a_j))$ \\

C14.2& $\forall f_i: (\forall a_j: \reqe(f_i,a_j) = \start(a_j))$ \textbf{OR} \\
&\hspace{1.1cm}$(\forall a_j: \reqe(f_i,a_j) = \en(a_j))$ \\

C15& $\forall f_i: (\forall a_j: \tim(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%  & Effects of the schema instantiations\\
&\hspace{0.9cm}$(\forall a_j: \tim(f_i,a_j) = \en(a_j))$ \\


C16 &$\sum_{i=1}^{n} \tim(f_i,a) > n \times \start(a)$ 

\end{tabular}
\end{scriptsize}
\label{table:21constraints}
\end{center}
\end{table}

%Note that if a condition is never deleted in a plan, it can be considered an {\em invariant} condition for such a plan that represents {\em static} knowledge; e.g. a link between two locations that makes driving possible, or modeling a petrol station that allows a refuel action in a given location, etc. The constraint to be added for invariant conditions $f \in \cond_o(a)$ is simply: $((\reqs(f,a) = \start(a))$ AND $(\reqe(f,a) = \en(a)))$, i.e. Surprisingly, invariant conditions are modeled differently depending on the human modeler. See, for instance, \texttt{(link ?from ?to)} of Figure~\ref{fig:exampleactions2}, which is modeled as an \emph{at start} condition despite: i) the link should be necessary all over the driving, and ii) no action in this domain can be planned to delete that link.
%This also happens in the \emph{transport} domain of the IPC, where a refuel action requires to have a petrol station in a location only \emph{at start}, rather than \emph{over all} which makes more sense. This shows that modeling temporal planning tasks depends on the human's common sense. On the contrary, our formulation checks the invariant conditions and deals with them always in a consistent way.

\subsection{The CSP cost functions}
We want to produce plans that are consist with the input knowledge but that as well provides an {\em explanation} of the observations that is as tight and lean as possible. To prefer this kind of {\em tight} and {\em lean} explanations we define the following two positive functions:
\begin{enumerate}
\item[$f_1$] {\em Causal-links}. This function counts the number of causal links that are created to support the provided observations.
\item[$f_2$] {\em Side-effects}. This function counts the number of possitive effects that are added by the actions in a plan but that do not build any causal link.  
\end{enumerate}

Our aim is to compute solutions to the CSP that minimize function $f_1$ while function $f_2$. is maximized. To achieve this we ask the CSP solve to {\em pareto optimize} functions $f_2$ and -$f_1$(i.e. the negation of function 1). 
  






\section{A UNIFIED CP FORMULATION FOR PLANNING, VALIDATION AND LEARNING}
\label{sec:usingCPValidation}


Our formulation is connected to the tasks of plan {\em synthesis} and plan {\em validation}, and this connection applies not only to temporal planning but also to the classical planning model, the vanilla model of AI planning where actions are instantaneous~\cite{geffner2013concise}. 

The connection between planning, validation and learning tasks lies on the fact that we can restrict the variables of our CP formulation to known values. This feature is useful to leverage a priori knowledge of a given planning domain. For instance, because we have some knowledge about the possible durations of a given action or because we already know that a given action produces for sure certain effects. This approach allows us to synthesize a plan with a given action model. In this case, each variable representing the conditions, effects and duration of the actions are constrained to a single value. Likewise, we can validate a plan by constraining the start times of actions, as we will see in Section~\ref{sec:evaluation}.

What is more, we can either synthesize (or validate) a plan despite some of the variables that representing the conditions, effects or duration of an action do not have a fixed value (its value is initially unknown). When addressing learning, planning or validating tasks, our formulation is flexible to accept different levels of specification of the input knowledge:

\begin{itemize}
\item Partial knowledge of the conditions/effects of actions.
\item Partial knowledge of actions durations (i.e. a set of possible durations).
\item Partial knowledge of the plan to validate or synthesize.
\end{itemize}

To illustrate this, let us assume that the distribution of all (or just a few) conditions and/or effects is known and, in consequence, represented in the model $A?$ of $\mathcal{L}$. If a solution to the CSP is found, then that structure of conditions/effects is consistent for the learned model. On the contrary, if no solution is found that structure is inconsistent and cannot be explained. We can also represent known values for the durations by bounding the value of $\dur(a)$ variables to a given value. We can also introduce a priori knowledge about plans by bounding the value of the $\start(a)$ variables.






  
\section{EVALUATION}
\label{sec:evaluation}

[DE MOMENTO ESTO ESTA EN EL AIRE PORQUE NO SABEMOS COMO LO VAMOS A ABORDAR??]

The CP formulation has been implemented in \textsf{Choco}\footnote{\texttt{http://www.choco-solver.org}}, an open-source Java library for constraint programming that provides an object-oriented API to state the constraints to be satisfied. \textsf{Choco} uses a static model of variables and constraints, i.e. it is not a DCSP.

The empirical evaluation of a learning task can be addressed from two perspectives. From a pure syntactic perspective, learning can be considered as an automated design task to create a new model that is similar to a reference (or {\em ground truth}) model. Consequently, the success of learning is an accuracy measure of how similar these two models are, which usually counts the number of differences (in terms of incorrect durations or distribution of conditions/effects). Unfortunately, there is not a unique reference model when learning temporal models at real-world problems. Also, a pure syntax-based measure usually returns misleading and pessimistic results, as it may count as incorrect a different duration or a change in the distribution of conditions/effects that really represent equivalent reformulations of the reference model. For instance, given the example of Figure~\ref{fig:exampleactions2}, the condition learned \texttt{(over all (link ?from ?to))} would be counted as a difference in action \texttt{drive-truck}, as it is \texttt{at start} in the reference model; but it is, semantically speaking, even more correct. Analogously, some durations may differ from the reference model but they should not be counted as incorrect. As seen in section~\ref{sec:simpleTask}, some learned durations cannot be granted, but the underlying model is still consistent. Therefore, performing a syntactic evaluation in learning is not always a good idea.

From a semantic perspective, learning can be considered as a classification task where we first learn a model from a training dataset, then tune the model on a validation test and, finally, asses the model on a test dataset. Our approach represents a one-shot learning task because we only use one plan sample to learn the model and no validation step is required.
Therefore, the success of the learned model can be assessed by analyzing the success ratio of the learned model \emph{vs.} all the unseen samples of a test dataset. In other words, we are interested in learning a model that fits as many samples of the test dataset as possible. This is the evaluation that we consider most valuable for learning, and define the success ratio as the percentage of samples of the test dataset that are consistent with the learned model. A higher ratio means that the learned model explains, or adequately fits, the observed constraints the test dataset imposes.

%\subsection{The CSP heuristics}
%\label{sec:implementation}
%Our CSP formulation is solver-independent, which means we do not use heuristics that require changes in the implementation of the CSP engine. Although this reduces the solver performance, we are interested in using it as a blackbox that can be easily changed with no modification in our formulation. However, the experimentation showed us that the following {\em value selection} heuristics are effective to solve the defined CSPs:
%\begin{enumerate}
%\item $\dur(a)$, lower values first, thus preferring shortest solutions that make the learned model consistent.
%\item $\reqs(f,a)$ and $\reqe(f,a)$. For $\reqs$, lower values first, whereas for $\reqe$, upper values first. This gives priority to $\cond_o(a)$, keeping conditions active as long as possible.  
%\item $\tim(f,a)$. Lower values first, for negative effects, while upper values first, for positive effects. This gives priority to $\eff_s(a)$ delete effects and $\eff_e(a)$ positive effects. 
%\item $\supp(f,a)$, lower values first to prefer supporters that start earlier in the plan.
%\end{enumerate}



\subsection{Learning from partially specified action models}

We have run experiments on nine IPC planning domains. It is important to highlight that these domains are encoded in PDDL2.1, with the number of operators shown in Table~\ref{table:evaluationExperiments}, so we have included the constraints given in section~\ref{sec:PDDL21constraints}. We first get the plans for these domains by using five planners (\textit{LPG-Quality}~\cite{gerevini2003planning}, \textit{LPG-Speed}~\cite{gerevini2003planning}, \textit{TP}~\cite{jimenez2015temporal}, \textit{TFD}~\cite{eyerich2009using} and \textit{TFLAP}~\cite{marzal2016temporal}), where the planning time is limited to 100s.
The actions and observations on each plan are automatically compiled into a CSP learning instance. Then,
%we create the CP formulation and
we run the one-shot learning task to get a temporal action model for each instance, where the learning time is limited to 100s on an Intel i5-6400 @ 2.70GHz with 8GB of RAM.
In order to assess the quality of the learned model, we validate each model \emph{vs.} the other models \emph{w.r.t.} the \emph{struct}ure, the \emph{dur}ation and the \emph{struct}ure+\emph{dur}ation, as discussed in section~\ref{sec:usingCPValidation}.
For instance, the \emph{zenotravel} domain contains 78 instances, which means learning 78 models. Each model is validated by using the 77 remaining models, thus producing 78$\times$77=6006 validations per struct, dur and struct+dur each. The value for each cell is the average success ratio.
In \emph{zenotravel}, the struct value means that the distribution of conditions/effects learned by using only one plan sample is consistent with all the samples used as dataset (100\% of the 6006 validations), which is the perfect result, as also happens in \emph{floortile} and \emph{sokoban} domains.
The dur value means the durations learned explain 68.83\% of the dataset. This value is usually lower because any learned duration that leads to inconsistency in a sample counts as a failure. The struct+dur value means that the learned model explains entirely 35.76\% of the samples. This value is always the lowest because a subtle structure or duration that leads to inconsistency in a sample counts as a failure.
As seen in Table~\ref{table:evaluationExperiments}, the results are specially good, taking into consideration that we use only one sample to learn the temporal action model.
These results depend on the domain size (number of operators, which need to be grounded), the relationships (causal links, threats and interferences) among the actions, and the size and quality of the plans.

\begin{table}
\begin{center}
\caption{Number of operators to learn. Instances used for validation. Average success ratio of the one-shot learned model \emph{vs.} the test dataset in different IPC planning domains.}
\begin{scriptsize}
\begin{tabular}{l|llrrr}
\hline	
& {\bf ops} & {\bf ins} & {\bf struct} & {\bf dur} & {\bf struct+dur}  \\\hline

\emph{zenotravel} & 5 & 78 & 100\% & 68.83\% & 35.76\% \\
\emph{driverlog} & 6 & 73 & 97.60\% & 44.86\% & 21.04\% \\
\emph{depots} & 5 & 64 & 55.41\% & 76.22\% & 23.19\% \\
\emph{rovers} & 9 & 84 & 78.84\% & 5.35\% & 0.17\% \\
\emph{satellite} & 5 & 84 & 80.74\% & 57.13\% & 40.53\% \\
\emph{storage} & 5 & 69 & 58.08\% & 70.10\% & 38.36\% \\
\emph{floortile} & 7 & 17 & 100\% & 80.88\% & 48.90\%\\
\emph{parking} & 4 & 49 & 86.69\% & 81.38\% & 54.89\% \\
\emph{sokoban} & 3 & 51 & 100\% & 87.25\% & 79.96\% \\

\end{tabular}
\end{scriptsize}
\label{table:evaluationExperiments}
\end{center}
\end{table}


We have observed that some planners return plans with unnecessary actions, which has a negative impact for learning precise durations.
%Some planners return plans with unnecessary actions, thus making an adequate learning more difficult. In particular, actions that are redundant have a negative impact for learning precise durations.
The worst result is returned in the \emph{rovers} domain, which models a group of planetary rovers to explore the planet they are on. Since there are many parallel actions for taking pictures/samples and navigation of multiple rovers, learning the duration and the structure+duration is particularly complex in this domain.

\subsection{Learning from scratch}

\section{CONCLUSIONS}
\label{sec:conclusions}


We have presented a purely declarative CP formulation, which is independent of any CSP solver, to address the learning of temporal action models. Learning in planning is specially interesting to recognize past behavior in order to predict and anticipate actions to improve decisions.
The main contribution is a simple formulation that is automatically derived from the actions and observations on each plan execution, without the necessity of specific hand-coded domain knowledge. It is also flexible to support a very expressive temporal planning model, though it can be easily modified to be PDDL2.1-compliant.
Formal properties are inherited from the formulation itself and the CSP solver. The formulation is correct because the definition of constraints to solve causal links, threats and effect interferences are supported, which avoids contradictions. It is also complete because the solution needs to be consistent with all the imposed constraints, while a complete exploration of the domain of each variable returns all the possible learned models in the form of alternative consistent solutions.

Unlike other approaches that need to learn from datasets with many samples, we perform a one-shot learning. This reduces both the size of the required datasets and the computation time. The one-shot learned models are very good and explain a high number of samples in the datasets used for testing. Moreover, the same CP formulation is valid for learning and for validation, by simply adding constraints to the variables. This is an advantage, as the same formulation allows us to carry out different tasks: from entirely learning, partial learning/validation (structure and/or duration) to entirely plan validation.
According to our experiments, learning the structure of the actions in a one-shot way leads to representative enough models, but learning the precise durations is more difficult, and even impossible, when many actions are executed in parallel.

Our CP formulation can be adapted straightforward to address learning, planning or validation tasks within the classical planning model. In this case actions cannot have conditions {\em overall} or {\em at end} as well as they cannot have {\em at start} effects. Therefore the variables representing this kind of information can be removed from the CSP model (or be set to {\tt false}). Further the duration of any action is fixed to one unit~\cite{jimenez2015temporal}.  Finally, our CP formulation can be represented and solved by Satisfiability Modulo Theories, which is part of our current work. 


%\ack We would like to 


\bibliographystyle{ecai}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
