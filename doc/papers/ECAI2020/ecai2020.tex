\documentclass{ecai}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{array}

\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}    % precondition
\newcommand{\eff}{\mathsf{eff}}    % effect
\newcommand{\cond}{\mathsf{cond}}  % condition
\newcommand{\dur}{\mathsf{dur}}    % duration
\newcommand{\iscond}{\mathsf{is\_cond}}    % is_cond
\newcommand{\iseff}{\mathsf{is\_eff}}    % is_eff
\newcommand{\obs}{\mathsf{obs}}    % observation
\newcommand{\start}{\mathsf{start}}% start
\newcommand{\en}{\mathsf{end}}     % end
\newcommand{\til}{\mathsf{til}}    % TIL
\newcommand{\supp}{\mathsf{sup}}   % sup
\newcommand{\tim}{\mathsf{time}}   % time
\newcommand{\reqs}{\mathsf{req\_{start}}} % req_start
\newcommand{\reqe}{\mathsf{req\_{end}}}   % req_end
\newcommand{\ini}{\mathsf{init}}   % init
\newcommand{\goal}{\mathsf{goal}}  % goal


%TamaÃ±o 7+1 pages

\begin{document}
\title{One-Shot Learning of Temporal Actions Models via Constraint Programming}
\author{Antonio Garrido \and Sergio Jim\'enez}
 
%\author{Name1 Surname1 \and Name2 Surname2 \and Name3 Surname3\institute{University, Country, email: somename@university.edu} }
 
\maketitle

\begin{abstract}
  We present a {\em Constraint Programming} (CP) formulation for learning temporal planning  action models from the observation of a single plan execution ({\em one shot}). Inspired by the CSP approach to {\em temporal planning}, our CP formulation models {\em time-stamps} for states and actions, {\em causal-link} relationships, condition {\em threats} and effect {\em interferences}. This modeling evidences the connection between the tasks of {\em plan synthesis}, {\em plan validation} and {\em action model learning} in the temporal planning setting. Our CP formulation is solver-independent so off-the-shelf CSP solvers can be used for the resolution of any of these three tasks. The performance of our CP formulation is assessed when {\em learning} and {\em validating} action models at several temporal planning domains specified in the PDDL2.1 representation language. 
\end{abstract}



\section{INTRODUCTION}

{\em Temporal planning} is an expressive planning model that relaxes the assumption of instantaneous actions of {\em classical planning}~\cite{geffner2013concise}. Actions in temporal planning are called {\em durative}, have an associated duration and, their conditions/effects may hold/happen at different times~\cite{fox2003pddl2}. This means that {\em durative actions} can be executed in parallel and overlap in several different ways~\cite{cushing2007temporal}, and that valid solutions for temporal planning instances specify the precise time-stamp when each durative action starts and ends~\cite{howey2004val}.

Despite the potential of state-of-the-art planners, their application to real world problems is still somewhat limited mainly because of the difficulty of specifying correct and complete planning models~\cite{kambhampati2007model}. The more expressive the planning model, the more evident becomes this {\em knowledge acquisition bottleneck} that jeopardizes the usability of planning technology. There are however growing efforts in the planning community for the machine learning of action models from sequential plans: since pioneering learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with {\em quantifiers}~\cite{AmirC08,ZhuoYHL10}, from {\em noisy} actions or states~\cite{MouraoZPS12,zhuo2013action}, from {\em null state information}~\cite{cresswell2013}, or from {\em incomplete} domain models~\cite{ZhuoK17,ZhuoNK13}.

Most of the cited approaches for model learning are purely inductive and require large input datasets, e.g. hundreds of plan observations, to compute statistically significant models. With the aim of understanding better the connection between the learning of durative action models, {\em temporal planning}  and the validation of temporal plans, this paper follows a radically different approach and studies the singular learning scenario where just the observation of a single plan execution ({\em one-shot}) is available. We leverage a solver-independent CP formulation that integrates the {\em learning} of temporal planning action models with the {\em synthesis} and the {\em validation} of temporal plans. 

As far as we know this paper presents the first approach for learning action models for the {\em temporal planning} setting. While learning an action model for classical planning means computing the actions' conditions and effects that are consistent with the input observations, learning temporal action models requires additionally: i) identifying how conditions and effects are temporally distributed within the actions, and ii) estimate the action duration. Further our approach allows the learning of action models from observations of plans with overlapping actions. This feature makes our approach appealing for learning action models from observations of multi-agent environments~\cite{furelos2018carpool}. 



\section{BACKGROUND}

This section formalizes the {\em temporal planning} and the {\em constraint satisfaction} model that we follow in this work.

\subsection{Temporal Planning}
\label{sec:temporalplanning}

We assume that {\em states} are factored into a set $F$ of Boolean variables. A state $s$ is a time-stamped assignment of values to all the variables in $F$. A {\em temporal planning problem} is a tuple $P=\tup{F,I,G,A}$ where the {\em initial state} $I$ is a fully observed state (i.e. a total assignment of the state variables $|I|=|F|$) time-stamped with $t=0$; $G \subseteq F$ is a conjunction of {\em goal conditions} over the variables in $F$ that defines the set of goal states; and $A$ represents the set of {\em durative actions}.

A {\em durative action} has an associated duration and may have conditions/effects on $F$ at different times~\cite{garrido2009constraint,vidal2006branching}. To compactly represent temporal planning problems, we assume that the state variables in $F$ are intantiations of a given set of predicates $\Psi$ (like in the PDDL language~\cite{younes2004ppddl1}) and that durative actions in $A$ are fully grounded from {\em action schemes} (also known as {\em operators}).

PDDL2.1 is the input representation language for the temporal track of the International Planning Competition (IPC)~\cite{fox2003pddl2,ghallab2004automated}. According to PDDL2.1, a durative action $a\in A$ is defined with the following elements:
\begin{enumerate}
\item $\dur(a)$, a positive value indicating the {\em duration} of the action.

\item $\cond_s(a), \cond_o(a), \cond_e(a)$ representing the three types of action {\em conditions}. Unlike the \emph{pre}conditions of classical actions, action conditions in PDDL2.1 must hold: before $a$ is executed ({\em at start}), during the entire execution of $a$ ({\em over all}) or when $a$ finishes ({\em at end}), respectively. 

\item $\eff_s(a)$ and $\eff_e(a)$ represent the two types of action effects. In PDDL2.1, effects can happen {\em at start} or {\em at end} of action $a$ respectively, and can be either positive or negative (i.e. asserting or retracting variables). 

\end{enumerate}

PDDL2.1 is a restricted temporal planning model that defines the semantics of a {\em durative action} $a$ as two discrete events, $\start(a)$ and $\en(a)=\start(a)+\dur(a)$. This means that if $a$ starts on state $s$ with time-stamp $\start(a)$, then $\cond_s(a)$ must hold in $s$. Ending action $a$ in state $s'$, with time-stamp $\en(a)$, means $\cond_e(a)$ must hold in $s'$. {\em Over all} conditions must hold at any state between $s$ and $s'$ or, in other words, throughout the closed interval $[\start(a)..\en(a)]$. Likewise, {\em at start} and {\em at end} effects are instantaneously applied at states $s$ and $s'$, respectively (continuous effects are not considered in this work). Figure~\ref{fig:exampleactions2} shows an example of two schemes for PDDL2.1 durative actions taken from the {\em driverlog} domain. The \texttt{board-truck} schema defines a fixed duration of two time units while the duration of \texttt{drive-truck} depends on the driving time associated to the two given locations.

\begin{figure}
\begin{footnotesize}    
\begin{verbatim}
(:durative-action board-truck
  :parameters (?d - driver ?t - truck ?l - location)
  :duration (= ?duration 2)
  :condition (and (at start (at ?d ?l)) 
                  (at start (empty ?t))
                  (over all (at ?t ?l)))
  :effect (and (at start (not (at ?d ?l))) 
               (at start (not (empty ?t)))
               (at end (driving ?d ?t))))


(:durative-action drive-truck
  :parameters (?t - truck ?l1 - location ?l2 - location 
               ?d - driver)
  :duration (= ?duration (driving-time ?l1 ?l2))
  :condition (and (at start (at ?t ?l1)) 
                  (at start (link ?l1 ?l2))
                  (over all (driving ?d ?t)))
  :effect (and (at start (not (at ?t ?l1))) 
               (at end (at ?t ?l2))))
\end{verbatim}
\end{footnotesize}    
%\end{tabular}
\caption{Two action schemes of {\em durative actions} represented in PDDL2.1.}
\label{fig:exampleactions2}
\end{figure}

PDDL2.2 is an extension of the PDDL2.1 representation language that includes the notion of {\em Timed Initial Literal}~\cite{hoffmann2005}, denoted as $\til(f,t)$, and representing that variable $f\in F$ becomes true at a certain time $t>0$, independently of the actions in the plan~\cite{Edelkamp04}. TILs are useful to model {\em exogenous events}; for instance, in a logistics scenario, the 8h-20h time window when a warehouse is open can be modeled with these two timed initial literals: $\til(openWarehouse,8)$ and $\til(\neg openWarehouse,20)$.

A {\em temporal plan} is a set of pairs $\pi=\{(a_1,t_{a_1}),(a_2,t_{a_2})\ldots (a_n,t_{a_n})\}$. Each pair $(a,t_a)$ contains a {\em durative action} $a$ and the action {\em time-stamp} $t_a=\start(a)$ . The execution of a temporal plan starting from a given initial state $I$ induces a state sequence formed by the union of all states $\{s_{t_i}, s_{t_i+\dur(a_i)}\}$, where there exists an initial state $s_{0}=I$, and a state $s_{end}$ that is the last state induced by the execution of the plan. Sequential plans can then be expressed as temporal plans but not the opposite. A {\em solution} to a given temporal planning problem $P$ is a {\em temporal plan} $\pi$ such that its execution, starting from the corresponding initial state, eventually reaches a state that meets the goal conditions, $G\subseteq s_{end}$. A solution is {\em optimal} iff it minimizes the plan {\em makespan} (i.e., the maximum $\en(a)=\start(a)+\dur(a)$ of any actions in the plan).

\subsection{Constraint Satisfaction}
A {\em Constraint Satisfaction Problem} (CSP) is a tuple $\tup{X,D,C}$, where $X$ is a set of finite-domain {\em variables}, $D$ represents the {\em domain} for each of these variables and $C$ is a set of {\em constraints} among the variables in $X$ that bound their possible values in $D$.

A {\em solution} to a CSP is an assignment of values to all the variables in $X$ that is {\em consistent} with all the input constraints. Given a CSP there may be many different solutions to that problem, i.e., different variable assignments that are {\em consistent} with the input constraints.

A {\em cost-function} can be defined over the variables in $X$ to specify user preferences about the space of possible solutions. Given a CSP and a cost-function, then an {\em optimal solution} is a full variable assignment that is consistent with the constraints of the CSP such that it also minimizes the value of the defined cost-function.



\section{One-shot learning of temporal actions models}
\label{section:learningTemporalModels}
We formalize the task of the {\em one-shot learning of temporal action models} as a tuple $\mathcal{L}=\tup{F,I,G,A?,O,C}$ where:

\begin{itemize}
\item $\tup{F,I,G,A?}$ is a {\em temporal planning problem} such that the actions in $A?$ are {\em incomplete}. By {\em incomplete} we mean that the exact conditions/effects, their temporal annotation, and the duration of actions are unknown while the actions {\em header} (i.e., the {\em  name} and {\em parameters} of each action) is known. With this regard, we say that a fluent $f\in F$ is {\em candidate} to appear in the condition/effects of an action $a\in A?$ iff $f$ appears in the set of FOL interpretations of the predicates that shape the fluents $F$ over the action parameters $pars(a)$. We call this subset of candidate fluents the {\em alphabet} of an action $a$ and is denoted as $\alpha(a)$. For instance, Figure~\ref{fig:exampleCandidates} shows $\alpha($board-truck(driver1,truck1,loc1)$)$, i.e., the {\em alphabet} of six {\em candidates} to appear in the conditions/effect of the ground action {\em board-truck(driver1,truck1,loc1)}.
 
\begin{figure}[hbt!]
\begin{footnotesize}    
\begin{verbatim}
;;;
;; Alphabet board-truck(driver1,truck1,loc1)

(at driver1 loc1) (at truck1 loc1) 
(driving driver1 truck1)  (empty truck1) 
(path loc1 loc1) (link loc1 loc1)
\end{verbatim}
\end{footnotesize}    
\caption{{\em Alphabet} for the ground action {\em board-truck(driver1,truck1,loc1)}.}
\label{fig:exampleCandidates}
\end{figure}  
 
\item $O$ is the set of {\em observations} over a single plan execution. Al least this set contains a full observation of the initial state (time-stamped with $t=0$) and a final state observation, that equals the goals $G$ of the temporal planning problem, time-stamped with $t_{end}$ (the {\em makespan} of the observed plan). Additionally, it can contain time-stamped observations of traversed intermediate {\em partial states}\footnote{In this work, not all variables can be observed at any time; that is, we deal with {\em partial observations} (e.g. just a subset of variables is observable by associated sensors). Observations are noiseless, which means that if a value is observed, that is the actual value of that variable.} as well as the times when actions start and/or end their execution. Figure~\ref{fig:exampleObservations} shows an example of the observation of a plan execution taken from the {\em driverlog} domain.

\begin{figure}[hbt!]
\begin{scriptsize}    
\begin{verbatim}
(:objects driver1 driver2 - driver
          truck1 truck2 - truck
          package1 package2 - obj
          s0 s1 s2 p1-0 p1-2 - location)

(:init	(at driver1 s2)	(at driver2 s2) (at truck1 s0)
        (empty truck1) (at truck2 s0) (empty truck2) 
        (at package1 s0) (at package2 s0)
        (path s1 p1-0) (path p1-0 s1) (path s0 p1-0) 
        (path p1-0 s0) (path s1 p1-2) (path p1-2 s1)
        (path s2 p1-2) (path p1-2 s2)
        (link s0 s1) (link s1 s0) (link s0 s2) (link s2 s0)
        (link s2 s1) (link s1 s2))

(:observation :time-stamp 56
              (at driver1 s1) (at truck1 s1))

(:observation :time-stamp 78
              (at package1 s0) (at package2 s0))
\end{verbatim}
\end{scriptsize}    
\caption{Example of a set of three observations (containing the fully observed initial state and two time-stamped partial states) extracted from the execution of a plan from the {\em driverlog} domain.}
\label{fig:exampleObservations}
\end{figure}  


\item $C$ is a set of {\em constraints} that captures domain-specific expert knowledge. In this work these constraints are of two kinds: 
\begin{itemize}
\item Constraints that specify that a given $f\in \alpha(a)$ is actually in the conditions/effects of action $a$. These constraints allow to represent partially specified action models~\cite{zhuo2013refining}. For instance we may know in advance that  the action {\tt board-truck} requires the {\tt driver} and the {\tt truck} to be at the same location. 
\item Mutually-exclusive ({\em mutex}) constraints that allow to (1), deduce new observations and (2), prune action models inconsistent with these constraints. Figure~\ref{fig:example-statecs} shows an example of a set of five {\em mutex constraints} for the {\em driverlog} domain. 
\end{itemize}
\end{itemize}

 \begin{figure}
\begin{scriptsize}    
\begin{tabular}{p{2.7cm}l}
$\forall truck,driver:$ & $\neg empty(truck)\vee\neg driving(driver,truck)$.\\
  $\forall driver,loc_1,loc_2:$ & $\neg at(driver,loc_1)\vee\neg at(driver,loc_2),$ \\
  & $\neq (loc_1,loc_2)$.\\
  $\forall driver, truck1, truck2:$  & $\neg driving(driver,truck_1)\vee$\\
  & $\neg driving(driver,truck_2),\neq (truck_1,truck_2)$.\\
  $\forall drvr1, drvr2, truck:$  & $\neg driving(drvr_1,truck)\vee$\\
  &$\neg driving(driver_2,truck), \neq (drvr_1,drvr_2)$.\\
$\forall drvr,location,truck:$ & $\neg at(drvr,location)\vee\neg driving(drvr,truck)$.
\end{tabular}  
\end{scriptsize}    
\caption{Examples of five {\em mutex constraints} for the {\em driverlog} domain.}
\label{fig:example-statecs}
\end{figure}

A {\em solution} to a learning task $\mathcal{L}$ is a fully specified model of durative actions $\mathcal{A}$ such that the {\em conditions}, {\em effects}, their temporal annotations and the {\em duration} of any action in $\mathcal{A}$ are: i) completely specified; and ii) {\em consistent} with $\mathcal{L}=\tup{F,I,G,A?,O,C}$. By {\em consistent} we mean that there exists a valid plan that exclusively contains actions in $\mathcal{A}$ and whose execution starting in $I$, produces all the observations in $O$ at the associated time-stamps, while it satisfies all constraints in $C$, and reaches a final state that satisfies $G$.



\section{Learning action models with CSPs}
\label{section:learningAsCSP}
Given a {\em one-shot learning task} $\mathcal{L}$ as defined in Section~\ref{section:learningTemporalModels}, we automatically create a CSP, whose solution induces an action model that solves $\mathcal{L}$. This method is solver-independent and integrates previous work on {\em temporal planning} as satisfiability~\cite{vidal2006branching,hu2007temporally,garrido2009constraint,rintanen2015discretization}. 

\subsection{The variables}
For each action $a\in A?$ and candidate $f\in\alpha(a)$ to appear in the conditions/effects of $a$, we create the eight CSP variables of Table~\ref{table:variables}. 

Variable X1 represents the time when an action {\em starts} (its time-stamp), X2 represents when the action {\em ends} and variable X3 represents the action duration. The value of X1,X2 and X3 can either be observed in $O$ or derived from the expression $\en(a)=\start(a)+\dur(a)$. We  model time in $\mathbb{Z}^+$ and bound all maximum times to the {\em makespan} of the observed plan ($t_{end}$ if observed in $O$). If the observation of $t_{end}$ is unavailable, we consider a large enough domain for time. Boolean variables X4/X5 model whether $f$ is actually a condition/effect of action $a$. X6.1 and X6.2 define the closed interval throughout condition $f$ must hold for the application of action $a$ (provided $\iscond(f,a)$=\textit{true}). X7 models a {\em causal link} representing that action $b$ supports $f$ that is required by $a$. If $f$ is not a condition of $a$ ($\iscond(f,a)$=\textit{false}) then $\supp(f,a)$=$\emptyset$, representing an empty supporter. Last but not least, variable X8 models the time-stamp when effect $f$ happens in $a$ (provided $\iseff(f,a)$=\textit{true}).

\begin{table}
\begin{center}
\caption{The CSP variables, their domains and semantics.}
\begin{scriptsize}
\begin{tabular}{llll}
\hline	
{\bf ID} & {\bf Variable} & {\bf Domain} & {\bf Description} \\
\hline
X1 &$\start(a)$ & $[0..t_{end}]$ & {\em Start time} of action $a$ \\
X2 &$\en(a)$ & $[0..t_{end}]$ & {\em End time} of action $a$ \\
X3 &$\dur(a)$ & $[0..t_{end}]$ & {\em Duration} of action $a$ \\

X4 &$\iscond(f,a)$ & $\{0,1\}$ & 1 if $f$ is a {\em condition} of $a$; 0 otherwise \\
X5 &$\iseff(f,a)$ & $\{0,1\}$ & 1 if $f$ is an {\em effect} of $a$; 0 otherwise \\

X6.1 &$\reqs(f,a)$, & $[0..t_{end}]$ & Interval when action $a$ requires $f$\\ 
X6.2 &$\reqe(f,a)$  &  & \\

X7 &$\supp(f,a)$ & $\{b\}_{b\in A?} \cup \emptyset $&  Supporters for causal link $\tup{b,f,a}$ \\ 
X8 &$\tim(f,a)$ & $[0..t_{end}]$ & Time when the effect $f$ of $a$ happens
\end{tabular}
\end{scriptsize}
\label{table:variables}
\end{center}
\end{table}

This simple formulation is powerful enough to model $\til$s and {\em observations}. The intuition is that modeling a $\til$ is analogous to modeling the {\em initial state} of a planning task (both represent information that is given at a particular time but externally to the execution of the plan). Likewise modeling observations is analogous to modeling the $\goal$s of a planning task, as they both represent conditions that must be satisfied by the execution of the plan. On the one hand a $\til(f,t)$ is modeled as a {\em dummy} action that starts at time $t$ and has instantaneous duration ($\start(\til(f,t))=t$ and $\dur(\til(f,t))=0$) with no conditions and the single effect $f$ that happens at time $t$ ($\iseff(f,\til(f,t))$=\textit{true} and $\tim(f,\til(f,t))=t$). On the other hand, an observation $\obs(f,t)$ is modeled as another {\em dummy} action that also starts at time $t$ and has instantaneous duration ($\start(\obs(f,t))=t$ and $\dur(\obs(f,t))=0$) but with only one condition $f$, which is the value observed for fact $f$ ($\iscond(f,\obs(f,t))$=\textit{true}, $\supp(f,\obs(f,t))\neq \emptyset$ and $\reqs(f,\obs(f,t))=\reqe(f,\obs(f,t))=t$), and no effects at all. Observations can also refer to the {\em start} and {\em end} of an action, $\obs(is\_start(a),t)$ represents it was observed that action $a$ starts at $t$ while $\obs(is\_end(a),t)$ represents it was observed that action $a$ ends at $t$.

\begin{table*}
\begin{center}
\caption{The CSP constraints and brief description.}	
\begin{scriptsize}
\begin{tabular}{lp{10.4cm}p{6.6cm}}
\hline
{\bf ID}&{\bf Constraint}&{\bf Description}\\\hline
C1& $\en(a)=\start(a)+\dur(a)$ & Relationship among start, end and duration of $a$ \\
C2& $\en(a) \leq \start(\goal)$ & Always $\goal$ is the last action of the plan \\
C3& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{then} $\reqs(f,a) \leq \reqe(f,a)$ & [$\reqs(f,a)..\reqe(f,a)$] is a valid interval\\
C4& \textbf{iff} ($\iscond(f,a)$=\textit{false}) \textbf{then} $\supp(f,a) = \emptyset$ & $f$ is not a condition of $a \iff $ the supporter of $f$ in $a$ is $\emptyset$ \\
C5& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$))  & Modeling the causal link $\tup{b,f,a}$: supporting $f$ before it is \\
&\hspace{0.5cm}\textbf{then} $\tim(f,b) < \reqs(f,a)$ & required (obviously $b \neq \emptyset$) \\
C6& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$) & Solving threat of $c$ to causal link $\tup{b,f,a}$ by promotion or \\
&\hspace{0.5cm}\textbf{AND} ($c \neq a$) \textbf{then} ($\tim($\textit{not-f}$,c) < \tim(f,b)$) \textbf{OR} ($\tim($\textit{not-f}$,c) > \reqe(f,a)$) & demotion (obviously $b \neq \emptyset$) \\
C7& \textbf{if} ($\iseff(f,a)$=\textit{false}) \textbf{then} $\forall b$ that requires $f$: $\supp(f,b) \neq a$ & $a$ cannot be a supporter of $f$ for any other action $b$\\
C8& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,a)$=\textit{true}) \textbf{then} $\tim($\textit{not-f}$,a) \geq \reqe(f,a)$ & $a$ requires and deletes $f$: the condition holds before the effect \\ 
C9& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{then} $\tim(f,b) \neq \tim($\textit{not-f}$,c)$ & Solving effect interference at the same time ($f$ and \textit{not-}$f$) \\
C10& $\sum \iscond(f_i,a) \geq 1$ \textbf{AND} $\sum \iseff(f_j,a) \geq 1$ \textbf{forall} condition $f_i$ and effect $f_j$ of $a$ & Every non-dummy action has at least one condition/effect
\end{tabular}
\end{scriptsize}	
\label{table:constraints}
\end{center}	
\end{table*}

\subsection{The constraints}
\label{section:CSPconstraints}
Table~\ref{table:constraints} shows the constraints defined among the CSP variables of Table~\ref{table:variables}. C1 models the duration of an action while C2 indicates that actions must end before $t_{end}$. C3 forces to have a well-defined $[\reqs,\reqe]$ interval, when the conditions of action $a$ are required. C4 models that only action conditions require supporters and C5 models that the time when $b$ supports $f$ must be before $a$ requires it because of the causal link $\tup{b,f,a}$\footnote{$\tim(f,b) < \reqs(f,a)$ and not $\leq$ because our temporal planning model assumes $\epsilon > 0$ ($\epsilon$ denotes a small tolerance that implies no collision between the time when effect $f$ is supported and when it is required, like in PDDL2.1~\cite{fox2003pddl2}). When time is modeled in $\mathbb{Z}^+$, $\epsilon=1$ so $\leq$ becomes $<$.}. Given a causal link $\tup{b,f,a}$, constraint C6 avoids {\em threats} of actions $c$ deleting $f$ (threats are solved via {\em promotion} or {\em demotion}~\cite{ghallab2004automated}). C7 prevents action $a$ being a supporter of $f$ when $\iseff(f,a)$=\textit{false}. Constraint C8 models the fact that when the same action requires and deletes $f$ the effect cannot happen before the condition. Note the $\geq$ inequality here: if one condition and one effect of the same action happen at the same time, the underlying semantics in planning considers the condition is checked instantly before the effect~\cite{fox2003pddl2}. C9 prevents two actions have contradictory effects and C10 forces actions to have at least one condition and one effect (C9 applies to any action, including dummy actions $\ini$, $\goal$, $\til$ and $\obs$, while C10 only applies to {\em non-dummy} actions).

%Some conditions of Table~\ref{table:constraints} are redundant. For instance C5 and C6, $\supp(f,a)=b$ means obligatorily $\iseff(f,b)=$ \textit{true}. We include them here to define an homogeneous formulation but they are not included in our implementation. For simplicity, the value of some unnecessary variables is not bounded in the table. For instance, if $\iscond(f,a)$=\textit{false}, variables $\reqs(f,a)$ and $\reqe(f,a)$ become useless.

\subsubsection{Constraints for PDDL2.1}
\label{sec:PDDL21constraints}
The presented CSP formulation accommodates a level of expressiveness beyond PDDL2.1 because it allows conditions/effects to be at any time, even outside the execution of the action. For example, it allows a condition $f\in\alpha(a)$ to hold in $\start(a)\pm$2: $\reqs(f,a)=\start(a)-2$ and $\reqe(f,a)=\start(a)+2$. Likewise an effect $f\in\alpha(a)$ might also happen after the action ends e.g., $\tim(f,a)=\en(a)+2$.

Making our formulation {\em PDDL2.1-compliant} is straightforward, by adding the constraints of Table~\ref{table:21constraints} for all {\em non-dummy} actions: C11 limits the {\em conditions} of an action to be only at \emph{at start}, \emph{over all} or \emph{at end}. C12 limits the {\em effects} of an action to only happen \emph{at start} or \emph{at end}. In PDDL2.1 the structure of conditions/effects of all actions $\{a_j\}$ grounded from a particular operator are fixed. With this regard, C13 makes the conditions of all $\{a_j\}$ the same while C14 makes the effects of all $\{a_j\}$ the same. C15 makes the duration of all occurrences of the same action equal (if desired, this is not mandatory in PDDL2.1). Last but not least, C16 forces all actions to have at least one of its \textit{n}-effects \textit{at end}. Actions with only \textit{at start} effects turn the value of the duration irrelevant besides they could exceed the plan makespan (this last constraint is not specific of PDDL2.1 but produces more rationale models for {\em durative actions}).

\begin{table}
\begin{center}   
\caption{Constraints to learn PDDL2.1-compliant action models.}	
\begin{scriptsize}
\begin{tabular}{ll}
\hline	
{\bf ID} &{\bf Constraint} \\ %& {\bf Description} \\
\hline
C11.1& ($\reqs(f,a) = \start(a)$) \textbf{OR} ($\reqs(f,a) = \en(a)$) \\% & Conditions at start\\
C11.2& ($\reqe(f,a) = \start(a)$) \textbf{OR} ($\reqe(f,a) = \en(a)$) \\% & Conditions at end\\
C12& ($\tim(f,a) = \start(a)$) \textbf{OR} ($\tim(f,a) = \en(a)$) \\ %& Effects at start or at end\\
C13.1& $\forall f_i: (\forall a_j: \reqs(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%& Conditions of the schema instantiations\\
&\hspace{1.1cm}$(\forall a_j: \reqs(f_i,a_j) = \en(a_j))$ \\
C13.2& $\forall f_i: (\forall a_j: \reqe(f_i,a_j) = \start(a_j))$ \textbf{OR} \\
&\hspace{1.1cm}$(\forall a_j: \reqe(f_i,a_j) = \en(a_j))$ \\
C14& $\forall f_i: (\forall a_j: \tim(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%  & Effects of the schema instantiations\\
&\hspace{0.9cm}$(\forall a_j: \tim(f_i,a_j) = \en(a_j))$ \\
C15& $\forall a_i,a_j$ occurrences of the same action: $\dur(a_i) = \dur(a_j)$ \\ %& Duration of the schema instantiations\\
C16 &$\sum_{i=1}^{n} \tim(f_i,a) > n \times \start(a)$ 

\end{tabular}
\end{scriptsize}
\label{table:21constraints}
\end{center}
\end{table}


\subsubsection{Mutex constraints}
Mutex-constraints can be exploited in a pre-proces step for completing the input observations of a {\em one-shot learning task} $\mathcal{L}$. The set of mutexes that is given as input to a learning task $\mathcal{L}$ allows to infer new information: if two Boolean variables $\tup{f_i, f_j}$ are mutex they cannot hold simultaneously. This means that if we observe $f_i$, then we can infer $\neg f_j$ (despite $\neg f_j$ was not actually observed). Likewise if we observe $f_j$, we can infer $\neg f_i$. This source of additional knowledge is specially relevant for correctly learning  {\em negative effects} when there is a lack of input observations. Mutexes helps to fill this void inferring the observation of negated variables, which forces later to satisfy the {\em causal links} of negative variables. 

Note that, in {\em temporal planning} models with {\em durative actions}, given a $\tup{f_i, f_j}$ mutex $\neg f_i$ does not necessarily implies $f_j$. See the effects \texttt{(not (at ?t ?l1))} and \texttt{(at ?t ?l2)} of action \texttt{drive-truck} of Figure~\ref{fig:exampleactions2} that respectively happen \textit{at start} and \textit{at end}. If \texttt{(at ?t ?l1)} and \texttt{(at ?t ?l2)} are mutex (as defined in Figure~\ref{fig:example-statecs}), the same truck cannot be in two locations simultaneously. It is valid however for a truck to be, for some time, at no location. This situation does not happen however in classical planning models where actions have instantaneous effects, so if $\tup{f_i, f_j}$ are mutex then $f_i$ implies \textit{not-}$f_j$ and vice versa.

{\em Dynamic observations} can be created to exploit mutex constraints at any generated intermediate state (even if the intermediate state was not observed at all but was inferred by the CSP solver). A mutex $\tup{f_i, f_j}$ means that, immediately after $a$ asserts $f_i$, we need to ensure the observation \textit{not-}$f_j$. Furher, if $\iseff(f_i,a)$ takes the value \textit{true}, then the next observation is added: $\obs($\textit{not-}$f_j,\tim(f_i,a)+\epsilon)$. The time of the observation cannot be just $\tim(f_i,a)$, as we first need to assert $f_i$ and one $\epsilon$ later observe \textit{not-}$f_j$. Adding the variables and constraints for this new observation during the CSP search is trivial for {\em Dynamic CSPs} (DCSPs)~\cite{mittal1990dynamic}. Otherwise, we need to statically define a new type of observation $\obs(f_i,a,$\textit{not-}$f_j)$, where $a$ supports $f_i$ which is mutex with $f_j$ and, consequently, we will need to observe \textit{not-}$f_j$. The difference \textit{w.r.t.} an original $\obs$ is two-fold: i) the observation time is now initially unknown, and ii) the observation will be activated or not according to the following constraints:
\newline

{\scriptsize 
\textbf{if} ($\iseff(f_i,a)$=\textit{true}) \textbf{then} ($\start(\obs(f_i,a,$\textit{not-}$f_j))=\tim(f_i,a)+\epsilon$) \textbf{AND}

\hspace{2.83cm}($\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{true})

\textbf{else} $\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{false}
}

\subsection{The cost-function}
There is often a set of possible action models that are {\em consistent} with the inputs of a {\em one-shot learning of temporal action models} task. A {\em cost-function} allows us to define user preferences among these action models to guide the CSP process towards the most desired solutions.

For instance the {\em conditions} of actions that are never deleted by any action, are hard to be learned with a pure satisfiabiliy aproach (e.g., the {\tt (link ?l1 ?l2)} condition in the {\tt drive-truck} action showed in the Figure~\ref{fig:exampleactions2}). In general, this is an issue when learning action models in which {\em static predicates} appear in the actions {\em conditions}~\cite{gregory2015domain}. We address this issue extending the CP formulation to not only deal with the satisfaction of constraints but also to prefer action models that support the input observations as {\em compactly} as possible. To prefer this kind of {\em compact} support of the input observations we define the following two positive functions:
\begin{enumerate}
\item[$\phi_1$] {\em No-dummy causal links}. This function counts the number of causal links created to support the provided observations with no-dummy actions. That is causal links $\tup{b,f,a}$ such that: $\obs(f,t)$ is an input observation and action $b\neq start$, i.e., the supporter is not the {\em start} dummy action.
\item[$\phi_2$] {\em Side-effects}. This function counts the number of effects that are added by the actions but that are not building any causal link. Formally, the number of $\tim(f,b)$ such that there is no $b=\supp(f,a)$ for any action $a$. 
\end{enumerate}

Our aim is to compute solutions to the CSP that minimizes functions $\phi_1$ (to prefer the creation of causal links with the given input knowledge) and $\phi_2$ (to prefer action models that tie up loos ends). To achieve this we ask the CSP solve to {\em pareto minimize} functions $\phi_1$ and $\phi_2$. 
  


\section{Flexibility of the CSP approach}
\label{sec:usingCPValidation}
This section displays the flexibility of our formulation for different tasks in the {\em temporal} planning setting (and even in the {\em classical} planning setting) and for different levels of input knowledge that express different levels of prior knowledge.

\subsection{Incomplete and complete action models}
In between having in advance a consistent value for all of the CSPs variables and not having any, there is a {\em grayscale} of different levels of prior knowledge. Focusing this discussion on our CP formulation, variables X3, X4, X5, X6 and X8 from Table~\ref{table:variables} represent the duration, the conditions and effects of a given action $a$ and their distribution over time. If this information is {\em apriori} known for a given action $a$ (e.g. because we are not learning from scratch but filling the gaps of an {\em incomplete} action model, as introduced in Section~\ref{section:learningTemporalModels}) then these variables are initially set to the given values so they can be propagated by the CSP reducing the CSP branching factor. 

Encoding {\em durative action} models with {\em finite-domain} variables allows also to compute similarity metrics between action models (e.g., to compare a learned action model with respect to a reference model). Given $\alpha(a)$, the {\em alphabet} of {\em candidates} to appear in the conditions/effect of an action $a$, the size of its space of possible action models is $\mathcal{D}\times 2^{5|\alpha(a)|}$ where $\mathcal{D}$ is the number of different possible durations for $a$ (i.e., the domain of X3). This means that the {\em conditions} and {\em effects} of an action $a$ can be compactly coded by 5 bit-vectors, each of length $|\alpha(a)|$ (three vectors representing the conditions {\em at start}, {\em overall}, {\em at end} and two vectors representing the effects {\em at start} and {\em at end}). A 0-bit in the vector represents that the corresponding {\em condition}/{\em effect} is not part of the schema while a 1-bit represents that is part of the schema. This also means that the {\em Hamming distance} can be used straightforward as a syntactic similarity metric between durative actions. In this case, the number of wrong 1-bits in the learned schema provide us a measure of the {\em incorrectness} of the learned model (number of {\em conditions} and {\em effects} that should not be in the learned model) and the number of wrong 0-bits in the learned schema provide us a measure of the {\em incompleteness} of that model (number of {\em conditions} and {\em effects} that are missing in the learned model). A similar method was already defined for {\em strips} action models~\cite{aineto2019model}. 

\subsection{Integrating planning, validation and learning}
Our CSP formulation provides an integrative view for the tasks of plan {\em synthesis}, plan {\em validation} and the learning of action models in the {\em temporal planning} setting. This integration lies on the fact that, within the same CSP formluation, we can initially set some CSP variables to known values, when they are available.

In more detail, we can reuse the CSP for a given {\em one-shot learning of temporal action models} $\mathcal{L}=\tup{F,I,G,A?,O,C}$, to solve the following tasks:
\begin{itemize}
\item {\em Plan validation}. If $A?$ is a set of complete durative actions and given a plan $\pi=\{(a_1,t_{a_1}),(a_2,t_{a_2})\ldots (a_n,t_{a_n})\}$ then, the domain of all the CSP variables (X1,X2,X3,X4,X5,X6,X7,X8) is a singleton so plan validation reduces to checking whether this full assignment of the CSP is {\em consistent} with the input constraints. In the case of {\em inconsistency}, the plan can be executed starting from the initial state until an action condition is unsatisfied to identify the source of the plan failure.
\item {\em Plan synthesis}. If $A?$ is a set of complete durative actions, then the value of the corresponding variables $X3$, $X4$, $X5$ is set as defined by $?A$ while the domains of the remaining variables is defined as explained in Section~\ref{section:learningAsCSP}. In that case solving the CSP is equivalent to solving a temporal planning task $\mathcal{P}=\tup{F,I,G,A?}$ where the observations in $\mathcal{O}$ can be understood as a sequence of ordered {\em landmarks}~\cite{hoffmann2004ordered} for $\mathcal{P}$ that are given as input (the fluents of the sets in $\mathcal{O}$ must be achieved by any plan that solves $\mathcal{P}$ and in the same order as defined in the observation $\mathcal{O}$).

\end{itemize}  
What is more, our CSP formluation allows us to validate plans despite some of the variables that representing the conditions, effects and duration of an action do not have a fixed value and despite some $(a,t_{a})$ pairs are unknown or incomplete in the input plan to validate. In such scenario the plan validation ability of our CP formulation is beyond the functionality of VAL (the standard plan validation tool~\cite{howey2004val}) since it can address plan validation of partial, or even empty, action models and with partially observed plan traces (VAL requires both a full plan and a full action model for plan validation).

Likewise, we can synthesize a plan despite some of the variables that representing the conditions, effects and duration of an action do not have a fixed value (its value is initially unknown). This goes beyond the capacities of off-the-shelf planners that require a complete action model to synthesize a plan.

Last but not least, this integrative view of the tasks of {\em plan synthesis}, {\em plan validation} and the {\em learning} of action models applies, not only to the {\em temporal planning} setting but also to {\em classical planning}, the vanilla model of AI planning where actions are instantaneous and solutions are sequence of totally ordered actions~\cite{geffner2013concise}. In more detail, our CP formulation allows to transform the presented temporal planning model into a classical planning model by initially setting for every action $dur(a)=0$ and adding the extra constraints $\start(a)=\en(a)=\reqs(a)=\reqe(a)=\tim(f,a)$.



  
\section{EVALUATION}
\label{sec:evaluation}

[DE MOMENTO ESTO ESTA EN EL AIRE PORQUE NO SABEMOS COMO LO VAMOS A ABORDAR??]

The CP formulation has been implemented in \textsf{Choco}\footnote{\texttt{http://www.choco-solver.org}}, an open-source Java library for constraint programming that provides an object-oriented API to state the constraints to be satisfied. \textsf{Choco} uses a static model of variables and constraints, i.e. it is not a DCSP.

The empirical evaluation of a learning task can be addressed from two perspectives. From a pure syntactic perspective, learning can be considered as an automated design task to create a new model that is similar to a reference (or {\em ground truth}) model. Consequently, the success of learning is an accuracy measure of how similar these two models are, which usually counts the number of differences (in terms of incorrect durations or distribution of conditions/effects). Unfortunately, there is not a unique reference model when learning temporal models at real-world problems. Also, a pure syntax-based measure usually returns misleading and pessimistic results, as it may count as incorrect a different duration or a change in the distribution of conditions/effects that really represent equivalent reformulations of the reference model. For instance, given the example of Figure~\ref{fig:exampleactions2}, the condition learned \texttt{(over all (link ?from ?to))} would be counted as a difference in action \texttt{drive-truck}, as it is \texttt{at start} in the reference model; but it is, semantically speaking, even more correct. Analogously, some durations may differ from the reference model but they should not be counted as incorrect. As seen in section~\ref{sec:simpleTask}, some learned durations cannot be granted, but the underlying model is still consistent. Therefore, performing a syntactic evaluation in learning is not always a good idea.

From a semantic perspective, learning can be considered as a classification task where we first learn a model from a training dataset, then tune the model on a validation test and, finally, asses the model on a test dataset. Our approach represents a one-shot learning task because we only use one plan sample to learn the model and no validation step is required.
Therefore, the success of the learned model can be assessed by analyzing the success ratio of the learned model \emph{vs.} all the unseen samples of a test dataset. In other words, we are interested in learning a model that fits as many samples of the test dataset as possible. This is the evaluation that we consider most valuable for learning, and define the success ratio as the percentage of samples of the test dataset that are consistent with the learned model. A higher ratio means that the learned model explains, or adequately fits, the observed constraints the test dataset imposes.

%\subsection{The CSP heuristics}
%\label{sec:implementation}
%Our CSP formulation is solver-independent, which means we do not use heuristics that require changes in the implementation of the CSP engine. Although this reduces the solver performance, we are interested in using it as a blackbox that can be easily changed with no modification in our formulation. However, the experimentation showed us that the following {\em value selection} heuristics are effective to solve the defined CSPs:
%\begin{enumerate}
%\item $\dur(a)$, lower values first, thus preferring shortest solutions that make the learned model consistent.
%\item $\reqs(f,a)$ and $\reqe(f,a)$. For $\reqs$, lower values first, whereas for $\reqe$, upper values first. This gives priority to $\cond_o(a)$, keeping conditions active as long as possible.  
%\item $\tim(f,a)$. Lower values first, for negative effects, while upper values first, for positive effects. This gives priority to $\eff_s(a)$ delete effects and $\eff_e(a)$ positive effects. 
%\item $\supp(f,a)$, lower values first to prefer supporters that start earlier in the plan.
%\end{enumerate}






\subsection{Learning from partially specified action models}

We have run experiments on nine IPC planning domains. It is important to highlight that these domains are encoded in PDDL2.1, with the number of operators shown in Table~\ref{table:evaluationExperiments}, so we have included the constraints given in section~\ref{sec:PDDL21constraints}. We first get the plans for these domains by using five planners (\textit{LPG-Quality}~\cite{gerevini2003planning}, \textit{LPG-Speed}~\cite{gerevini2003planning}, \textit{TP}~\cite{jimenez2015temporal}, \textit{TFD}~\cite{eyerich2009using} and \textit{TFLAP}~\cite{marzal2016temporal}), where the planning time is limited to 100s.
The actions and observations on each plan are automatically compiled into a CSP learning instance. Then,
%we create the CP formulation and
we run the one-shot learning task to get a temporal action model for each instance, where the learning time is limited to 100s on an Intel i5-6400 @ 2.70GHz with 8GB of RAM.
In order to assess the quality of the learned model, we validate each model \emph{vs.} the other models \emph{w.r.t.} the \emph{struct}ure, the \emph{dur}ation and the \emph{struct}ure+\emph{dur}ation, as discussed in section~\ref{sec:usingCPValidation}.
For instance, the \emph{zenotravel} domain contains 78 instances, which means learning 78 models. Each model is validated by using the 77 remaining models, thus producing 78$\times$77=6006 validations per struct, dur and struct+dur each. The value for each cell is the average success ratio.
In \emph{zenotravel}, the struct value means that the distribution of conditions/effects learned by using only one plan sample is consistent with all the samples used as dataset (100\% of the 6006 validations), which is the perfect result, as also happens in \emph{floortile} and \emph{sokoban} domains.
The dur value means the durations learned explain 68.83\% of the dataset. This value is usually lower because any learned duration that leads to inconsistency in a sample counts as a failure. The struct+dur value means that the learned model explains entirely 35.76\% of the samples. This value is always the lowest because a subtle structure or duration that leads to inconsistency in a sample counts as a failure.
As seen in Table~\ref{table:evaluationExperiments}, the results are specially good, taking into consideration that we use only one sample to learn the temporal action model.
These results depend on the domain size (number of operators, which need to be grounded), the relationships (causal links, threats and interferences) among the actions, and the size and quality of the plans.

\begin{table}
\begin{center}
\caption{Number of operators to learn. Instances used for validation. Average success ratio of the one-shot learned model \emph{vs.} the test dataset in different IPC planning domains.}
\begin{scriptsize}
\begin{tabular}{l|llrrr}
\hline	
& {\bf ops} & {\bf ins} & {\bf struct} & {\bf dur} & {\bf struct+dur}  \\\hline

\emph{zenotravel} & 5 & 78 & 100\% & 68.83\% & 35.76\% \\
\emph{driverlog} & 6 & 73 & 97.60\% & 44.86\% & 21.04\% \\
\emph{depots} & 5 & 64 & 55.41\% & 76.22\% & 23.19\% \\
\emph{rovers} & 9 & 84 & 78.84\% & 5.35\% & 0.17\% \\
\emph{satellite} & 5 & 84 & 80.74\% & 57.13\% & 40.53\% \\
\emph{storage} & 5 & 69 & 58.08\% & 70.10\% & 38.36\% \\
\emph{floortile} & 7 & 17 & 100\% & 80.88\% & 48.90\%\\
\emph{parking} & 4 & 49 & 86.69\% & 81.38\% & 54.89\% \\
\emph{sokoban} & 3 & 51 & 100\% & 87.25\% & 79.96\% \\

\end{tabular}
\end{scriptsize}
\label{table:evaluationExperiments}
\end{center}
\end{table}


We have observed that some planners return plans with unnecessary actions, which has a negative impact for learning precise durations.
%Some planners return plans with unnecessary actions, thus making an adequate learning more difficult. In particular, actions that are redundant have a negative impact for learning precise durations.
The worst result is returned in the \emph{rovers} domain, which models a group of planetary rovers to explore the planet they are on. Since there are many parallel actions for taking pictures/samples and navigation of multiple rovers, learning the duration and the structure+duration is particularly complex in this domain.

\subsection{Learning from scratch}

\section{CONCLUSIONS}
\label{sec:conclusions}



%\ack We would like to 


\bibliographystyle{ecai}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
