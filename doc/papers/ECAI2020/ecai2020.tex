\documentclass{ecai}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{array}

\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}    % precondition
\newcommand{\eff}{\mathsf{eff}}    % effect
\newcommand{\cond}{\mathsf{cond}}  % condition
\newcommand{\dur}{\mathsf{dur}}    % duration
\newcommand{\iscond}{\mathsf{is\_cond}}    % is_cond
\newcommand{\iseff}{\mathsf{is\_eff}}    % is_eff
\newcommand{\obs}{\mathsf{obs}}    % observation
\newcommand{\start}{\mathsf{start}}% start
\newcommand{\en}{\mathsf{end}}     % end
\newcommand{\til}{\mathsf{til}}    % TIL
\newcommand{\supp}{\mathsf{sup}}   % sup
\newcommand{\tim}{\mathsf{time}}   % time
\newcommand{\reqs}{\mathsf{req\_{start}}} % req_start
\newcommand{\reqe}{\mathsf{req\_{end}}}   % req_end
\newcommand{\ini}{\mathsf{init}}   % init
\newcommand{\goal}{\mathsf{goal}}  % goal


%TamaÃ±o 7+1 pages

\begin{document}
\title{One-Shot Learning of Temporal Actions Models via Constraint Programming}
\author{Antonio Garrido \and Sergio Jim\'enez}
 
%\author{Name1 Surname1 \and Name2 Surname2 \and Name3 Surname3\institute{University, Country, email: somename@university.edu} }
 
\maketitle

\begin{abstract}
  We present a {\em Constraint Programming} (CP) formulation for learning temporal planning  action models from the observation of a single plan execution ({\em one shot}). Inspired by the CSP approach to {\em temporal planning}, our CP formulation models {\em time-stamps} for states and actions, {\em causal-link} relationships, {\em threats} and effect {\em interferences} and evidences the connection between the tasks of {\em plan synthesis}, {\em plan validation} and {\em action model learning} in the temporal planning setting. The CP formulation is solver-independent so off-the-shelf CSP solvers can be used for the resolution of any of these three tasks. The performance of the CP formulation is assessed when learning and validating action models of several temporal planning domains specified in PDDL2.1. %The paper also shows that our CP formulation is flexible to accommodate a different range of expressiveness, subsuming the PDDL2.1 temporal semantics.
\end{abstract}



\section{INTRODUCTION}

{\em Temporal planning} is an expressive planning model that relaxes the assumption of instantaneous actions of {\em classical planning}~\cite{geffner2013concise}. Actions in temporal planning are called {\em durative}, because each action has an associated duration and hence, the conditions/effects of durative actions may hold/happen at different times~\cite{fox2003pddl2}. This means that {\em durative actions} can be executed in parallel and overlap in several different ways~\cite{cushing2007temporal}, and that valid solutions for temporal planning instances specify the precise time-stamp when durative actions start and end~\cite{howey2004val}.

Despite the potential of state-of-the-art planners, their application to real world problems is still somewhat limited mainly because of the difficulty of specifying correct and complete planning models~\cite{kambhampati2007model}. The more expressive the planning model, the more evident becomes this knowledge acquisition bottleneck that jeopardizes the usability of AI planning technology. With the aim of relieving this limitation there are growing efforts in the planning community for the machine learning of action models~\cite{kuvcera2018louga,MouraoZPS12,yang2007learning,zhuo2013action}. There is a  wide range of different approaches for learning classical action models from sequential plans: since pioneering learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with {\em quantifiers}~\cite{AmirC08,ZhuoYHL10}, from {\em noisy} actions or states~\cite{MouraoZPS12,zhuo2013action}, from {\em null state information}~\cite{cresswell2013}, or from {\em incomplete} domain models~\cite{ZhuoK17,ZhuoNK13}.

As far as we know this paper presents the first approach for learning action models for the {\em temporal planning} setting. While learning an action model for classical planning means computing the actions' conditions and effects that are consistent with the input observations, learning temporal action models requires additionally: i) identifying how conditions and effects are temporally distributed within the actions, and ii) estimate the action duration.

Most of the cited approaches for model learning are purely inductive and require large input datasets, e.g. hundreds of plan observations, to compute statistically significant models and focus on learning models from sequential plans for classical planning. With the aim of understanding better the connection between the learning of durative action models, {\em temporal planning}  and the validation of temporal plans, this paper follows a radically different approach and studies the singular learning scenario where just the observation of a single plan execution ({\em one-shot}) is available.

The contributions of this work are two-fold:
\begin{enumerate}  
\item The learning of action models from observations of plans with overlapping actions. This feature makes our approach appealing for learning action models from observations of multi-agent environments~\cite{furelos2018carpool}. 
\item A solver-independent CP formulation that connnects the {\em learning} of planning action models with the {\em synthesis} and the {\em validation} of temporal plans. Off-the-shelf CSP solvers can be used for any of these tasks.  
\end{enumerate}



\section{BACKGROUND}

This section formalizes the {\em temporal planning} and {\em Constraint Satisfaction} models that we follow in this work.

\subsection{Temporal Planning}
\label{sec:temporalplanning}

We assume that {\em states} are factored into a set $F$ of Boolean variables. A state $s$ is a time-stamped assignment of values to all the variables in $F$. A {\em temporal planning problem} is a tuple $P=\tup{F,I,G,A}$ where the {\em initial state} $I$ is a fully observed state (i.e. a total assigment of the state variables $|I|=|F|$) and that is time-stamped with $t=0$; $G \subseteq F$ is a conjunction of {\em goal conditions} over the variables in $F$ that defines the set of goal states; and $A$ represents the set of {\em durative actions}.

A {\em durative action} has an associated duration and may have conditions/effects on $F$ at different times~\cite{garrido2009constraint,vidal2006branching}. Like in PDDL and to compactly represent temporal planning problems, we assume that the state variables in $F$ are intantiations of a given set of predicates $\Psi$ and that durative actions in $A$ are fully grounded from {\em action schemes} (also known as {\em operators}) .  PDDL2.1 is the input representation language for the temporal track of the International Planning Competition (IPC)~\cite{fox2003pddl2,ghallab2004automated}. According to PDDL2.1, a durative action $a\in A$ is defined with the following elements:

\begin{enumerate}
\item $\dur(a)$, a positive value indicating the {\em duration} of the action.

\item $\cond_s(a), \cond_o(a), \cond_e(a)$ representing the three types of action {\em conditions}. Unlike the \emph{pre}conditions of classical actions, action conditions in PDDL2.1 must hold: before $a$ is executed ({\em at start}), during the entire execution of $a$ ({\em over all}) or when $a$ finishes ({\em at end}), respectively. 

\item $\eff_s(a)$ and $\eff_e(a)$ represent the two types of action effects. In PDDL2.1, effects can happen {\em at start} or {\em at end} of action $a$ respectively, and can be either positive or negative (i.e. asserting or retracting variables). 

\end{enumerate}

PDDL2.1 is a restricted temporal planning model that defines the semantics of a {\em durative action} $a$ as two discrete events, $\start(a)$ and $\en(a)=\start(a)+\dur(a)$. This means that if $a$ starts on state $s$ with time-stamp $\start(a)$, then $\cond_s(a)$ must hold in $s$. Ending action $a$ in state $s'$, with time-stamp $\en(a)$, means $\cond_e(a)$ must hold in $s'$. {\em Over all} conditions must hold at any state between $s$ and $s'$ or, in other words, throughout the closed interval $[\start(a)..\en(a)]$. Likewise, {\em at start} and {\em at end} effects are instantaneously applied at states $s$ and $s'$, respectively (continuous effects are not considered in this work). Figure~\ref{fig:exampleactions2} shows an example of two schemes for PDDL2.1 durative actions taken from the {\em driverlog} domain. The schema \texttt{board-truck} has a fixed duration while the duration of \texttt{drive-truck} depends on the driving time associated to the two given locations.

\begin{figure}
\begin{scriptsize}    
\begin{verbatim}
(:durative-action board-truck
  :parameters (?d - driver ?t - truck ?l - location)
  :duration (= ?duration 2)
  :condition (and (at start (at ?d ?l)) 
                  (at start (empty ?t))
                  (over all (at ?t ?l)))
  :effect (and (at start (not (at ?d ?l))) 
               (at start (not (empty ?t)))
               (at end (driving ?d ?t))))


(:durative-action drive-truck
  :parameters (?t - truck ?l1 - location ?l2 - location 
               ?d - driver)
  :duration (= ?duration (driving-time ?l1 ?l2))
  :condition (and (at start (at ?t ?l1)) 
                  (at start (link ?l1 ?l2))
                  (over all (driving ?d ?t)))
  :effect (and (at start (not (at ?t ?l1))) 
               (at end (at ?t ?l2))))
\end{verbatim}
\end{scriptsize}    
%\end{tabular}
\caption{Two action schemes of durative actions represented in PDDL2.1.}
\label{fig:exampleactions2}
\end{figure}

PDDL2.2 is an extension of the PDDL2.1 language that includes the notion of {\em Timed Initial Literal}~\cite{hoffmann2005}, denoted as $\til(f,t)$, and representing that variable $f\in F$ becomes true at a certain time $t>0$, independently of the actions in the plan~\cite{Edelkamp04}. TILs are useful to model {\em exogenous happenings}; for instance, a time window when a warehouse is open in a logistics scenario can be modeled with two timed initial literals as follows, $\til(open,8)$ and $\til($\textit{not-}$open,20)$.

A {\em temporal plan} is a set of pairs $\pi=\{(a_1,t_1),(a_2,t_2)\ldots (a_n,t_n)\}$. Each pair $(a_i,t_i)$ contains a durative action $a_i$ and the $t_i=\start(a_i)$ time-stamp. The execution of a temporal plan starting from a given initial state $I$ induces a state sequence formed by the union of all states $\{s_{t_i}, s_{t_i+\dur(a_i)}\}$, where there exists an initial state $s_{0}=I$, and a state $s_{end}$ that is the last state induced by the execution of the plan. Note then that sequential plans can be expressed as temporal plans but not the opposite. A {\em solution} to a given temporal planning problem $P$ is a temporal plan $\pi$ such that its execution, starting from the corresponding initial state, eventually reaches a state that meets the goal conditions, $G\subseteq s_{end}$. A solution is {\em optimal} iff it minimizes the plan {\em makespan} (i.e., the maximum $\en(a)=\start(a)+\dur(a)$ of an action in the plan).

\subsection{Constraint Satisfaction}
A {\em Constraint Satisfaction Problem} (CSP) is a tuple $\tup{X,D,C}$, where $X$ is a set of finite-domain {\em variables}, $D$ represents the {\em domain} for each of these variables and $C$ is a set of {\em constraints} among the variables in $X$ that bound their possible values in $D$.

A {\em solution} to a CSP as an assignment of values to all the variables in $X$ that is {\em consistent} with all the input constraints. Given a CSP there may be many different solutions to that problem, i.e., different variable assignments that are {\em consistent} with the input constraints.

A {\em cost-function} can be defined to specify user preferences about the space of possible solutions. Given a CSP and cost-function, then an {\em optimal solution} is a variable assignment that is consistent with the constraints of the CSP and minimizes the value of the defined cost-function.



\section{One-shot learning of temporal actions models}
\label{section:learningTemporalModels}
We formalize the task of the {\em one-shot learning of temporal action models} as a tuple $\mathcal{L}=\tup{F,I,G,A?,O,C}$ where:

\begin{figure}[hbt!]
\begin{scriptsize}    
\begin{verbatim}
(at driver1 loc1) (at truck1 loc1) (driving driver1 truck1) 
(empty truck1) (path loc1 loc1) (link loc1 loc1)
\end{verbatim}
\end{scriptsize}    
\caption{Set of six {\em candidates} to appear in the conditions/effects of the ground action {\tt board-truck(driver1,truck1,loc1)}.}
\label{fig:exampleCandidates}
\end{figure}  


\begin{itemize}
\item $\tup{F,I,G,A?}$ is a {\em temporal planning problem} such that the actions in $A?$ are {\em partially specified}. This means that the exact conditions/effects, their temporal annotation, and the duration of actions are unknown while the actions {\em header} (i.e., the {\em  name} and {\em parameters} of each action) is known. With this regard, we say that a fluent $f\in F$ is a {\em candidate} to appear in the condition/effects of an action $aA?$ iff $f$ appears in the set of FOL interpretations of the predicates over the action parameters $pars(a)$. For instance Figure~\ref{fig:exampleCandidates} shows the set of six {\em candidates} to appear in the conditions/effect of the ground action {\tt board-truck(driver1,truck1,loc1)}.
 
\begin{figure}[hbt!]
\begin{scriptsize}    
\begin{verbatim}
(:objects driver1 driver2 - driver
          truck1 truck2 - truck
          package1 package2 - obj
          s0 s1 s2 p1-0 p1-2 - location)

(:init	(at driver1 s2)	(at driver2 s2) (at truck1 s0)
        (empty truck1) (at truck2 s0) (empty truck2) 
        (at package1 s0) (at package2 s0)
        (path s1 p1-0) (path p1-0 s1) (path s0 p1-0) 
        (path p1-0 s0) (path s1 p1-2) (path p1-2 s1)
        (path s2 p1-2) (path p1-2 s2)
        (link s0 s1) (link s1 s0) (link s0 s2) (link s2 s0)
        (link s2 s1) (link s1 s2))

(:observation :time-stamp 56
              (at driver1 s1) (at truck1 s1))

(:observation :time-stamp 78
              (at package1 s0) (at package2 s0))
\end{verbatim}
\end{scriptsize}    
\caption{Example of a set of three observations (containing the fully observed initial state and two time-stamped partial states) extracted from the execution of a plan from the {\em driverlog} domain.}
\label{fig:exampleObservations}
\end{figure}  

 
\item $O$ is the set of {\em observations} over a plan execution. Al least this set contains a full observation of the initial state (time-stamped with $t=0$) and a final state observation that equals the goals $G$ of the temporal planning problem (time-stamped with $t_{end}$, the makespan of the observed plan). Additionally, it can contain time-stamped observations of traversed intermediate {\em partial states}\footnote{In this work, not all variables can be observed at any time; that is, we deal with {\em partial observations} (e.g. just a subset of variables is observable by associated sensors). Observations are noiseless, which means that if a value is observed, that is the actual value of that variable.} (e.g., $\obs(f,t)$ denotes that $f \in F$ was observed at time $t$) and the times when actions start and/or end their execution. For instance, $\obs(is\_start(a),t_i)$  represents that action $a$ starts at $t_i$ while $\obs(is\_end(a),t_j)$ represents that action $a$ ends at $t_j$. Figure~\ref{fig:exampleObservations} shows an example of the observation of a plan execution taken from the {\em driverlog} domain.

 \begin{figure}
\begin{scriptsize}    
\begin{tabular}{p{2.7cm}l}
$\forall truck,driver:$ & $\neg empty(truck)\vee\neg driving(driver,truck)$.\\
  $\forall driver,loc_1,loc_2:$ & $\neg at(driver,loc_1)\vee\neg at(driver,loc_2),$ \\
  & $\neq (loc_1,loc_2)$.\\
  $\forall driver, truck1, truck2:$  & $\neg driving(driver,truck_1)\vee$\\
  & $\neg driving(driver,truck_2),\neq (truck_1,truck_2)$.\\
  $\forall driver1, driver2, truck:$  & $\neg driving(driver_1,truck)\vee$\\
  &$\neg driving(driver_2,truck), \neq (driver_1,driver_2)$.\\
$\forall driver,location,truck:$ & $\neg at(driver,location)\vee\neg driving(driver,truck)$.
\end{tabular}  
\end{scriptsize}    
\caption{Examples of five mutex constraints for the {\em driverlog} domain.}
\label{fig:example-statecs}
\end{figure}


\item $C$ is a set of {\em constraints} that captures domain-specific expert knowledge. In this work these constraints are mutually-exclusive ({\em mutex}) constraints that allow us to (1), deduce new observations and (2), prune action models inconsistent with these constraints. Figure~\ref{fig:example-statecs} shows an example of a set of five mutex constraints for the {\em driverlog} domain. 
\end{itemize}

A {\em solution} for the learning task $\mathcal{L}$ is a fully specified model of durative actions $\mathcal{A}$ such that the {\em conditions}, {\em effects}, their temporal annotations and the {\em duration} of any action in $\mathcal{A}$ are: i) completely specified; and ii) {\em consistent} with $\mathcal{L}=\tup{F,I,G,A?,O,C}$. By {\em consistent} we mean that there exists a valid plan that exclusively contains actions in $\mathcal{A}$ and whose execution starting in $I$, produces all the observations in $O$ at the associated time-stamps, while it satisfies all constraints in $C$, and reaches a final state that satisfies $G$.



\section{One-Shot learning of action models with CSPs}
\label{section:learningAsCSP}
Given a one-shot learning task $\mathcal{L}$, as defined in Section~\ref{section:learningTemporalModels}, we automatically create a CSP, whose solution induces an action model that solves $\mathcal{L}$. This method is solver-independent and integrates well with previous work on {\em temporal planning} as CP~\cite{garrido2009constraint,vidal2006branching}. 

\begin{table}
\begin{center}
\caption{The CSP variables, their domains and semantics.}
\begin{scriptsize}
%\begin{tabular}{llll}
\begin{tabular}{p{0.3cm}p{1.3cm}p{1.5cm}p{3.8cm}}
\hline	
{\bf ID} & {\bf Variable} & {\bf Domain} & {\bf Description} \\
\hline
X1 &$\start(a)$ & $[0..t_{end}]$ & {\em Start time} of action $a$ \\
X2 &$\en(a)$ & $[0..t_{end}]$ & {\em End time} of action $a$ \\
X3 &$\dur(a)$ & $[0..t_{end}]$ & {\em Duration} of action $a$ \\

X4 &$\iscond(f,a)$ & $\{0,1\}$ & 1 if $f$ is a {\em condition} of $a$; 0 otherwise \\
X5 &$\iseff(f,a)$ & $\{0,1\}$ & 1 if $f$ is an {\em effect} of $a$; 0 otherwise \\

X6.1 &$\reqs(f,a)$, &  & \\ 
X6.2 &$\reqe(f,a)$  & $[0..t_{end}]$ & Interval when action $a$ requires $f$ \\

X7 &$\supp(f,a)$ & $\{b\}_{b\in A?} \cup \emptyset $&  Supporters for causal link $\tup{b,f,a}$ \\ 
X8 &$\tim(f,a)$ & $[0..t_{end}]$ & Time when the effect $f$ of $a$ happens\\

\end{tabular}
\end{scriptsize}
\label{table:variables}
\end{center}
\end{table}

\subsection{The CSP variables}
For each action $a\in A?$ and {\em candidate} $f$ to appear in the conditions/effects of $a$, we create the following eight CSP variables that are shown in Table~\ref{table:variables}. 

Variables X1 and X2 represent the times when a given action {\em starts} and {\em ends} while variable X3 represents the duration of that action. For simplicity, we model time in $\mathbb{Z}^+$ and bound all maximum times to the {\em makespan} of the observed plan ($t_{end}$ observed in $O$). If the observation of $t_{end}$ is unavailable, we consider a large enough domain for time. The value of these three variables (X1,X2 and X3) can be either observed in $O$ or derived from the expression $\en(a)=\start(a)+\dur(a)$. Variables X4 and X5 are Boolean variables modeling whether $f$ is actually a condition of $a$ and whether $f$ is an effect of $a$. Variables X6.1 and X6.2 model the closed interval throughout condition $f$ must hold, provided that $\iscond(f,a)$=\textit{true}. Variable X7 models {\em causal links} representing that actions $b$ support $f$ that is required by $a$. If $f$ is not a condition of $a$ ($\iscond(f,a)$=\textit{false}) then $\supp(f,a)$=$\emptyset$, thus representing an empty supporter. Last but not least variable X8 models the time-stamp when effect $f$ happens in $a$, provided $\iseff(f,a)$=\textit{true}.

This formulation is able to model $\til$ and {\em observations}. The intuition is that modeling a $\til$ is analogous to modeling the full observation of the initial state (both represent information that is given at a particular time but externally to the execution of the plan). Likewise modeling an observation is analogous to the modeling of the $\goal$ of the planning task, as they both represent conditions that must be satisfied in the execution of the plan at a particular time. On the one hand, $\til(f,t)$ is modeled as a {\em dummy} action that starts at time $t$ and has instantaneous duration ($\start(\til(f,t))=t$ and $\dur(\til(f,t))=0$) with no conditions and the single effect $f$ that happens at time $t$ ($\iseff(f,\til(f,t))$=\textit{true} and $\tim(f,\til(f,t))=t$). On the other hand, $\obs(f,t)$ is modeled as another {\em dummy} action that also starts at time $t$ and has instantaneous duration ($\start(\obs(f,t))=t$ and $\dur(\obs(f,t))=0$) but with only one condition $f$, which is the value observed for fact $f$ ($\iscond(f,\obs(f,t))$=\textit{true}, $\supp(f,\obs(f,t))\neq \emptyset$ and $\reqs(f,\obs(f,t))=\reqe(f,\obs(f,t))=t$), and no effects at all. 

Our variable formulation is accommodating a level of expressiveness beyond PDDL2.1 since it allows conditions/effects to be at any time, even outside the execution of the action. For example, we allow a condition $f$ to hold in $\start(a)\pm$2: $\reqs(f,a)=\start(a)-2$ and $\reqe(f,a)=\start(a)+2$. An effect $f$ might also happen after the action ends e.g., $\tim(f,a)=\en(a)+2$. 

\begin{table*}
\begin{center}
\caption{The CSP constraints and a brief description.}	
\begin{scriptsize}
%\begin{tabular}{lp{10.2cm}p{6.4cm}}
\begin{tabular}{p{0.1cm}p{10.2cm}p{6.4cm}}
\hline
{\bf ID}&{\bf Constraint}&{\bf Description}\\\hline
			
C1& $\en(a)=\start(a)+\dur(a)$ & Relationship among start, end and duration of $a$ \\

C2& $\en(a) \leq \start(\goal)$ & Always $\goal$ is the last action of the plan \\

C3& \textbf{iff} ($\iscond(f,a)$=\textit{false}) \textbf{then} $\supp(f,a) = \emptyset$ & $f$ is not a condition of $a \iff $ the supporter of $f$ in $a$ is $\emptyset$ \\

C4& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{then} $\reqs(f,a) \leq \reqe(f,a)$ & [$\reqs(f,a)..\reqe(f,a)$] is a valid interval\\

C5& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$))  & Modeling the causal link $\tup{b,f,a}$: supporting $f$ before it is \\
&\hspace{0.2cm}\textbf{then} $\tim(f,b) < \reqs(f,a)$ & required (obviously $b \neq \emptyset$) \\

C6& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{AND} ($\supp(f,a)=b$) & Solving threat of $c$ to causal link $\tup{b,f,a}$ by promotion or \\
&\hspace{0.2cm}\textbf{AND} ($c \neq a$) \textbf{then} ($\tim($\textit{not-f}$,c) < \tim(f,b)$) \textbf{OR} ($\tim($\textit{not-f}$,c) > \reqe(f,a)$) & demotion (obviously $b \neq \emptyset$) \\

C7& \textbf{if} ($\iseff(f,a)$=\textit{false}) \textbf{then forall} $b$ that requires $f$: $\supp(f,b) \neq a$ & $a$ cannot be a supporter of $f$ for any other action $b$\\

C8& \textbf{if} ($\iscond(f,a)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,a)$=\textit{true}) \textbf{then} $\tim($\textit{not-f}$,a) \geq \reqe(f,a)$ & $a$ requires and deletes $f$: the condition holds before the effect \\ 

C9& \textbf{if} ($\iseff(f,b)$=\textit{true}) \textbf{AND} ($\iseff($\textit{not-f}$,c)$=\textit{true}) \textbf{then} $\tim(f,b) \neq \tim($\textit{not-f}$,c)$ & Solving effect interference at the same time ($f$ and \textit{not-}$f$) \\

C10& \textbf{forall} condition $f_i$ and effect $f_j$ of $a$: $\sum \iscond(f_i,a) \geq 1$ \textbf{AND} $\sum \iseff(f_j,a) \geq 1$ & Every non-dummy action has at least one condition/effect \\

\end{tabular}
\end{scriptsize}	
\label{table:constraints}
\end{center}	
\end{table*}

\subsection{The CSP constraints}
\label{section:CSPconstraints}
Table~\ref{table:constraints} shows the constraints defined among the CSP variables of Table~\ref{table:variables}. 

Constraint C1 represents the duration of an action. Constraint C2 indicates that actions must end before $t_{end}$. C3 is a double implication meaning that supporters are only required by action conditions. Constraint C4 forces to have valid values for the $[\reqs,\reqe]$ interval that defines when conditions are required. Constraint C5 models that the time when $b$ supports $f$ must be before $a$ requires because of the causal link $\tup{b,f,a}$\footnote{$\tim(f,b) < \reqs(f,a)$ and not $\leq$ because, like in PDDL2.1~\cite{fox2003pddl2}, our temporal planning model assumes $\epsilon > 0$ ($\epsilon$ denotes a small tolerance that implies no collision between the time when effect $f$ is supported and when it is required). When time is modeled in $\mathbb{Z}^+$, $\epsilon=1$ so $\leq$ becomes $<$.}. Given a causal link $\tup{b,f,a}$, constraint C6 avoids threats of actions $c$ deleting $f$. The thread is solved via {\em promotion} or {\em demotion}~\cite{ghallab2004automated}, which means bringing $\tim($\textit{not-}$f,c)$ backward or forward, respectively, in time. Constraint C7 avoids action $a$ from being a supporter of $f$ when $\iseff(f,a)$=\textit{false}. Constraint C8 models the fact that the same action requires and deletes $f$; then the effect cannot happen before the condition. Note the $\geq$ inequality here: if one condition and one effect of the same action happen at the same time, the underlying semantics in planning considers the condition is checked instantly before the effect~\cite{fox2003pddl2}. Constraint C9 prevents two actions have contradictory effects. Constraint C10 forces actions to have at least one condition and one effect (actions without effects are unnecessary for any plan). Constraint C9 applies to any type of action, including the dummy actions ($\ini$, $\goal$, $\til$ and $\obs$), while constraint C10 only applies to {\em non-dummy} actions. 

Some conditions of Table~\ref{table:constraints} are redundant. For instance C5 and C6, $\supp(f,a)=b$ means obligatorily $\iseff(f,b)=$ \textit{true}. We include them here to define an homogeneous formulation but they are not included in our implementation. For simplicity, the value of some unnecessary variables is not bounded in the table. For instance, if $\iscond(f,a)$=\textit{false}, variables $\reqs(f,a)$ and $\reqe(f,a)$ become useless.

\begin{table}
\begin{center}   
\caption{Constraints to learn PDDL2.1-compliant action models.}	
\begin{scriptsize}
%\begin{tabular}{ll}
\begin{tabular}{p{0.4cm}p{7cm}}
\hline	
{\bf ID} &{\bf Constraint} \\ %& {\bf Description} \\
\hline
C11.1& ($\reqs(f,a) = \start(a)$) \textbf{OR} ($\reqs(f,a) = \en(a)$) \\% & Conditions at start\\
C11.2& ($\reqe(f,a) = \start(a)$) \textbf{OR} ($\reqe(f,a) = \en(a)$) \\% & Conditions at end\\
C12& ($\tim(f,a) = \start(a)$) \textbf{OR} ($\tim(f,a) = \en(a)$) \\ %& Effects at start or at end\\
C13.1& $\forall f_i: (\forall a_j: \reqs(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%& Conditions of the schema instantiations\\
&\hspace{0.65cm}$(\forall a_j: \reqs(f_i,a_j) = \en(a_j))$ \\
C13.2& $\forall f_i: (\forall a_j: \reqe(f_i,a_j) = \start(a_j))$ \textbf{OR} \\
&\hspace{0.65cm}$(\forall a_j: \reqe(f_i,a_j) = \en(a_j))$ \\
C14& $\forall f_i: (\forall a_j: \tim(f_i,a_j) = \start(a_j))$ \textbf{OR} \\%  & Effects of the schema instantiations\\
&\hspace{0.65cm}$(\forall a_j: \tim(f_i,a_j) = \en(a_j))$ \\
C15& $\forall a_i,a_j$ occurrences of the same action: $\dur(a_i) = \dur(a_j)$ \\ %& Duration of the schema instantiations\\
C16 &$\sum_{i=1}^{n} \tim(f_i,a) > n \times \start(a)$ 

\end{tabular}
\end{scriptsize}
\label{table:21constraints}
\end{center}
\end{table}

\subsubsection{Constraints for the PDDL2.1 model}
\label{sec:PDDL21constraints}
Here we show that making the presented CP formulation PDDL2.1-compliant is straightforward, by adding the constraints of Table~\ref{table:21constraints} for all {\em non-dummy} actions.

Constraints C11 limit the {\em conditions} of an action to be only at \emph{at start}, \emph{over all} or \emph{at end}. Likewise, constraint C12 limits the {\em effects} of an action to only happen \emph{at start} or \emph{at end}. In PDDL2.1 the structure of conditions/effects of all actions $\{a_j\}$ grounded from a particular operator are fixed. With this regard, constraints C13 makes the conditions of all $\{a_j\}$ equal and constraint C14 makes the effects of all $\{a_j\}$ equal. Constraint C15 makes the duration of all occurrences of the same action equals. Last but not least, constraint C16 forces all actions to have at least one of its \textit{n}-effects \textit{at end}. Actions with only \textit{at start} effects would turn the value of the duration irrelevant and they could exceed the plan makespan. Although this constraint is not specific of PDDL2.1, we include it to learn more rational {\em durative actions}.


\subsubsection{Mutex constraints}
The set of mutexes that is given to a learning task $\mathcal{L}$ allows to infer new information in form of {\em dynamic observations}. In more detail, if two Boolean variables $\tup{f_i, f_j}$ are mutex they cannot hold simultaneously. This means that if we observe $f_i$, then we can infer $\neg f_j$ (despite $\neg f_j$ was not observed). This source of knowledge is specially relevant for the learning of {\em negative effects} when thre is absence of many observations. Mutex information helps to fill this void by inferring the observation of negated variables, which forces later to satisfy the {\em causal links} of negative variables. 

Given a $\tup{f_i, f_j}$ mutex, in our {\em temporal planning} model, $\neg f_i$ does not necessarily implies $f_j$. To illustrate this see action \texttt{drive-truck} of Figure~\ref{fig:exampleactions2}, where \texttt{(at ?t ?l1)} and \texttt{(at ?t ?l2)} are mutex (as defined in Figure~\ref{fig:example-statecs}). Effects \texttt{(not (at ?t ?l1))} and \texttt{(at ?t ?l2)} happen \textit{at start} and \textit{at end}, respectively. This means that the same truck cannot be in two locations simultaneously, but it is valid that the truck is, for some time, at no location (i.e., the truck is at some location \texttt{l2} some time after \textit{not being} at location \texttt{l1}). These situations do not happen in STRIPS, where actions have instantaneous effects, so if $\tup{f_i, f_j}$ are mutex $f_i$ implies \textit{not-}$f_j$ and vice versa.

Mutex-constraints can be exploited in a pre-proces step for completing the input observations given in a one-shot learning task $\mathcal{L}$. Furthermore, {\em dynamic observations} can be created to exploit the mutex constraints for any generated intermediate state. This include states that where not observed but that are inferred by the CSP solutions. Given a mutex $\tup{f_i, f_j}$ it means that, immediately after $a$ asserts $f_i$, we need to ensure the observation \textit{not-}$f_j$. This is done while performing the CSP search, and if $\iseff(f_i,a)$ takes the value \textit{true}, then the next observation is added: $\obs($\textit{not-}$f_j,\tim(f_i,a)+\epsilon)$. The time of the observation cannot be just $\tim(f_i,a)$, as we first need to assert $f_i$ and one $\epsilon$ later observe \textit{not-}$f_j$. Adding the variables and constraints for this new observation is trivial for {\em Dynamic CSPs} (DCSPs), in which the original formulation can be altered. Otherwise, we need to statically define a new type of observation $\obs(f_i,a,$\textit{not-}$f_j)$, where $a$ supports $f_i$ which is mutex with $f_j$ and, consequently, we will need to observe \textit{not-}$f_j$. The difference \textit{w.r.t.} an original $\obs$ is twofold: i) the observation time is now initially unknown, and ii) the observation will be activated or not according to the following constraints:
\newline

{\scriptsize 
\textbf{if} ($\iseff(f_i,a)$=\textit{true}) \textbf{then} ($\start(\obs(f_i,a,$\textit{not-}$f_j))=\tim(f_i,a)+\epsilon$) \textbf{AND}

\hspace{2.83cm}($\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{true})

\textbf{else} $\iscond($\textit{not-}$f_j,\obs(f_i,a,$\textit{not-}$f_j))$=\textit{false}
}



%The learning task addressed in the paper aims to specify the {\em conditions} and {\em effects} that correspond to the action schemes of a given temporal planning domain represented in the PDDL2.1 language. Like in PDDL, we assume then that the set $F$ of state variables is given by the instantiation of a given set of predicates $\Psi$. We denote as ${\mathcal I}_{\xi,\Psi}$ the {\em vocabulary} (set of symbols) that can appear in the conditions and effects of a given durative action schema $\xi$, with parameters $pars(\xi)$. This set is formally defined as the FOL interpretations of predicates $\Psi$, over the action parameters $pars(\xi)$. 

%For a durative action schema $\xi$, the size of its space of possible action models is then $\mathcal{D}\times 2^{5\times|{\mathcal I}_{\xi,\Psi}}|$ where $\mathcal{D}$ is the number of different possible durations for any action shaped by the $\xi$ schema. Note that this space is significantly larger than for learning classical STRIPS actions~\cite{yang2007learning}, where this number is bound to $2^{2\times|{\mathcal I}_{\xi,\Psi}}|$ forcing that negative effects must also be preconditions of the same action and cannot be positive effects of that action.

%Provided the vocabulary, then the {\em conditions} and {\em effects} of a given durative action schema can be compactly coded by 5 bit-vectors, each of length $|{\mathcal I}_{\xi,\Psi}|$. A 0-bit in the vector represents that the corresponding {\em condition}/{\em effect} is not part of the schema while a 1-bit represents that is part of the schema. This also means that the {\em Hamming distance} can be used straightforward as a sintactic similarity metric for durative schemes. For instance, we can use the {\em Hamming distance} to compare a learned action model with respect to a given reference model that serves as baseline. In this case, the number of wrong 1-bits in the learned schema provide us a measure of the {\em incorrectness} of the learned model (number of {\em conditions} and {\em effects} that should not be in the learned model) and the number of wrong 0-bits in the learned schema provide us a measure of the {\em incompleteness} of that model (number of {\em conditions} and {\em effects} that are missing in the learned model). 

\subsection{The CSP cost functions}
The {\em conditions} of actions that are not deleted by any action are specialy difficult to be learned with a CSP that exclusively implements a pure satisfiabiliy aproach. This is an issue when attempting to learn action models in which {\em static predicates} appear in the action {\em conditions}~\cite{gregory2015domain}. For instance the {\tt (link ?l1 ?l2)} condition of the {\tt drive-truck} action showed in the Figure~\ref{fig:exampleactions2}.

This issue can be addressed extending the CP formulation to not only deal with the satisfaction of hard constraints but also to optimize a given cost function that defines the user preferences among different possible solutions. In more detail, the cost function is used to produce solutions that are {\em consistent} with the given input knowledge of the one-shot learning task $\mathcal{L}$ but to prefer solutions that support the input observations in a way that is as {\em tight} as possible.

To prefer this kind of {\em tight} support of the input observations we define the following two positive functions:
\begin{enumerate}
\item[$\phi_1$] {\em Causal-links}. This function counts the number of causal links that are created to support the provided observations.
\item[$\phi_2$] {\em Side-effects}. This function counts the number of possitive effects that are added by the actions in a plan but that do not build any causal link.  
\end{enumerate}

Our aim is to compute solutions to the CSP that minimize function $\phi_1$ while function $\phi_2$. is maximized. To achieve this we ask the CSP solve to {\em pareto optimize} functions $\phi_2$ and $-\phi_1$(i.e. the negation of function 1). 
  


\section{A UNIFIED CP FORMULATION FOR PLANNING, VALIDATION AND LEARNING}
\label{sec:usingCPValidation}
Our CP formulation is connected to the tasks of plan {\em synthesis} and plan {\em validation} in the {\em temporal planning} settting. This connection lies on the fact that we can constrain the domain of the variables of our CP formulation to given known values. This feature is useful to leverage a priori knowledge of a given planning domain. For instance, because we have some available {\em prior knowledge} about the possible durations of a given action or because we already know that a given action produces for sure certain effects or requires some conditions. In this case the value of the corresponding variables is a priori specified while the remaining variables are then regular variables whose value will be determined solving the CSP.

If all the variables that represent the conditions, effects and duration of the actions are are a priori constrained to a single value (variables X3, X4 and X5) then solving the CSP is equivalent to solving a temporal planning task (that is synthesizing a plan that reaches a set of goals from certain initial stat and with a given action model). Likewise, if all the variables that represent when the different actions appear in a solution plan (when the start times of actions happen, variables X1,X2 and X3) then solving the CSP is equivalent to validating a plan in a given temporal planning problem.

What is more, we can either synthesize (or validate) a plan despite some of the variables that representing the conditions, effects or duration of an action do not have a fixed value (its value is initially unknown). That is planning (and validating plans) when the action model is partially specified. Therefore, that the plan validation ability of our CP formulation is beyond the functionality of VAL (the standard plan validation tool~\cite{howey2004val}) since it can address plan validation of partial, or even empty, action models and with partially observed plan traces (VAL requires both a full plan and a full action model for plan validation).

To wrap up, when addressing either {\em learning}, {\em planning} or {\em validating} tasks, our formulation is flexible to accept different levels of specification of the input knowledge:

\begin{itemize}
\item Partial knowledge of the conditions/effects of actions.
\item Partial knowledge of actions durations (i.e. a set of possible durations).
\item Partial knowledge of the plan to validate or synthesize.
\end{itemize}

To illustrate this, let us assume that the distribution of all (or just a few) conditions and/or effects is known and, in consequence, represented in the model $A?$ of $\mathcal{L}$. If a solution to the CSP is found, then that structure of conditions/effects is consistent for the learned model. On the contrary, if no solution is found that structure is inconsistent and cannot be explained. We can also represent known values for the durations by bounding the value of $\dur(a)$ variables to a given value. We can also introduce a priori knowledge about plans by bounding the value of the $\start(a)$ variables.

Last but not least this connection applies not only to temporal planning but also to the classical planning model, the vanilla model of AI planning where actions are instantaneous~\cite{geffner2013concise}. 





  
\section{EVALUATION}
\label{sec:evaluation}

[DE MOMENTO ESTO ESTA EN EL AIRE PORQUE NO SABEMOS COMO LO VAMOS A ABORDAR??]

The CP formulation has been implemented in \textsf{Choco}\footnote{\texttt{http://www.choco-solver.org}}, an open-source Java library for constraint programming that provides an object-oriented API to state the constraints to be satisfied. \textsf{Choco} uses a static model of variables and constraints, i.e. it is not a DCSP.

The empirical evaluation of a learning task can be addressed from two perspectives. From a pure syntactic perspective, learning can be considered as an automated design task to create a new model that is similar to a reference (or {\em ground truth}) model. Consequently, the success of learning is an accuracy measure of how similar these two models are, which usually counts the number of differences (in terms of incorrect durations or distribution of conditions/effects). Unfortunately, there is not a unique reference model when learning temporal models at real-world problems. Also, a pure syntax-based measure usually returns misleading and pessimistic results, as it may count as incorrect a different duration or a change in the distribution of conditions/effects that really represent equivalent reformulations of the reference model. For instance, given the example of Figure~\ref{fig:exampleactions2}, the condition learned \texttt{(over all (link ?from ?to))} would be counted as a difference in action \texttt{drive-truck}, as it is \texttt{at start} in the reference model; but it is, semantically speaking, even more correct. Analogously, some durations may differ from the reference model but they should not be counted as incorrect. As seen in section~\ref{sec:simpleTask}, some learned durations cannot be granted, but the underlying model is still consistent. Therefore, performing a syntactic evaluation in learning is not always a good idea.

From a semantic perspective, learning can be considered as a classification task where we first learn a model from a training dataset, then tune the model on a validation test and, finally, asses the model on a test dataset. Our approach represents a one-shot learning task because we only use one plan sample to learn the model and no validation step is required.
Therefore, the success of the learned model can be assessed by analyzing the success ratio of the learned model \emph{vs.} all the unseen samples of a test dataset. In other words, we are interested in learning a model that fits as many samples of the test dataset as possible. This is the evaluation that we consider most valuable for learning, and define the success ratio as the percentage of samples of the test dataset that are consistent with the learned model. A higher ratio means that the learned model explains, or adequately fits, the observed constraints the test dataset imposes.

%\subsection{The CSP heuristics}
%\label{sec:implementation}
%Our CSP formulation is solver-independent, which means we do not use heuristics that require changes in the implementation of the CSP engine. Although this reduces the solver performance, we are interested in using it as a blackbox that can be easily changed with no modification in our formulation. However, the experimentation showed us that the following {\em value selection} heuristics are effective to solve the defined CSPs:
%\begin{enumerate}
%\item $\dur(a)$, lower values first, thus preferring shortest solutions that make the learned model consistent.
%\item $\reqs(f,a)$ and $\reqe(f,a)$. For $\reqs$, lower values first, whereas for $\reqe$, upper values first. This gives priority to $\cond_o(a)$, keeping conditions active as long as possible.  
%\item $\tim(f,a)$. Lower values first, for negative effects, while upper values first, for positive effects. This gives priority to $\eff_s(a)$ delete effects and $\eff_e(a)$ positive effects. 
%\item $\supp(f,a)$, lower values first to prefer supporters that start earlier in the plan.
%\end{enumerate}



\subsection{Learning from partially specified action models}

We have run experiments on nine IPC planning domains. It is important to highlight that these domains are encoded in PDDL2.1, with the number of operators shown in Table~\ref{table:evaluationExperiments}, so we have included the constraints given in section~\ref{sec:PDDL21constraints}. We first get the plans for these domains by using five planners (\textit{LPG-Quality}~\cite{gerevini2003planning}, \textit{LPG-Speed}~\cite{gerevini2003planning}, \textit{TP}~\cite{jimenez2015temporal}, \textit{TFD}~\cite{eyerich2009using} and \textit{TFLAP}~\cite{marzal2016temporal}), where the planning time is limited to 100s.
The actions and observations on each plan are automatically compiled into a CSP learning instance. Then,
%we create the CP formulation and
we run the one-shot learning task to get a temporal action model for each instance, where the learning time is limited to 100s on an Intel i5-6400 @ 2.70GHz with 8GB of RAM.
In order to assess the quality of the learned model, we validate each model \emph{vs.} the other models \emph{w.r.t.} the \emph{struct}ure, the \emph{dur}ation and the \emph{struct}ure+\emph{dur}ation, as discussed in section~\ref{sec:usingCPValidation}.
For instance, the \emph{zenotravel} domain contains 78 instances, which means learning 78 models. Each model is validated by using the 77 remaining models, thus producing 78$\times$77=6006 validations per struct, dur and struct+dur each. The value for each cell is the average success ratio.
In \emph{zenotravel}, the struct value means that the distribution of conditions/effects learned by using only one plan sample is consistent with all the samples used as dataset (100\% of the 6006 validations), which is the perfect result, as also happens in \emph{floortile} and \emph{sokoban} domains.
The dur value means the durations learned explain 68.83\% of the dataset. This value is usually lower because any learned duration that leads to inconsistency in a sample counts as a failure. The struct+dur value means that the learned model explains entirely 35.76\% of the samples. This value is always the lowest because a subtle structure or duration that leads to inconsistency in a sample counts as a failure.
As seen in Table~\ref{table:evaluationExperiments}, the results are specially good, taking into consideration that we use only one sample to learn the temporal action model.
These results depend on the domain size (number of operators, which need to be grounded), the relationships (causal links, threats and interferences) among the actions, and the size and quality of the plans.

\begin{table}
\begin{center}
\caption{Number of operators to learn. Instances used for validation. Average success ratio of the one-shot learned model \emph{vs.} the test dataset in different IPC planning domains.}
\begin{scriptsize}
\begin{tabular}{l|llrrr}
\hline	
& {\bf ops} & {\bf ins} & {\bf struct} & {\bf dur} & {\bf struct+dur}  \\\hline

\emph{zenotravel} & 5 & 78 & 100\% & 68.83\% & 35.76\% \\
\emph{driverlog} & 6 & 73 & 97.60\% & 44.86\% & 21.04\% \\
\emph{depots} & 5 & 64 & 55.41\% & 76.22\% & 23.19\% \\
\emph{rovers} & 9 & 84 & 78.84\% & 5.35\% & 0.17\% \\
\emph{satellite} & 5 & 84 & 80.74\% & 57.13\% & 40.53\% \\
\emph{storage} & 5 & 69 & 58.08\% & 70.10\% & 38.36\% \\
\emph{floortile} & 7 & 17 & 100\% & 80.88\% & 48.90\%\\
\emph{parking} & 4 & 49 & 86.69\% & 81.38\% & 54.89\% \\
\emph{sokoban} & 3 & 51 & 100\% & 87.25\% & 79.96\% \\

\end{tabular}
\end{scriptsize}
\label{table:evaluationExperiments}
\end{center}
\end{table}


We have observed that some planners return plans with unnecessary actions, which has a negative impact for learning precise durations.
%Some planners return plans with unnecessary actions, thus making an adequate learning more difficult. In particular, actions that are redundant have a negative impact for learning precise durations.
The worst result is returned in the \emph{rovers} domain, which models a group of planetary rovers to explore the planet they are on. Since there are many parallel actions for taking pictures/samples and navigation of multiple rovers, learning the duration and the structure+duration is particularly complex in this domain.

\subsection{Learning from scratch}

\section{CONCLUSIONS}
\label{sec:conclusions}


We have presented a purely declarative CP formulation, which is independent of any CSP solver, to address the learning of temporal action models. Learning in planning is specially interesting to recognize past behavior in order to predict and anticipate actions to improve decisions.
The main contribution is a simple formulation that is automatically derived from the actions and observations on each plan execution, without the necessity of specific hand-coded domain knowledge. It is also flexible to support a very expressive temporal planning model, though it can be easily modified to be PDDL2.1-compliant.
Formal properties are inherited from the formulation itself and the CSP solver. The formulation is correct because the definition of constraints to solve causal links, threats and effect interferences are supported, which avoids contradictions. It is also complete because the solution needs to be consistent with all the imposed constraints, while a complete exploration of the domain of each variable returns all the possible learned models in the form of alternative consistent solutions.

Unlike other approaches that need to learn from datasets with many samples, we perform a one-shot learning. This reduces both the size of the required datasets and the computation time. The one-shot learned models are very good and explain a high number of samples in the datasets used for testing. Moreover, the same CP formulation is valid for learning and for validation, by simply adding constraints to the variables. This is an advantage, as the same formulation allows us to carry out different tasks: from entirely learning, partial learning/validation (structure and/or duration) to entirely plan validation.
According to our experiments, learning the structure of the actions in a one-shot way leads to representative enough models, but learning the precise durations is more difficult, and even impossible, when many actions are executed in parallel.

Our CP formulation can be adapted straightforward to address learning, planning or validation tasks within the classical planning model. In this case actions cannot have conditions {\em overall} or {\em at end} as well as they cannot have {\em at start} effects. Therefore the variables representing this kind of information can be removed from the CSP model (or be set to {\tt false}). Further the duration of any action is fixed to one unit~\cite{jimenez2015temporal}.  Finally, our CP formulation can be represented and solved by Satisfiability Modulo Theories, which is part of our current work. 


%\ack We would like to 


\bibliographystyle{ecai}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
