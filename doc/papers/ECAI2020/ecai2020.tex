\documentclass{ecai}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{amssymb}
\usepackage{array}

\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}    % precondition
\newcommand{\eff}{\mathsf{eff}}    % effect
\newcommand{\cond}{\mathsf{cond}}  % condition
\newcommand{\dur}{\mathsf{dur}}    % duration
\newcommand{\obs}{\mathsf{obs}}    % observation
\newcommand{\start}{\mathsf{start}}% start
\newcommand{\en}{\mathsf{end}}     % end
\newcommand{\til}{\mathsf{til}}    % TIL
\newcommand{\supp}{\mathsf{sup}}   % sup
\newcommand{\tim}{\mathsf{time}}   % time
\newcommand{\reqs}{\mathsf{req\_{start}}} % req_start
\newcommand{\reqe}{\mathsf{req\_{end}}}   % req_end
\newcommand{\ini}{\mathsf{init}}   % init
\newcommand{\goal}{\mathsf{goal}}  % goal



\begin{document}
\title{One-Shot Learning of Concurrent Action Models}
\author{Antonio Garrido \and Sergio Jim\'enez}
 
\maketitle

\begin{abstract}
  We present a {\em constraint programming} (CP) formulation for learning models of planning actions that can be executed in parallel and overlap. With the aim of understanding better the connection between the learning of planing action models, the synthesis of plans and the plan validation task, this paper studies a singular learning scenario where just a single observation of a plan execution ({\em one-shot}) is available. Our CP formulation is inspired by previous approaches for {\em temporal planning} so it models time-stamps for actions, {\em causal link} relationships, {\em threats} and effect {\em interferences}. Further, our CP formulation is flexible to accommodate a different range of expressiveness, subsuming the PDDL2.1 temporal semantics, and it is solver-independent (i.e. off-the-shelf CSP solvers can be used for resolution).   
\end{abstract}


\section{Introduction}
{\em Temporal planning} is an expressive planning model that relaxes the assumption of instantaneous actions of {\em classical planning}~\cite{geffner2013concise}. Actions in temporal planning are called {\em durative}, because each action has an associated duration and hence, actions conditions/effects may hold/happen at different times~\cite{fox2003pddl2}. This means that actions in the temporal planning model can be executed in parallel and overlap in several ways~\cite{cushing2007temporal} and that valid solutions for temporal planning instances must indicate the precise time-stamp when actions start and end~\cite{howey2004val}.

Despite the potential of state-of-the-art planners, its applicability to the real world is still somewhat limited because of the difficulty of specifying correct and complete planning models~\cite{kambhampati2007model}. The more expressive the planning model is, the more evident becomes this {\em knowledge acquisition bottleneck} that jeopardizes the usability of AI planning technology. This has led to a growing interest in the planning community for the learning of action models~\cite{yang2007learning,MouraoZPS12,zhuo2013action,kuvcera2018louga}. Most of these approaches are however purely inductive and require large datasets of observations, e.g. hundreds of plan observations to compute statistically significant models that minimize some error metric over the input observations. What is more, when model learning is considered an optimization task, it cannot guarantee that the learned action models do not fail to explain a given input observation (or that the states induced by executing the learned actions are incorrect).

This paper follows a radically different approach and analyzes the application of {\em Constraint Programming} (CP) for the {\em one-shot learning} of temporal action models, that is, for the singular scenario where action models are learned from a single observation of the execution of a plan. The contributions of this work are two-fold:
\begin{enumerate}  
\item As far as we know, this is the first approach for learning action models for temporal planning. Plan observations can refer to the execution of paralel and overlapping actions which makes our approach appealing for learning action models in multi-agent environments~\cite{furelos2018carpool}. Learning classical action models from sequential plans is a well-studied problem that has been previously addressed by a wide range of different approaches~\cite{arora2018review}. Since pioneering learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or states~\cite{MouraoZPS12,zhuo2013action}, from null state information~\cite{cresswell2013}, or from incomplete domain models~\cite{ZhuoK17,ZhuoNK13}. While learning an action model for classical planning means computing the actions' conditions and effects that are consistent with the input observations, learning {\em temporal action models} requires also: i) identifying how conditions and effects are temporally distributed in the action execution, and ii) estimate the action duration. 
\item Our CP formulation connects the {\em learning} of planning action models with the {\em synthesis} and the {\em validation} of plans since it allows that of-the-shelf CSP solvers can be used for any of these tasks. Further, we show that the plan validation capacity of our CP formulation is beyond the functionality of VAL (the standard plan validation tool~\cite{howey2004val}) since it can address {\em plan validation} of partial (or even an empty) action models and with partially observed plan traces (VAL requires both a full plan and a full action model for plan validation). 
\end{enumerate}

As a motivating example, let us assume an observation of the executions of actions in a {\em logistics scenario}. Learning the actions model will allow us: i) to better understand the insights of the logistics in terms of what is possible (or not) and why, the model must be consistent with the observed data; ii) to suggest changes that can improve the model originally created by a human, e.g. re-distributing the actions' conditions, provided they still explain the observations; and iii) to automatically elaborate similar models for similar scenarios, such as public transit for commuters, tourists or people in general in metropolitan areas ---\emph{a.k.a.} smart urban mobility.



\section{Background}
This section formalizes the {\em temporal planning} model that we follow in this work, the {\em hypothesis space} that confines the set of possible action models for the learning task we address and the {\em sampling space} that defines the kind of plan observations that we handle.

\subsection{Temporal Planning}
\label{sec:temporalplanning}
We assume that {\em states} are factored into a set $F$ of Boolean variables. A state $s$ is a time-stamped full assignment of values to the variables in $F$. 

A {\em temporal planning problem} is a tuple $\tup{F,I,G,A}$ where the {\em initial state} $I$, is a fully observed state (i.e. $|I|=|F|$) that is time-stamped with $t=0$; $G \subseteq F$ is a conjunction of {\em goal conditions} over the variables in $F$ that defines the set of goal states and $A$ represents the set of {\em durative actions}. Durative actions have an associated duration and may have conditions/effects at different times~\cite{vidal2006branching,garrido2009constraint}.

In this work we assume that the {\em durative actions} in $A$ are grounded from {\em action schemes} (also called {\em operators}) to compactly represent temporal planning problems. PDDL2.1 is a popular language for representing {\em temporal planning} problems and it is the input language for the temporal track of the International Planning Competition~\cite{fox2003pddl2,ghallab2004automated}. According to PDDL2.1 a {\em durative action} $a\in A$ is defined with the following elements:
\begin{enumerate}
\item $\dur(a)$, a positive value indicating the {\em duration} of the action.
\item $\cond_s(a), \cond_o(a), \cond_e(a) \subseteq F$ representing the action {\em conditions}. Unlike the \emph{pre}conditions of classical actions, action conditions in PDDL2.1 must hold: before $a$ is executed ({\em at start}), during the entire execution of $a$ ({\em over all}) or when $a$ finishes ({\em at end}), respectively. In the simplest case, $\cond_s(a) \cup \cond_o(a) \cup \cond_e(a) = \pre(a).$\footnote{Note that in classical planning, $\pre(a)=\{p,not-p\}$ is contradictory. In temporal planning, $\cond_s(a)=\{p\}$ and $\cond_e(a)=\{not-p\}$ is a possible situation, though unusual}
\item $\eff_s(a)$ and $\eff_e(a)$ represent the action {\em effects}. In PDDL2.1 effects may happen {\em at start} or {\em at end} of $a$, respectively (and can still be either positive or negative). Again, in the simplest case $\eff_s(a) \cup \eff_e(a) = \eff(a)$.
\end{enumerate}

PDDL2.1 somewhat restricts the expressiveness of the temporal planning model since the semantics of a PDDL2.1 {\em durative action} can be defined in terms of just two discrete events, $\start(a)$ and $\en(a)=\start(a)+\dur(a)$. This means that if action $a$ starts on state $s$ with time-stamp $\start(a)$, then $\cond_s(a)$ must hold in $s$. Ending action $a$ in state $s'$, with time-stamp $\en(a)$, means $\cond_e(a)$ must hold in $s'$. {\em Over all} conditions must hold at any state between $s$ and $s'$ or, in other words, throughout interval $[\start(a)..\en(a)]$. Likewise, {\em at start} and {\em at end} effects are instantaneously applied at states $s$ and $s'$, respectively ---continuous effects are not considered in this work. 

A {\em temporal plan} is a set of pairs $\{(a_1,t_1),(a_2,t_2)\ldots (a_n,t_n)\}$. Each $(a_i,t_i)$ pair contains a durative action $a_i$ and a {\em time-stamp} $t_i=\start(a_i)$. The {\em execution} of a temporal plan starting from a given initial state $I$ induces a state sequence formed by the union of all states $\{s_{t_i}, s_{t_i+\dur(a_i)}\}$, where there exists an initial state $s_{0}=I$, and a state $s_{end}$ that is the last state induced by the execution of the plan (sequential plans can then be expressed as temporal plans but not the opposite). We say that a temporal plan is a {\em solution} to a given temporal planning problem when its execution, starting from the corresponding initial state, eventually reaches a state that meets the goal conditions, $G\subseteq s_{end}$.

\subsection{The hypothesis space}
\label{sec:action-space}
The target of the learning task addressed in this paper is a set of PDDL2.1 {\em durative} actions schemes. Fig.~\ref{fig:exampleactions2} shows an example of two schemes for PDDL2.1 {\em durative actions} taken from the {\em driverlog} domain. The schema \texttt{board-truck} has a fixed duration while the duration of \texttt{drive-truck} depends on the driving time associated to the two given locations.

\begin{figure}
  \begin{tabular}{p{\textwidth}}
\begin{tiny}    
\begin{verbatim}
(:durative-action board-truck
  :parameters (?d - driver ?t - truck ?l - location)
  :duration (= ?duration 2)
  :condition (and (at start (at ?d ?l)) (at start (empty ?t))
                  (over all (at ?t ?l)))
  :effect (and (at start (not (at ?d ?l))) (at start (not (empty ?t)))
               (at end (driving ?d ?t))))


(:durative-action drive-truck
  :parameters (?t - truck ?l1 - location ?l2 - location ?d - driver)
  :duration (= ?duration (driving-time ?l1 ?l2))
  :condition (and (at start (at ?t ?l1)) (at start (link ?l1 ?l2))
                  (over all (driving ?d ?t)))
  :effect (and (at start (not (at ?t ?l1))) 
               (at end (at ?t ?l2))))
\end{verbatim}
\end{tiny}    
\end{tabular}
\caption{\small Two action schemes for PDDL2.1 durative actions.}
\label{fig:exampleactions2}
\end{figure}

Like in PDDL, we assume that the set $F$ of Boolean state variables is given by the instantiation of a given set of predicates $\Psi$. We denote as ${\mathcal I}_{\xi,\Psi}$ the {\em vocabulary} (set of symbols) that can appear in the {\em conditions} and {\em effects} of a given {\em durative} action schema $\xi$. This set is formally defined as the FOL interpretations of predicates $\Psi$, over the action parameters $pars(\xi)$.

For a {\em durative} action schema $\xi$, the size of its space of possible action models is then $D\times 2^{5\times|{\mathcal I}_{\xi,\Psi}}|$ where D is the number of different possible durations for any action shaped by the $\xi$ schema. Note that this space is significantly larger than for learning STRIPS actions~\cite{yang2007learning} where this number is $2^{2\times|{\mathcal I}_{\xi,\Psi}}|$ because negative effects must also be preconditions of the same action and cannot be positive effects of that action.

With this vocabulary defined then the {\em conditions} and {\em effects} of a {\em durative} action schema can be coded by 5 bit-vectors, each of length $|{\mathcal I}_{\xi,\Psi}|$. A 0-bit in the vector represents that the correpsonding condition of effect is not part of the schema while a 1-bit represents that is part of the schema. This also means that the {\em Hamming distance} can be used straightforward as a similarity metric for {\em durative} schemes. For instance, to compare a learned action model with respect to a given reference model that serves as baseline. The number of wrong 1-bits in the learned schema provide us a meassure of the incorrectness of the learned model and the number of wrong 0-bits in the learned schema provide us a meassure of the incompleteness of that model.



\subsection{The sampling space}
\label{sec:sampling-space}
In this work the learning examples are noiseless but partial observations of the execution of a temporal plan starting from a given initial state. This means that if the value of a state variable is observed then, that is the actual value of that variable but that not the value of all the state variables can be observed at any time. For instance, just a subset of them is observable because there are associated sensors reporting their value.

PDDL2.2 is an extension of PDDL2.1 that includes the notion of {\em Timed Initial Literal}~\cite{hoffmann2005} ($\til(f,t)$), as a way of representing that a given Boolean variable $f\in F$ becomes true at a certain time $t>0$, independently of the actions in the plan. Traditionally TILs are useful to model {\em exogenous happenings}; for instance, a time window when a warehouse is open in a logistics scenario, $\til(open,8)$ and $\til(\neg open,20)$. In this work we show that TILs are also suitable tool to model the observation of plan execution. The only difference with respect to the original semantic of TILs is that now, as happens with goals, observations represent conditions that must be {\em supported} by the execution of the plan at a particular time.

Figure~\ref{fig:exampletils} shows an example of the obsevation of a plan execution that is taken from the {\em driverlog} domain.
\begin{figure}
  \begin{tabular}{p{\textwidth}}
\begin{tiny}    
\begin{verbatim}

\end{verbatim}
\end{tiny}    
\end{tabular}
\caption{\small Example of the obsevation of a plan execution.}
\label{fig:exampletils}
\end{figure}



\section{Learning Action Models}
This section formalizes the learning task we address in this paper and presents our CP formulation for addressing it with off-the-shelf CSP solvers.

\subsection{One-shot learning of concurrent action models}
\label{subsec:ones}
We define the task of the {\em one-shot learning of temporal action models} as a tuple $\tup{F,I,G,A?,O,C}$, where:

\begin{itemize}
\item $\tup{F,I,G,A?}$ is a {\em temporal planning problem} where actions in $A?$ are partially specified (i.e., the exact {\em conditions/effects} and/or the {\em duration} of actions are unknown). In the worst case, we only know the vocabulary of the symbols that can appear in the {\em conditions/effects} of the actions. Available prior knowledge can be used to bound this vocabulary for certain action schemes.
\item $O$ is the {\em observation} of a plan execution. Al least it contains a full observation of the initial state (time-stamped with $t=0$) and a final state observation that equals the goals $G$ of the given {\em temporal planning problem} and it is time stamped with $t_{end}$, the maximum makes-span of a plan that solves that planning problem.  Additionally, it can also contain time-stamped observations of the traversed intermediate states and about the time when actions started/ended their execution.
\item $C$ is a set of {\em state-constraints} that reflects domain-specific expert knowledge. These constraints allow us to complete the input observation and/or to prune inconsistent action models. Figure~\ref{fig:example-statecs} show an example of a set of state-constraints for the {\em driverlog} domain.
\end{itemize}

\begin{figure}
  \begin{scriptsize}
  \begin{tabular}{l}
$\forall x_1,y_1,y_2\ \neg at(x_1,y_1)\vee\neg at(x_1,y_2), \neq (y_1,y_2).$\\
$\forall x_1,y_1,y_2\ \neg in(x_1,y_1)\vee\neg in(x_1,y_2), \neq (y_1,y_2).$\\
$\forall x_1,y_1,y_2\ \neg driving(x_1,y_1)\vee\neg driving(x_1,y_2), \neq (y_1,y_2).$\\
$\forall x_1,y_1,y_2\ \neg driving(y_1,x_1)\vee\neg driving(y_2,x_1), \neq (y_1,y_2).$\\    
$\forall x_1,y_1,y_2\ \neg at(x_1,y_1)\vee\neg driving(x_1,y_2).$\\
$\forall x_1,y_1,y_2\ \neg in(x_1,y_1)\vee\neg driving(x_1,y_2).$\\
$\forall x_1,y_1,y_2\ \neg at(x_1,y_1)\vee\neg in(x_1,y_2).$\\
$\forall x_1,y_1\ \neg empty(x_1)\vee\neg driving(y_1,x_1).$\\
$\forall x_1,y_1\ \neg empty(x_1)\vee\neg driving(y_1,x_1).$\\
$\forall x_1\ \neg link(x_1,x_1).$\\
$\forall x_1\ \neg path(x_1,x_1).$\\
  \end{tabular}
\end{scriptsize}      
\caption{\small Examples of state-constraints for the {\em driverlog} domain.}
\label{fig:example-statecs}
\end{figure}

A {\em solution} to the {\em one-shot learning of temporal action models} is a fully specified model of temporal actions $\mathcal{A}$ such that: (1), the conditions, effects and duration of the actions in $\mathcal{A}$ is completely specified, (there is no uncertainty about them). (2), the specified conditions, effects and duration of the actions in $\mathcal{A}$ are {\em consistent} with the given inputs $\tup{F,I,G,A?,O,C}$. In other words, such that we can build on top of the actions in $\mathcal{A}$ a valid plan whose execution starts in $I$, can produce the observations in $O$, and that reaches a final state that satisfies $G$. 

\subsection{Constraint satisfaction for learning action models}
\label{subsec:CPformulation}

Given a {\em one-shot learning} task, defined as in subsection~\ref{subsec:ones}, our approach is to create a CSP whose solution induces an action model that solves the given {\em one-shot learning} task. Our CP formulation is solver-independent (this means that any off-the-shelf CSP solver that supports the expressiveness of our CP formulation can be used) and it is inspired by previous work on {\em temporal planning as CP}~\cite{vidal2006branching,garrido2009constraint}. 

\subsubsection{The CSP Variables}
For each action $a$ in $A?$, the CSP contains the seven kinds of variables specified in Table~\ref{table:variables}. For simplicity, we model time in $\mathbb{Z}^+$ and bound all maximum times to the makespan $t_{end}$ of the observed plan execution. If the observation of the plan makespan it is not available we consider a long enough domain for durations.

\begin{table}
\setlength\extrarowheight{2pt}  
\begin{scriptsize}
\begin{tabular}{lll}
{\bf Variable} & {\bf Domain} & {\bf Description} \\\hline
$\start(a)$ & \emph{Known/derived value} & Start time for $a$.\\
$\en(a)$ & \emph{Known/derived value} & End time for $a$.\\
$\dur(a)$ & $[1..max(a)]$ & Duration of $a$ where \\
&&$max(a)=t_{end}-\start(a)$.\\

$\reqs(p,a)$, & $[0..t_{end}]$ & Interval $[\reqs(p,a)..\reqe(p,a)]$ \\ 
$\reqe(p,a)$  & $[0..t_{end}]$ & during which action $a$ requires $p$.\\

$\tim(p,a)$ & $[0..t_{end}]$ & Time when the effect $p$ of $a$ happens.\\

$\supp(p,a)$ & The action space & There is a causal link $\tup{b_i,p,a}$ s.t.\\
&& $b_i$ is an action in the action space. 
\end{tabular}
\end{scriptsize}
\caption{\small The CSP variables, their domains and semantics.}
\label{table:variables}
\end{table}

The value of the CSP variables $\start(a)$, $\dur(a)$ and $\en(a)$ is either given by the observation $O$ or derived from the expression $\en(a)=\start(a)+\dur(a)$. The remaining CSP variables model, respectively, the interval when conditions must hold, the time when the effects happen and the causal links of a solution plan that induces an action model that solves the given {\em one-shot learning} task.

Besides the actions of the given planning problem, we create two additional {\em dummy} actions:
\begin{itemize}
\item $\ini$, that represents the {\em initial state} ($\start(\ini)=0$ and $\dur(\ini)=0$). This dummy action has no conditions so it has no associated variables $\supp, \reqs$ and $\reqe$ and has as many $\tim(p_i,\ini)=0$ associated variables as $p_i$ in $I$.
\item $\goal$, that represents the {\em last state observation} ($\start(\goal)=t_{end}$ and $\dur(\goal)=0$). This dummy action has no effects so it has no $\tim(p,a)$ variables and has as many associated variable $\supp(p_i,\goal)$ and $\reqs(p_i,\goal)=\reqe(p_i,\goal)=t_{end}$ as different $p_i$ are in $G$. 
\end{itemize}  

Furthermore this formulation models TILs like any other regular actions. A $\til(f,t)$ can be seen as an additional {\em dummy} action ($\start(\til(f,t))=t$ and $\dur(\til(f,t))=0$) with no conditions and the single effect $f$ that happens at time $t$ ($\tim(f,\til(f,t))=t$). The modeling of TILs is then analogous to $\ini$, as they both represent information that is given at a particular time, but externally to the execution of the plan. The formulation can model also time-stamped state observations (as TILS that must be {\em supported} by the execution of the solution plan). In other words as regular actions but with a fixed starting time.

\subsubsection{The CSP Constraints}
Table~\ref{table:constraints} shows the constraints defined among the CSP variables of Table~\ref{table:variables}. The first three constraints are explicit enough. The fourth constraint models {\em causal links} $\tup{b,p,a}$ (i.e., the time when $b$ supports $p$ must be before $a$ requires $p$). Note that in a causal link $\tup{b,p,a}$, $\tim(p,b) < \reqs(p,a)$ and not $\leq$ because, like in PDDL~\cite{fox2003pddl2}, our temporal planning model assumes an $\epsilon > 0$ ($\epsilon=1$ since we are modeling time in $\mathbb{Z}^+$) as a small tolerance between the time when a given effect $p$ is supported and when it is required. 

\begin{table}
\setlength\extrarowheight{2pt}    
\begin{tiny}
\begin{tabular}{lll}
{\bf ID}&{\bf Constraint}&{\bf Description}\\\hline

1&$\en(a)=\start(a)+\dur(a)$ & End time of $a$. \\

2&$\en(a) \leq \start(\goal)$ & Always $\goal$ is the last action of the plan. \\

3&$\reqs(p,a) \leq \reqe(p,a)$ & [$\reqs(p,a)..\reqe(p,a)$]\\
& & is a valid interval.\\

4&if $\supp(p,a)=b$ then & Modeling causal links $\tup{b,p,a}$.\\
&\hspace{1pt}$\tim(p,b) < \reqs(p,a)$ & \\

5& $\forall c \neq a$ that deletes $p$ at time $t$: & Solving threat of $c$ to causal link $\tup{b,p,a}$\\
&\hspace{1pt}if $\supp(p,a)=b$ then &  by promotion or demotion.\\
&\hspace{2pt}$t < \tim(p,b)$ OR $t > \reqe(p,a)$ & \\

6&if $a$ requires and deletes $p$: & When $a$ requires and deletes $p$, the effect \\
&\hspace{1pt}$\tim(not-p,a) \geq \reqe(p,a)$ & cannot happen before the condition.\\

7&$\forall a_i,a_j \mid a_i$ supports $p$ and & Solving effect interference ($p$ and $not-p$)\\
&\hspace{1pt}$a_j$ deletes $p$: & they cannot happen at the same time.\\
&\hspace{2pt}$\tim(p,a_i) \neq \tim(not-p,a_j)$ &
\end{tabular}
\end{tiny}
\caption{\small The CSP constraints and their semantics.}
\label{table:constraints}
\end{table}

The fifth constraint avoids {\em threats} via {\em promotion} or {\em demotion}~\cite{ghallab2004automated}. The sixth constraint models the fact that when the same action requires and deletes $p$ then the effect cannot happen before the condition. Note the $\geq$ inequality here; if one condition and one effect of $a$ happen at the same time, the underlying semantics in planning considers the condition is checked instantly before the effect~\cite{fox2003pddl2}. The seventh constraint deals with the fact that two (possibly equal) actions have contradictory effects. These constraints apply to any type of action, including the additional dummy actions for representing $\ini$, $\goal$, the TILs and the time-stamped state observations that are as given input of the {\em one-shot learning} task.

In addition to the constraints of Table~\ref{table:constraints} we can add input {\em state-constraint} (if available), like the ones pictured in Figure~\ref{fig:example-statecs}, to our CSP formulation to prune inconsistent action models.

\subsubsection{The CSP heuristics}
\label{sec:implementation}
Our CSP formulation is solver-independent, which means we do not use heuristics that require changes in the implementation of the CSP engine. Although this reduces the solver performance, we are interested in using it as a blackbox that can be easily changed with no modification in our formulation. However, the experimentation showed us that the following {\em value selection} heuristics are effective to solve the defined CSPs:
\begin{enumerate}
\item $\dur(a)$, {\em lower values first}, thus preferring shortest solutions that make the learned model consistent.
\item $\reqs(p,a)$ and $\reqe(p,a)$. For $\reqs$, lower values first, whereas for $\reqe$, upper values first. This gives priority to $\cond_o(a)$, keeping conditions active as long as possible.  
\item $\tim(p,a)$. Lower values first, for negative effects, while upper values first, for positive effects. This gives priority to $\eff_s(a)$ delete effects and $\eff_e(a)$ positive effects. 
\item $\supp(p,a)$, lower values first to prefer supporters that start earlier in the plan.
\end{enumerate}

\subsection{Specific constraints for the PDDL2.1 model}
\label{sec:PDDL21constraints}
The defined CSP formulation deals with a {\em temporal planning} model that is more expressive than the defined by the PDDL2.1 language. For instance, it allows conditions and effects to happen at any time, even outside the execution of the action. Imagine a condition $p$ that only needs to be maintained for 5 time units before an action $a$ starts (e.g. warming-up a motor before driving): the expression $\reqe(p,a)=\start(a); \reqe(p,a) = \reqs(p,a)+5$ fits this CSP formulation. Likewise the CSP formulation supports also the modeling of effects $p$ that happen in the middle of an action, e.g., $\tim(p,a) = \start(a)+ (\dur(a) / 2)$.

We show here that, by adding extra constraints to our CSP formulation, we can make it PDDL2.1-compliant. Table~\ref{table:21constraints} summarizes these extra constraints. To limit conditions to only be \emph{at start}, \emph{over all} or \emph{at end} of an action execution we add constraints 1 and 2. Likewise, we make effects to exclusively happen either\emph{at start} or \emph{at end} of action executions with constraint 3. If all effects happen \emph{at start} the duration of the action would be irrelevant and could exceed the plan makespan. To avoid this, for any action $a$, at least one of its effects should happen \emph{at end}: $\sum_{i=1}^{n =|\eff(a)|} \tim(p_i,a) > n \cdot \start(a)$, which guarantees $\eff_e(a)$ is not empty.

\begin{table}
\setlength\extrarowheight{2pt}    
\begin{tiny}
\begin{tabular}{lll}
{\bf ID} & {\bf Constraint} & {\bf Description} \\\hline
1&$\reqs(p,a) = \start(a)$ OR $\reqs(p,a) = \en(a)$ & Conditions at start.\\
2&$\reqe(p,a) = \start(a)$ OR $\reqe(p,a) = \en(a)$ & Conditions at end.\\
3&$\tim(p,a) = \start(a)$ OR $\tim(p,a) = \en(a)$ & Effects at start or at end.\\
4&$\sum_{i=1}^{n =|\eff(a)|} \tim(p_i,a) > n \cdot \start(a)$ & At least one effect.\\
5&$\forall a_i,a_j$ instances of the same operator: & Duration of the schema instantiations.\\
&$\dur(a_i) = \dur(a_j)$ &\\

7&$\forall p_i: (\forall a_j: \reqs(p_i,a_j) = \start(a_j))$ OR & Conditions of the schema instantiations.\\
&$(\forall a_j: \reqs(p_i,a_j) = \en(a_j))$ & \\

8&$\forall p_i: (\forall a_j: \reqe(p_i,a_j) = \start(a_j))$ OR  & \\
&$(\forall a_j: \reqe(p_i,a_j) = \en(a_j))$ & \\

9&$\forall p_i: (\forall a_j: \tim(p_i,a_j) = \start(a_j))$ OR  & Effects of the schema instantiations.\\
&$(\forall a_j: \tim(p_i,a_j) = \en(a_j))$ &
\end{tabular}
\end{tiny}
\caption{\small The CSP constraints for the PDDL2.1.}
\label{table:21constraints}
\end{table}

Note that if a condition is never deleted in a plan, it can be considered an {\em invariant} condition for such a plan that represents {\em static} knowledge; e.g. a link between two locations that makes driving possible, or modeling a petrol station that allows a refuel action in a given location, etc. The constraint to be added for {\em invariant conditions} $p \in \cond_o(a)$ is simply: $((\reqs(p,a) = \start(a))$ AND $(\reqe(p,a) = \en(a)))$, i.e. Surprisingly, invariant conditions are modeled differently depending on the human modeler. See, for instance, \texttt{(link ?from ?to)} of Fig.~\ref{fig:exampleactions2}, which is modeled as an \emph{at start} condition despite: i) the link should be necessary all over the driving, and ii) no action in this domain can be planned to delete that link.
This also happens in the \emph{transport} domain of the IPC, where a refuel action requires to have a petrol station in a location only \emph{at start}, rather than \emph{over all} which makes more sense. This shows that modeling temporal planning tasks depends on the human's {\em common sense}. On the contrary, our formulation checks the invariant conditions and deals with them always in a consistent way.

The structure of conditions and effects for all grounded actions of the same operator is constant in PDDL2.1. This means that if \texttt{(empty ?t)} is an \emph{at start} condition of \texttt{board-truck}, it will be \emph{at start} in any of its grounded actions. Let $\{p_i\}$ be the conditions of an operator and $\{a_j\}$ be the instances of a particular operator, constraints 6,7 and 8 guarantee a constant structure for all the instances of the same operator. 

Durations in PDDL2.1 can be defined in two different ways. On the one hand, durations can be equal for all grounded actions of the same operator. For instance, any instantiation of \texttt{board-truck} of Fig.~\ref{fig:exampleactions2} will last 2 time units no matter its parameters. On the other hand, although different instantiations of \texttt{drive-truck} will last different depending on the locations, different occurrences of the same instantiated action will last equal. We model both cases adding constraint 5.  



\section{A unified formulation for planning, validation and learning}
\label{sec:usingCPValidation}
This section shows that our formulation for the {\em one-shot learning of temporal action models} is connected to the tasks of plan {\em synthesis} and plan {\em validation} and that this connection applies not only to temporal planning but also to the classical planning model, the vanilla model of AI planning where actions are instantaneous~\cite{geffner2013concise}. 

The connection between the planning, validation and learning tasks lies on the fact that we can constrain the CSP variables that model the conditions and effects of actions to known values. This feature is usefull to leverage a priori knowledge of a given planning domain. For instance, because we have some knowledge about the possible durations of a given action or because we already know that a given action produces for sure certain effects.  This approach allows to synthesize a plan with a given action model, in this case every variable representing the conditions, effects and duration of the actions are constrained to a single value. Likewise we can validate a plan contrained the value of the variales representing the time-stamps for actions starting time, as we will see in section~\ref{sec:evaluation}.

What is more, we can either synthesize (or validate) a plan despite some of the variables that representing the conditions, effects or duration of an action do not have a fixed value (its value is a priori unknwon). When addressing learning, planning or validating tasks, our formulation is flexible to accept different levels of specificatoin of the input knowledge:
\begin{itemize}
\item Partial knowledge of the conditions/effects of actions.
\item Partial knowledge of action durations (i.e. a set of possible durations).
\item Partial knowledge of the plan to validadate or synthesize.
\end{itemize}

To illustrate this, let us assume that the distribution of all (or just a few) conditions and/or effects is known and, in consequence, represented in the learning task. If a solution to the CSP is found, then that structure of conditions/effects is consistent for the learned model. On the contrary, if no solution is found that structure is inconsistent and cannot be explained. We can also represent known values for the durations by bounnding the value of $\dur(a)$ variables to a given value. We can also introduce a priori knowledge about plans by bounding the value of the $\start(a)$ variables.

Last but not least, our CSP formulation can be adapted straightforward to address learning, planning or validation tasks within the classical planning model. In this case actions cannot have conditions {\em overall} or {\em at end} as well as they cannot have {\em at start} effects. Therefore the variables representing this kind of information can be removed from the CSP model (or be set to {\tt false}). Further the duration of any action is fixed to one unit~\cite{jimenez2015temporal}.  


\section{Evaluation}
\label{sec:evaluation}
The CP formulation has been implemented in \textsf{Choco}\footnote{\texttt{http://www.choco-solver.org}}, an open-source Java library for constraint programming that provides an object-oriented API to state the constraints to be satisfied.

The empirical evaluation of a learning task can be addressed from two perspectives. From a pure syntactic perspective, learning can be considered as an automated design task to create a new model that is similar to a reference (or {\em ground truth}) model. Consequently, the success of learning is an accuracy measure of how similar these two models are, which usually counts the number of differences (in terms of incorrect durations or distribution of conditions/effects). Unfortunately, there is not a unique reference model when learning temporal models at real-world problems. Also, a pure syntax-based measure usually returns misleading and pessimistic results, as it may count as incorrect a different duration or a change in the distribution of conditions/effects that really represent equivalent reformulations of the reference model. For instance, given the example of Fig.~\ref{fig:exampleactions2}, the condition learned \texttt{(over all (link ?from ?to))} would be counted as a difference in action \texttt{drive-truck}, as it is \texttt{at start} in the reference model; but it is, semantically speaking, even more correct. Analogously, some durations may differ from the reference model but they should not be counted as incorrect. As seen in section~\ref{sec:simpleTask}, some learned durations cannot be granted, but the underlying model is still consistent. Therefore, performing a syntactic evaluation in learning is not always a good idea.

From a semantic perspective, learning can be considered as a classification task where we first learn a model from a training dataset, then tune the model on a validation test and, finally, asses the model on a test dataset. Our approach represents a one-shot learning task because we only use one plan sample to learn the model and no validation step is required.
Therefore, the success of the learned model can be assessed by analyzing the success ratio of the learned model \emph{vs.} all the unseen samples of a test dataset. In other words, we are interested in learning a model that fits as many samples of the test dataset as possible. This is the evaluation that we consider most valuable for learning, and define the success ratio as the percentage of samples of the test dataset that are consistent with the learned model. A higher ratio means that the learned model explains, or adequately fits, the observed constraints the test dataset imposes.


\subsection{Learning from partially specified action models}
We have run experiments on nine IPC planning domains. It is important to highlight that these domains are encoded in PDDL2.1, with the number of operators shown in Table~\ref{table:evaluationExperiments}, so we have included the constraints given in section~\ref{sec:PDDL21constraints}. We first get the plans for these domains by using five planners (\textit{LPG-Quality}~\cite{gerevini2003planning}, \textit{LPG-Speed}~\cite{gerevini2003planning}, \textit{TP}~\cite{jimenez2015temporal}, \textit{TFD}~\cite{eyerich2009using} and \textit{TFLAP}~\cite{marzal2016temporal}), where the planning time is limited to 100s.
The actions and observations on each plan are automatically compiled into a CSP learning instance. Then,
%we create the CP formulation and
we run the one-shot learning task to get a temporal action model for each instance, where the learning time is limited to 100s on an Intel i5-6400 @ 2.70GHz with 8GB of RAM.
In order to assess the quality of the learned model, we validate each model \emph{vs.} the other models \emph{w.r.t.} the \emph{struct}ure, the \emph{dur}ation and the \emph{struct}ure+\emph{dur}ation, as discussed in section~\ref{sec:usingCPValidation}.
For instance, the \emph{zenotravel} domain contains 78 instances, which means learning 78 models. Each model is validated by using the 77 remaining models, thus producing 78$\times$77=6006 validations per struct, dur and struct+dur each. The value for each cell is the average success ratio.
In \emph{zenotravel}, the struct value means that the distribution of conditions/effects learned by using only one plan sample is consistent with all the samples used as dataset (100\% of the 6006 validations), which is the perfect result, as also happens in \emph{floortile} and \emph{sokoban} domains.
The dur value means the durations learned explain 68.83\% of the dataset. This value is usually lower because any learned duration that leads to inconsistency in a sample counts as a failure. The struct+dur value means that the learned model explains entirely 35.76\% of the samples. This value is always the lowest because a subtle structure or duration that leads to inconsistency in a sample counts as a failure.
As seen in Table~\ref{table:evaluationExperiments}, the results are specially good, taking into consideration that we use only one sample to learn the temporal action model.
These results depend on the domain size (number of operators, which need to be grounded), the relationships (causal links, threats and interferences) among the actions, and the size and quality of the plans.

\begin{table}
\begin{scriptsize}
\begin{tabular}{l|llrrr}
& {\bf ops} & {\bf ins} & {\bf struct} & {\bf dur} & {\bf struct+dur}  \\\hline

\emph{zenotravel} & 5 & 78 & 100\% & 68.83\% & 35.76\% \\
\emph{driverlog} & 6 & 73 & 97.60\% & 44.86\% & 21.04\% \\
\emph{depots} & 5 & 64 & 55.41\% & 76.22\% & 23.19\% \\
\emph{rovers} & 9 & 84 & 78.84\% & 5.35\% & 0.17\% \\
\emph{satellite} & 5 & 84 & 80.74\% & 57.13\% & 40.53\% \\
\emph{storage} & 5 & 69 & 58.08\% & 70.10\% & 38.36\% \\
\emph{floortile} & 7 & 17 & 100\% & 80.88\% & 48.90\%\\
\emph{parking} & 4 & 49 & 86.69\% & 81.38\% & 54.89\% \\
\emph{sokoban} & 3 & 51 & 100\% & 87.25\% & 79.96\% \\

\end{tabular}
\end{scriptsize}
\caption{\small Number of operators to learn. Instances used for validation. Average success ratio of the one-shot learned model \emph{vs.} the test dataset in different IPC planning domains.}
\label{table:evaluationExperiments}
\end{table}


We have observed that some planners return plans with unnecessary actions, which has a negative impact for learning precise durations.
%Some planners return plans with unnecessary actions, thus making an adequate learning more difficult. In particular, actions that are redundant have a negative impact for learning precise durations.
The worst result is returned in the \emph{rovers} domain, which models a group of planetary rovers to explore the planet they are on. Since there are many parallel actions for taking pictures/samples and navigation of multiple rovers, learning the duration and the structure+duration is particularly complex in this domain.

\subsection{Learning from scratch}

\section{Conclussions}
\label{sec:conclusions}


We have presented a purely declarative CP formulation, which is independent of any CSP solver, to address the learning of temporal action models. Learning in planning is specially interesting to recognize past behavior in order to predict and anticipate actions to improve decisions.
The main contribution is a simple formulation that is automatically derived from the actions and observations on each plan execution, without the necessity of specific hand-coded domain knowledge. It is also flexible to support a very expressive temporal planning model, though it can be easily modified to be PDDL2.1-compliant.
Formal properties are inherited from the formulation itself and the CSP solver. The formulation is correct because the definition of constraints to solve causal links, threats and effect interferences are supported, which avoids contradictions. It is also complete because the solution needs to be consistent with all the imposed constraints, while a complete exploration of the domain of each variable returns all the possible learned models in the form of alternative consistent solutions.


Unlike other approaches that need to learn from datasets with many samples, we perform a one-shot learning. This reduces both the size of the required datasets and the computation time. The one-shot learned models are very good and explain a high number of samples in the datasets used for testing. Moreover, the same CP formulation is valid for learning and for validation, by simply adding constraints to the variables. This is an advantage, as the same formulation allows us to carry out different tasks: from entirely learning, partial learning/validation (structure and/or duration) to entirely plan validation.
According to our experiments, learning the structure of the actions in a one-shot way leads to representative enough models, but learning the precise durations is more difficult, and even impossible, when many actions are executed in parallel.


Finally,
%it is important to note that
our CP formulation can be represented and solved by Satisfiability Modulo Theories, which is part of our current work. As future work, we want to extend our formulation to learn meta-models, as combinations of many learned models, and a more complete action model.
In the latter, rather than using a partially specified set of actions, we want to find out the conditions/effects together with their distribution.
%We aim at finding out the conditions/effects together with their distribution, thus removing constraints from the partially specified set of actions in $A?$.
The underlying idea of finding an action model consistent with all the constraints will remain the same, but the model will need to be extended with additional decision variables and constraints. %$\mathsf{is\_condition(p,a)}$, $\mathsf{is\_effect(p,a)}$ to decide whether $p$ is a condition or effect of action $a$.
This will probably lead to the analysis of new heuristics for resolution.

\bibliographystyle{ecai}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
