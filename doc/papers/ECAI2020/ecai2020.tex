\documentclass{ecai}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{amssymb}

\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}    % precondition
\newcommand{\eff}{\mathsf{eff}}    % effect
\newcommand{\cond}{\mathsf{cond}}  % condition
\newcommand{\dur}{\mathsf{dur}}    % duration
\newcommand{\obs}{\mathsf{obs}}    % observation
\newcommand{\start}{\mathsf{start}}% start
\newcommand{\en}{\mathsf{end}}     % end
\newcommand{\til}{\mathsf{til}}    % TIL
\newcommand{\supp}{\mathsf{sup}}   % sup
\newcommand{\tim}{\mathsf{time}}   % time
\newcommand{\reqs}{\mathsf{req\_{start}}} % req_start
\newcommand{\reqe}{\mathsf{req\_{end}}}   % req_end
\newcommand{\ini}{\mathsf{init}}   % init
\newcommand{\goal}{\mathsf{goal}}  % goal



\begin{document}
\title{One-Shot Learning of Concurrent Action Models}
\author{Antonio Garrido \and Sergio Jim\'enez}
 
\maketitle

\begin{abstract}
The paper presents a constraint programming (CP) formulation for the learning of temporal planning action models. Like existing formulations for the synthesis of temporal plans, our CP formulation models also time-stamps for actions, {\em causal link} relationships, {\em threats} and effect {\em interferences}. Further we show that it can accommodate a different range of expressiveness, subsuming the PDDL2.1 temporal semantics and that is solver-independent, meaning that an arbitrary CSP solver can be used for its resolution.  With the aim of understanding better the relations between the learning of planning action models, plan synthesis and plan validation, the paper focuses on the extreme learning scenario where just a single partial observation of the execution of a plan is available (i.e. one-shot). 
\end{abstract}


\section{Introduction}
{\em Temporal planning} is an expressive planning model that relaxes the assumption of instantaneous actions of {\em classical planning}~\cite{geffner2013concise}. Actions in temporal planning are called {\em durative}, i.e. they have durations so conditions/effects may hold/happen at different times~\cite{fox2003pddl2}. This means that actions in the temporal planning model can be executed in parallel and overlap in several ways~\cite{cushing2007temporal} and that valid solutions for temporal planning instances indicate the precise time-stamp when actions start and end~\cite{howey2004val}.

Despite the potential of state-of-the-art planners, its applicability to the real world is still somewhat limited because of the difficulty of specifying correct and complete planning models~\cite{kambhampati2007model}. The more expressive the planning model is, the more evident becomes this {\em knowledge acquisition bottleneck} that jeopardizes the usability of AI planning technology. This has led to a growing interest in the planning community for the learning of action models. Most approaches for learning planning action models are purely inductive and often require large datasets of observations, e.g. thousands of plan observations to compute statistically significant models that minimizes some error metric over the observations~\cite{yang2007learning,MouraoZPS12,zhuo2013action,kuvcera2018louga}. However, when model learning is considered an optimization task over a set of observations, it does not guarantee that the learned action model may fail to explain input observations or that the states induced by the execution of the plan generated with the model are incorrect.

This paper analyzes the application of {\em Constraint Programming} for the {\em one-shot learning} of temporal action models, that is, the extreme scenario where action models are learned from a single observation of the execution of a temporal plan. The contributions of our CP formulation for the one-shot learning of temporal action models are two-fold:
\begin{enumerate}  
\item This is the first approach for learning action models for temporal planning where plan observations can refer to the execution of overlapping actions, this makes our approach appealing for learning in multi-agent environments. Learning classical action models from sequential plans has been previously addressed by a wide range of different approaches~\cite{arora2018review}. Since pioneering learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or states~\cite{MouraoZPS12,zhuo2013action}, from null state information~\cite{cresswell2013}, or from incomplete domain models~\cite{ZhuoK17,ZhuoNK13}. While learning an action model for classical planning means computing the actions' conditions and effects that are consistent with the input observations, learning temporal action models extends this task to: i) identifying how conditions and effects are temporally distributed in the action execution, and ii) estimate the action duration. 
\item Our CP formulation evidences the relations of the {\em learning} of planning action models with the {\em synthesis} and the {\em validation} of plans. Further, we show that the plan validation capacity of our CP formulation is beyond the functionality of VAL (the standard plan validation tool~\cite{howey2004val}) since it can address {\em plan validation} of partial (or even an empty) action models and with partially observed plan traces (VAL requires a full plan and a full action model for plan validation). Further, our CP formulation allows that an arbitrary CSP solver can be used for the learning, planning and validation tasks.  
\end{enumerate}

As a motivating example, let us assume a {\em logistics scenario}. Learning the temporal planning model will allow us: i) to better understand the insights of the logistics in terms of what is possible (or not) and why, because the model is consistent with the observed data; ii) to suggest changes that can improve the model originally created by a human, e.g. re-distributing the actions' conditions, provided they still explain the observations; and iii) to automatically elaborate similar models for similar scenarios, such as public transit for commuters, tourists or people in general in metropolitan areas ---\emph{a.k.a.} smart urban mobility.



\section{Background}
This section formalizes the {\em temporal} planning model that we follow in this work, the hypothesis space for our learning target, FOL schemes of {\em durative} actions, and the sampling space.

\subsection{Temporal Planning}
\label{sec:temporalplanning}

We assume that states are factored into a set $F$ of Boolean variables. A state $s$ is then a time-stamped full assignment of values to these variables. The initial state $I$, is fully observed state (i.e. $|I|=|F|$) that is stamped with time $t=0$. Last but not least $G \subseteq F$ is a conjunction of goal conditions over $F$ that defines the set of goal states.

A {\em temporal planning problem} is a tuple $\tup{F,I,G,A}$ where $A$ represents the set of {\em durative actions}. There are several options that allow for a high expressiveness of durative actions. On the one hand, an action can have a fixed duration, a duration that ranges within an interval or a distribution of durations. On the other hand, actions may have conditions/effects at different times, such as conditions that must hold some time before the action starts, effects that happen just when the action starts, in the middle of the action or some time after the action finishes~\cite{garrido2009constraint}.

We assume that actions are grounded from action schemes (also known as operators) to compactly represent {\em temporal planning problems}.  PDDL2.1 is a popular language for representing {\em temporal planning} problems and it is the input language for the temporal track of the international planning competition~\cite{fox2003pddl2,ghallab2004automated}. PDDL2.1 somewhat restricts temporal expressiveness since it defines a {\em durative action} $a$ with the following elements:

\begin{enumerate}
\item The action {\em duration}, $\dur(a)$, that is a positive value.
\item The action {\em conditions}, $\cond_s(a), \cond_o(a), \cond_e(a) \subseteq F$. Unlike the \emph{pre}conditions of a classical action, action conditions in PDDL2.1 must hold: before $a$ ({\em at start}), during the entire execution of $a$ ({\em over all}) or when $a$ finishes ({\em at end}), respectively. In the simplest case, $\cond_s(a) \cup \cond_o(a) \cup \cond_e(a) = \pre(a).$\footnote{Note that in classical planning, $\pre(a)=\{p,not-p\}$ is contradictory. In temporal planning, $\cond_s(a)=\{p\}$ and $\cond_e(a)=\{not-p\}$ is a possible situation, though unusual}
\item The action {\em effects}, $\eff_s(a)$ and $\eff_e(a)$. In PDDL2.1 effects can happen {\em at start} or {\em at end} of $a$, respectively, and can still be positive or negative. Again, in the simplest case $\eff_s(a) \cup \eff_e(a) = \eff(a)$.
%Again, $\eff_s(a) \cup \eff_e(a) \subseteq \eff(a)$.
\end{enumerate}


%Despite durative actions are no longer instantaneous in PDDL2.1, {\em at start} and {\em at end} conditions are checked instantaneously.

The semantics of a PDDL2.1 durative action $a$ can be defined in terms of two discrete events, $\start(a)$ and $\en(a)=\start(a)+\dur(a)$. This means that if action $a$ starts on state $s$ with time-stamp $\start(a)$, $\cond_s(a)$ must hold in $s$; and ending $a$ in state $s'$, with time-stamp $\en(a)$, means $\cond_e(a)$ holds in $s'$. {\em Over all} conditions must hold at any state between $s$ and $s'$ or, in other words, throughout interval $[\start(a)..\en(a)]$. Analogously, {\em at start} and {\em at end} effects are instantaneously applied at states $s$ and $s'$, respectively ---continuous effects are not considered. Fig.~\ref{fig:exampleactions2} shows two scheme for durative actions taken from the {\em driverlog} domain. The schema \texttt{board-truck} has a fixed duration whereas in \texttt{drive-truck} duration depends on the driving time associated to the two locations.

\begin{figure}
  \begin{tabular}{p{\textwidth}}
\begin{tiny}    
\begin{verbatim}
(:durative-action board-truck
  :parameters (?d - driver ?t - truck ?l - location)
  :duration (= ?duration 2)
  :condition (and (at start (at ?d ?l)) (at start (empty ?t))
                  (over all (at ?t ?l)))
  :effect (and (at start (not (at ?d ?l))) (at start (not (empty ?t)))
               (at end (driving ?d ?t))))

(:durative-action drive-truck
  :parameters (?t - truck ?from - location ?to - location ?d - driver)
  :duration (= ?duration (driving-time ?from ?to))
  :condition (and (at start (at ?t ?from)) (at start (link ?from ?to))
                  (over all (driving ?d ?t)))
  :effect (and (at start (not (at ?t ?from))) 
               (at end (at ?t ?to))))
\end{verbatim}
\end{tiny}    
\end{tabular}
\caption{\small Example of two PDDL2.1 action schemes.}
\label{fig:exampleactions2}
\end{figure}

PDDL2.2 is an extension of PDDL2.1 that includes the notion of {\em Timed Initial Literal}~\cite{hoffmann2005} ($\til(f,t)$), as a way of defining a fact $f\in F$ that becomes true at a certain time $t$, independently of the actions in the plan. TILs are useful to define exogenous happenings; for instance, a time window when a warehouse is open in a logistics scenario ($\til(open,8)$ and $\til(\neg open,20)$).

A temporal plan is a set of pairs $\tup{(a_1,t_1),(a_2,t_2)\ldots (a_n,t_n)}$. Each $(a_i,t_i)$ pair contains a durative action $a_i$ and a time-stamp $t_i=\start(a_i)$. The execution of a temporal plan starting from a given initial state $I$ induces a state sequence formed by the union of all states $\{s_{t_i}, s_{t_i+\dur(a_i)}\}$, where there exists a state $s_{0}=I$, and $G\subseteq s_{end}$, being $s_{end}$ the last state induced by the plan. Therefore sequential plans can be expressed a temporal plans but not the opposite. We say that a temporal plan is a {\em solution} to a given temporal planning problem when the plan execution, starting from the corresponding initial state, eventually reaches a state that met the goal conditions.


\subsection{The space of {\em durative} action models}
\label{sec:action-space}

We denote as ${\mathcal I}_{\xi,\Psi}$ the {\em vocabulary} (set of symbols) that can appear in the {\em conditions} and {\em effects} of a given {\em durative} action schema $\xi$. This set is defined as the FOL interpretations of the set of predicates $\Psi$, over the action parameters $pars(\xi)$.

For a {\em durative} action schema $\xi$ its space of possible action models is then $D\times 2^{5\times|{\mathcal I}_{\xi,\Psi}}|$ where D is the number of different possible values for the durations of any action shaped by the $\xi$ schema. This space is significantly larger than for learning STRIPS actions~\cite{yang2007learning} where this number is just $2^{2\times|{\mathcal I}_{\xi,\Psi}}|$ because negative effects must also be preconditions of the same action and cannot be positive effects of that action.

The conditions and effects of the schema of a {\em durative} action can be coded by 5 bit-vectors, each of length $|{\mathcal I}_{\xi,\Psi}|$. This means that the {\em Hamming distance} can be used straightforward as a similarity metric for {\em durative} schemes. For instance for comparing the error in a learned action model with respect to a given reference model that serves as baseline. Note also that the Machine Learning metrics {\em precision and recall} can also be used straightforward over the 5 bit-vectors that defines the conditions and effect of the action schema of a {\em durative action}.

\subsection{The sampling space}
\label{sec:sampling-space}
This paper studies the extreme scenario where just a single learning example is available (i.e. one-shot). In this work the single example is the noiseless but partial observation of the execution of a temporal plan in a given initial state.

This means that if the value of a fluent in $F$ is observed then its the actual value of the fluent but that not the value of all the fluents can be osbserved at any time. Just a subset of them can be observed because, for instance, there are associated sensors reporting their value. 



\section{Learning Concurrent Action Models with Constraint Programming}
This section formalizes the learning task we address in this paper and our CP formulation for addressing it with off-the-shelf CSP solvers.

\subsection{One-shot learning of temporal action models}
We define the task of the {\em one-shot learning of temporal action models} as the following tuple $\tup{F,I,G,A?,O,C}$, where:

\begin{itemize}
\item $\tup{F,I,G,A?}$ is a {\em temporal planning problem} in which actions in $A?$ are partially specified. They are partially specified because we do not know the exact structure in terms of distribution of conditions/effects nor the duration. In the worst case, we only know the vocabulary of the symbols that can appear in the {\em conditions} and/or {\em effects} of the actions but we can also assume that expert or prior knowledge is available bounding this vocabulary for a given action.
\item $O$ is the {\em observation} of a plan execution starting in the initial state $I$ and that contains the time when every action $a$ in $A?$ starts, i.e. all $\start(a)$ that have been observed (by a sensor or human observer). In addition it contains a full observation of the initial state (time-stamped with $t=0$) and a time stamped final observation that equals the goals of the {\em temporal planning problem} $\tup{F,I,G,A?}$.  
\item $C$ is a set of {\em state-constraints} that reflects domain-specific expert knowledge to complete the input observation and prune the space of possible  {\em durative} action models.  
\end{itemize}

A {\em solution} to this learning task is a fully specified model of temporal actions $\mathcal{A}$, where the duration and distribution of conditions/effects is completely specified and that is {\em consistent} with the given inputs $\tup{F,I,G,A?,O,C}$. In other words, for each action $a \in A?$, we have its equivalent version in $\mathcal{A}$ where we have learned $\dur(a)$, $\cond_s(a)$, $\cond_o(a)$, $\cond_e(a)$, $\eff_s(a)$ and $\eff_e(a)$. In addition, actions in $\mathcal{A}$ can produce a plan execution that starts in $i$, as observed in $O$, and that satisfies $G$. 

The {\em one-shot learning of temporal action models} is related to plan {\em synthesis} and plan {\em valifation}: In the case of plan sythesis the intermediate observations in $O$ are empty while in the case of plan validation the actions in $A?$ are fully specified.


\subsection{Constraint satisfaction for the learning of temporal action models}
\label{subsec:CPformulation}

Our approach is to create a CSP whose solution provide us an action model {\em consitent} with the inputs of the {\em one-shot learning of temporal action models} and s.t. it meets the causal structure of the plan with all possible supports; and avoid threats and possible contradictory effects. Our CP formulation is inspired by previous work on the synthesis of temporal plans with CSP~\cite{garrido2009constraint} and it is solver-independent. This means that any off-the-shelf CSP solver that supports the expressiveness of our CP formulation can be used.

\subsubsection{The CSP Variables}
For each action $a$ in $A?$, we create the seven kinds of variables specified in Table~\ref{table:variables}. Variables define the time-stamps for actions, the causal links, the interval when conditions must hold and the time when the effects happen. For simplicity, and to deal with integer variables, we model time in $\mathbb{Z}^+$. To prevent time from exceeding the plan horizon, we bound all times to the makespan of the plan.\footnote{We use the makespan, which can be observed, to restrict the duration of the actions. However, it is dispensable if we consider a long enough domain for durations}


\begin{table}
\begin{scriptsize}
\begin{tabular}{lll}
{\bf Variable} & {\bf Domain} & {\bf Description} \\

\hline

%\multicolumn{3}{l}{Vars. necessary for action times} \\

$\start(a)$ & \emph{known value} & start time of $a$ observed in $O$ \\
$\dur(a)$ & $[1..max(a)]$ & duration of $a$ where \\
&&$max(a)=makespan-\start(a)$\\
$\en(a)$ & \emph{derived value} & end time of $a$: $\en(a)=\start(a)+\dur(a)$ \\

%\multicolumn{3}{l}{Vars. necessary for action conditions+effects} \\

$\supp(p,a)$ & $\{b_i\}$ that supports $p$ & symbolic variable for the set of potential supporters\\
&& $b_i$ of condition $p$ of $a$ (causal link $\tup{b_i,p,a}$) \\

$\reqs(p,a)$, \\
$\reqe(p,a)$ & $[0..makespan]$ & interval $[\reqs(p,a)..\reqe(p,a)]$ at which \\
&&action $a$ requires $p$ \\

$\tim(p,a)$ & $[0..makespan]$ & time when effect $p$ of $a$ happens \\
\end{tabular}
\end{scriptsize}
\caption{\small The CSP variables and their domains.}
\label{table:variables}
\end{table}

This formulation allows us to model TILs and observations exactly like other actions. On the one hand, $\til(f,t)$ can be seen as a dummy action ($\start(\til(f,t))=t$ and $\dur(\til(f,t))=0$) with no conditions and only one effect $f$ that happens at time $t$ ($\tim(f,\til(f,t))=t$). A $\til$ is analogous to $\ini$, as they both represent information that is given at a particular time, but externally to the execution of the plan. On the other hand, $\obs(f,t)$ can be seen as another dummy action ($\start(\obs(f,t))=t$ and $\dur(\obs(f,t))=0$) with only one condition $f$, which is the value observed for fact $f$, and no effects at all. An $\obs$ is analogous to $\goal$, as they both represent conditions that must be satisfied in the execution of the plan at a particular time.

Additionally, we create two dummy actions $\ini$ and $\goal$ for each planning problem $\tup{F,I,G,A}$. First, $\ini$ represents the initial state $I$ ($\start(\ini)=0$ and $\dur(\ini)=0$). $\ini$ has no variables $\supp, \reqs$ and $\reqe$ because it has no conditions. $\ini$ has as many $\tim(p_i,\ini)=0$ as $p_i$ in $I$. Second, $\goal$ represents $G$ ($\start(\goal)=makespan$ and $\dur(\goal)=0$). $\goal$ has as many $\supp(p_i,\goal)$ and $\reqs(p_i,\goal)=\reqe(p_i,\goal)=makespan$ as $p_i$ in $G$. $\goal$ has no variables $\tim$ as it has no effects.


\subsubsection{The CSP Constraints}

Table~\ref{table:constraints} shows the constraints that we define among the variables of Table~\ref{table:variables}. The three first constraints are intuitive enough. The fourth constraint models {\em causal links}. Note that in a causal link $\tup{b_i,p,a}$, $\tim(p,b_i) < \reqs(p,a)$ and not $\leq$. This is because temporal planning assumes an $\epsilon > 0$ as a small tolerance between the time when an effect $p$ is supported and when it is required~\cite{fox2003pddl2}. Since we model time in $\mathbb{Z}^+$, $\epsilon=1$ and $\leq$ becomes $<$.

The fifth constraint avoids any threat via promotion or demotion~\cite{ghallab2004automated}. The sixth constraint models the fact the same action requires and deletes $p$. Note the $\geq$ inequality here; this is possible because if one condition and one effect of $a$ happen at the same time, the underlying semantics in planning considers the condition is checked instantly before the effect~\cite{fox2003pddl2}.

The seventh constraint solves the fact that two (possibly equal) actions have contradictory effects. It is important to note that these constraints involve any type of action, including $\ini$ and $\goal$.

\begin{table}
\begin{scriptsize}
\begin{tabular}{ll}
Constraint & Description \\

\hline

$\en(a)=\start(a)+\dur(a)$ & end time of $a$ \\

$\en(a) \leq \start(\goal)$ & $\goal$ is always the last action of the plan \\

%$\reqs(p,a) \leq \reqe(p,a)$ & [$\reqs(p,a)..\reqe(p,a)$] must be a valid interval \\
$\reqs(p,a) \leq \reqe(p,a)$ & interval [$\reqs(p,a)..\reqe(p,a)$] must be valid \\

\begin{tabular}{l}
if $\supp(p,a)=b_i$ then \\
\hspace{0.2cm}$\tim(p,b_i) < \reqs(p,a)$
\end{tabular}
& modeling causal link $\tup{b_i,p,a}$: the time when $b_i$ supports $p$ must be before $a$ requires $p$ \\

\begin{tabular}{l}
$\forall b_j \neq a$ that deletes $p$ at time $\tau_j$: \\
\hspace{0.2cm}if $\supp(p,a)=b_i$ then \\
\hspace{0.4cm}$\tau_j < \tim(p,b_i)$ OR \\
\hspace{0.4cm}$\tau_j > \reqe(p,a)$
\end{tabular}
& solving threat of $b_j$ to causal link $\tup{b_i,p,a}$ being $b_j \neq a$ (promotion OR demotion)\\

\begin{tabular}{l}
if $a$ requires and deletes $p$: \\
\hspace{0.2cm}$\tim(not-p,a) \geq \reqe(p,a)$ \\
\end{tabular}
& when $a$ requires and deletes $p$, the effect cannot happen before the condition \\

\begin{tabular}{l}
$\forall a_i,a_j \mid a_i$ supports $p$ and \\
\hspace{1.2cm}$a_j$ deletes $p$: \\
\hspace{0.2cm}$\tim(p,a_i) \neq \tim(not-p,a_j)$ \\
\end{tabular}
& solving effect interference ($p$ and $not-p$): they cannot happen at the same time \\

\end{tabular}
\end{scriptsize}
\caption{\small The CSP constraints.}
\label{table:constraints}
\end{table}



\subsection{Specific constraints for the PDDL2.1 action model}
\label{sec:PDDL21constraints}
This temporal model formulation is more expressive than PDDL2.1 (see more details in section~\ref{sec:PDDL21constraints}), and allows conditions and effects to be at any time, even outside the execution of the action. For instance, let us imagine a condition $p$ that only needs to be maintained for 5 time units before an action $a$ starts (e.g. warming-up a motor before driving): the expression $\reqe(p,a)=\start(a); \reqe(p,a) = \reqs(p,a)+5$ is possible in our formulation. Additionally, we can represent an effect $p$ that happens in the middle of action $a$: $\tim(p,a) = \start(a)+ (\dur(a) / 2)$ is also possible.

We present now our formulation for PDDL2.1. As section~\ref{sec:temporalplanning} explains, PDDL2.1 restricts the expressiveness of temporal planning in terms of conditions, effects, durations and structure of the actions. Hence, our temporal formulation subsumes and is significantly richer than PDDL2.1; but adding constraints to make it fully PDDL2.1-compliant is straightforward.

First, adding $((\reqs(p,a) = \start(a))$ OR ($\reqs(p,a) = \en(a)))$ AND $((\reqe(p,a) = \start(a))$ OR ($\reqe(p,a) = \en(a)))$ limits condition $p$ to be \emph{at start}, \emph{over all} or \emph{at end}, i.e. $p$ is in $\cond_s(a)$, $\cond_o(a)$ or $\cond_e(a)$, respectively.
Further, if a condition is never deleted in a plan, it can be considered as an invariant condition for such a plan. In other words, it represents static information. This type of condition is commonly used in planning to ease the grounding process from the operators; e.g. to model that there is a link between two locations and, consequently, a driving is possible, or modeling a petrol station that allows a refuel action in a given location, etc. Therefore, the constraint to be added for an invariant condition $p$ is simply: $((\reqs(p,a) = \start(a))$ AND $(\reqe(p,a) = \en(a)))$, i.e. $p \in \cond_o(a)$.
Surprisingly, invariant conditions are modeled differently depending on the human modeler. See, for instance, \texttt{(link ?from ?to)} of Fig.~\ref{fig:exampleactions2}, which is modeled as an \emph{at start} condition despite: i) the link should be necessary all over the driving, and ii) no action in this domain can be planned to delete that link.
This also happens in the \emph{transport} domain of the IPC, where a refuel action requires to have a petrol station in a location only \emph{at start}, rather than \emph{over all} which makes more sense. This shows that modeling a planning domain is not easy and it highly depends on the human's decision. On the contrary, our formulation checks the invariant conditions and deals with them always in the same coherent way.


Second, $((\tim(p,a) = \start(a))$ OR $(\tim(p,a) = \en(a)))$ makes an effect $p$ happen only \emph{at start} or \emph{at end} of action $a$, i.e. $p$ is in $\eff_s(a)$ or $\eff_e(a)$.
Also, if all effects happen \emph{at start} the duration of the action would be irrelevant and could exceed the plan makespan. To avoid this, for any action $a$, at least one of its effects should happen \emph{at end}: $\sum_{i=1}^{n =|\eff(a)|} \tim(p_i,a) > n \cdot \start(a)$, which guarantees $\eff_e(a)$ is not empty.

%\begin{displaymath}
%\sum_{i=1}^{n = \textrm{number of effects of }a} \tim(p_i,a) > n \cdot \start(a)
%\end{displaymath}


Third, durations in PDDL2.1 can be defined in two different ways. On the one hand, durations can be equal for all grounded actions of the same operator. For instance, any instantiation of \texttt{board-truck} of Fig.~\ref{fig:exampleactions2} will last 2 time units no matter its parameters. Although this may seem a bit odd, it is not an uncommon practice to simplify the model. The constraint to model this is: $\forall a_i,a_j$ being instances of the same operator: $\dur(a_i) = \dur(a_j)$.
On the other hand, although different instantiations of \texttt{drive-truck} will last different depending on the locations, different occurrences of the same instantiated action will last equal.
In a PDDL2.1 temporal plan, multiple occurrences of \texttt{drive-truck(truck1,loc1,loc2,driver1)} will have the same duration no matter when they start. Intuitively, they are different occurrences of the same action, but in the real-world the durations would differ from driving at night or in peak times. Since PDDL2.1 makes no distinction among different occurrences, the constraint to add is: $\forall a_i,a_j$ being occurrences of the same durative action: $\dur(a_i) = \dur(a_j)$.
Obviously, this second constraint is subsumed by the first one in the general case where all instances of the same operator have the same duration.

Fourth, the structure of conditions and effects for all grounded actions of the same operator is constant in PDDL2.1. This means that if \texttt{(empty ?t)} is an \emph{at start} condition of \texttt{board-truck}, it will be \emph{at start} in any of its grounded actions.
Let $\{p_i\}$ be the conditions of an operator and $\{a_j\}$ be the instances of a particular operator. The following constraints are necessary to guarantee a constant structure:

$\forall p_i: (\forall a_j: \reqs(p_i,a_j) = \start(a_j))$ OR $(\forall a_j: \reqs(p_i,a_j) = \en(a_j))$

$\forall p_i: (\forall a_j: \reqe(p_i,a_j) = \start(a_j))$ OR $(\forall a_j: \reqe(p_i,a_j) = \en(a_j))$

And analogously for all effects $\{p_i\}$ and the instances $\{a_j\}$ of an operator:

$\forall p_i: (\forall a_j: \tim(p_i,a_j) = \start(a_j))$ OR $(\forall a_j: \tim(p_i,a_j) = \en(a_j))$


As a conclusion, in our formulation each action of $A?$ is modeled separately so it does not need to share the same structure or duration of other actions. Moreover, the time-stamps for conditions/effects can be arbitrarily placed inside or outside the execution of the action, which allows for a flexible and expressive temporal model. But, when necessary, we can simply include additional constraints to restrict the expressiveness of the model, such as the ones provided by PDDL2.1.


\subsection{Example}
\label{sec:example}

We now show a fragment of the formulation for the example depicted in Fig.~\ref{fig:exampleplantrace}. For simplicity, we only show the variables and constraints for action $a3$, but the formulation is analogous for all other actions.

The variables and domains are: $\start(a3)=7$; $\dur(a3) \in [1..15]$; $\en(a3)=\start(a3)+\dur(a3)$; $\supp(r,a3) \in \{a1\}$; $\reqs(r,a3),\reqe(r,a3) \in [0..22]$; and $\tim(not-p,a3) \in [0..22]$.
On the other hand, the constraints are: $\en(a3) \leq \start(\goal)$; $\reqs(r,a3) \leq \reqe(r,a3)$; if $\supp(r,a3)=a1$ then $\tim(r,a1) < \reqs(r,a3)$; if $\supp(r,a3)=a1$ then $((\tim(not-r,a4) < \tim(r,a1))$ OR $(\tim(not-r,a4) > \reqe(r,a3)))$; $\tim(not-p,a3) \neq \tim(p,a1)$ and $\tim(not-p,a3) \neq \tim(p,a2)$.

There are many consistent solutions for this simple example, mainly because there is a huge range of possible durations that make the learned model consistent with the partially specified model $A?$.
Fig.~\ref{figure:solutionsExample} shows six arbitrary models as solutions. What is important to note is that the structure, i.e. distribution of conditions/effects, is similar in all the learned models. Actually, the distribution of the effects is identical (except for $q$ in model 2), and the distribution of conditions is very similar (e.g. $q$ is always in $\cond_o$ and $r$ in $a4$ is very often in $\cond_o$).
This shows that the one-shot learning returns not only consistent models but also similar, which is very positive.
The durations are, however, more different: $\dur(a1)$ ranges in these models from 7 to 19, whereas $\dur(a2)$ ranges from 5 to 18. As explained in section~\ref{sec:simpleTask}, learning the precise duration from just one sample may not be always possible, which is the main limitation of the one-shot learning task.
%Clearly, the duration learned for only one sample of \texttt{drive-truck(truck1,loc1,loc2,driver1)} cannot be always generalized to any other driving between these two locations.
In fact, the specific constraint of PDDL2.1, with regard to having multiple occurrences of the same action having the same duration, can significantly help us to learn the actions' duration in a more precise way as the learned duration must be consistent with all those occurrences.


\begin{figure}
\footnotesize
\begin{tabular}{l}
\hspace{-0.6cm}\begin{tabular}{lllllll}
Action & $\dur$ & $\cond_s$ & $\cond_o$ & $\cond_e$ & $\eff_s$ & $\eff_e$ \\

\hline

\multicolumn{7}{l}{Learned model 1} \\
$a1$ & 8 & & & & $r$ & $p$ \\
$a2$ & 18 & & & & $q$ & $p$ \\
$a3$ & 1 & & $r$ & & & $not-p$ \\
$a4$ & 1 & & $q,r$ & $p$ & & $not-r$ \\

\hline

\multicolumn{7}{l}{Learned model 2} \\
$a1$ & 19 & & & & $r$ & $p$ \\
$a2$ & 5 & & & & & $p,q$ \\
$a3$ & 1 & & & $r$ & & $not-p$ \\
$a4$ & 1 & $r$ & $p,q$ & & & $not-r$ \\

\hline

\multicolumn{7}{l}{Learned model 3} \\
$a1$ & 7 & & & & $r$ & $p$ \\
$a2$ & 18 & & & & $q$ & $p$ \\
$a3$ & 1 & & & $r$  & & $not-p$ \\
$a4$ & 1 & & $p,q,r$ & & & $not-r$ \\
\end{tabular} 



\end{tabular}
\normalsize

\caption{\small Three learned models for the example of Fig.~\ref{fig:exampleplantrace}.}
\label{figure:solutionsExample}
\end{figure}


%\subsection{Properties}

%Soundness of our formulation is guaranteed by the definition of the constraints of Table~\ref{table:constraints}, where all the branching alternatives to solve causal links, threats and effect interferences are supported. Completeness is guaranteed by the complete exploration of the domain of each variable of Table~\ref{table:variables} which can return many learned models in the form of consistent alternative solutions.



\subsection{Implementation. Use of Heuristics for Resolution}
\label{sec:implementation}

Our CSP formulation is automatically compiled from a partially specified action model, as defined in a classical planning problem, and the observations from a plan execution.
The formulation has been implemented in \textsf{Choco}\footnote{\texttt{http://www.choco-solver.org}}, an open-source Java library for constraint programming that provides an object-oriented API to state the constraints to be satisfied.

Our formulation is solver-independent, which means we do not use heuristics that may require changes in the implementation of the CSP engine.
Although this reduces the solver performance, we are interested in using it as a blackbox that can be easily changed with no modification in our formulation. However, we can easily encode standard static heuristics for variable and value selection that help improve efficiency by following the next ordering, which has shown very efficient in our experiments:

%%IMPORTANTE: ahorramos espacio juntando en la vi√±eta el orden de seleccion de variables y de valores

%From the variable perspective, we have detected it is better to instantiate variables that first lead to failure: effects, conditions, supporters and durations. This implies this ordering: $\tim$, $\reqs$, $\reqe$, $\supp$ and $\dur$.
%From the value perspective, we select first the values that lead to a more reasonable model of actions:

\begin{enumerate}
  \item Effects ($\tim$). For negative effects, first the lower value and for positive effects, first the upper value. This gives priority to delete effects as $\eff_s(a)$ and positive effects as $\eff_e(a)$.
  \item Conditions ($\reqs$ and $\reqe$). For $\reqs$, first the lower value, whereas for $\reqe$, first the upper value. This gives priority to $\cond_o(a)$, trying to keep the conditions as long as possible.
  \item Supporters ($\supp$). First the lower value, thus preferring the supporter that starts earlier in the plan.
  \item Duration ($\dur$). First the lower value, thus applying the principle of the shortest actions that make the learned model consistent.
\end{enumerate}


%This simple collection of heuristics is very intuitive and has been used in \textsf{Choco} by simply overriding the default search strategy for the variable and value selectors. Although these heuristics have shown very efficient in our experiments, they cannot always guarantee the best performance.


\subsection{Using the CP Formulation for Plan Validation}
\label{sec:usingCPValidation}

%As seen above, adding constraints allows us to restrict the temporal expressiveness of the learned model.
We explained that adding extra constraints allows us to restrict the temporal expressiveness of the learned model. We show here that we can also restrict the learned model by constraining the variables to known values, which is specially interesting when there is additional information on the temporal model that needs to be represented. For instance, based on past learned models, we may know the precise duration of an action $a$ is 6,
%or we can figure out that a condition is a start condition
or we can figure out that an effect $p$ always happens at end.
Our CP formulation can include this by simply adding $\dur(a)=6$ and $\tim(p,a)=\en(a)$, respectively, which is useful to enrich the partially specified actions in $A?$ of the learning task.

%The CP formulation is also useful to validate whether a given action model is consistent, as we will see in section~\ref{sec:evaluation}.
In particular, the possibility of adding those constraints is very appealing when used for validating whether a partial action model allows us to learn a consistent model, as we will see in section~\ref{sec:evaluation}.
Let us assume that the distribution of all (or just a few) conditions and/or effects is known and, in consequence, represented in the learning task. If a solution is found, then that structure of conditions/effects is consistent for the learned model. On the contrary, if no solution is found that structure is inconsistent and cannot be explained.
Analogously, we can represent known values for the durations. If a solution is found, the durations are consistent, and inconsistent otherwise.
Hence, we have three options for validating a partial model \emph{w.r.t.}: i) a known structure with the distribution of conditions/effects; ii) a known set of durations; and iii) a known structure plus a known set of durations (i+ii).
The first and second option allows for some flexibility in the learning task because some variables remain open. On the contrary, the third option checks whether a learned model can fit the given constraints, thus reproducing a strict plan validation task equivalent to~\cite{howey2004val}.



\section{Evaluation}
\label{sec:evaluation}

\subsection{Evaluation Metrics}


The empirical evaluation of a learning task can be addressed from two perspectives. From a pure syntactic perspective, learning can be considered as an automated design task to create a new model that is similar to a reference (or {\em ground truth}) model. Consequently, the success of learning is an accuracy measure of how similar these two models are, which usually counts the number of differences (in terms of incorrect durations or distribution of conditions/effects).
Unfortunately, there is not a unique reference model when learning temporal models at real-world problems.
Also, a pure syntax-based measure usually returns misleading and pessimistic results, as it may count as incorrect a different duration or a change in the distribution of conditions/effects that really represent equivalent reformulations of the reference model. For instance, given the example of Fig.~\ref{fig:exampleactions2}, the condition learned \texttt{(over all (link ?from ?to))} would be counted as a difference in action \texttt{drive-truck}, as it is \texttt{at start} in the reference model; but it is, semantically speaking, even more correct. Analogously, some durations may differ from the reference model but they should not be counted as incorrect. As seen in section~\ref{sec:simpleTask}, some learned durations cannot be granted, but the underlying model is still consistent. Therefore, performing a syntactic evaluation in learning is not always a good idea.


From a semantic perspective, learning can be considered as a classification task where we first learn a model from a training dataset, then tune the model on a validation test and, finally, asses the model on a test dataset. Our approach represents a one-shot learning task because we only use one plan sample to learn the model and no validation step is required.
Therefore, the success of the learned model can be assessed by analyzing the success ratio of the learned model \emph{vs.} all the unseen samples of a test dataset. In other words, we are interested in learning a model that fits as many samples of the test dataset as possible. This is the evaluation that we consider most valuable for learning, and define the success ratio as the percentage of samples of the test dataset that are consistent with the learned model. A higher ratio means that the learned model explains, or adequately fits, the observed constraints the test dataset imposes.

\subsection{Experimental Results}




\section{Conclussions}
\label{sec:conclusions}


We have presented a purely declarative CP formulation, which is independent of any CSP solver, to address the learning of temporal action models. Learning in planning is specially interesting to recognize past behavior in order to predict and anticipate actions to improve decisions.
The main contribution is a simple formulation that is automatically derived from the actions and observations on each plan execution, without the necessity of specific hand-coded domain knowledge. It is also flexible to support a very expressive temporal planning model, though it can be easily modified to be PDDL2.1-compliant.
Formal properties are inherited from the formulation itself and the CSP solver. The formulation is correct because the definition of constraints to solve causal links, threats and effect interferences are supported, which avoids contradictions. It is also complete because the solution needs to be consistent with all the imposed constraints, while a complete exploration of the domain of each variable returns all the possible learned models in the form of alternative consistent solutions.


Unlike other approaches that need to learn from datasets with many samples, we perform a one-shot learning. This reduces both the size of the required datasets and the computation time. The one-shot learned models are very good and explain a high number of samples in the datasets used for testing. Moreover, the same CP formulation is valid for learning and for validation, by simply adding constraints to the variables. This is an advantage, as the same formulation allows us to carry out different tasks: from entirely learning, partial learning/validation (structure and/or duration) to entirely plan validation.
According to our experiments, learning the structure of the actions in a one-shot way leads to representative enough models, but learning the precise durations is more difficult, and even impossible, when many actions are executed in parallel.


Finally,
%it is important to note that
our CP formulation can be represented and solved by Satisfiability Modulo Theories, which is part of our current work. As future work, we want to extend our formulation to learn meta-models, as combinations of many learned models, and a more complete action model.
In the latter, rather than using a partially specified set of actions, we want to find out the conditions/effects together with their distribution.
%We aim at finding out the conditions/effects together with their distribution, thus removing constraints from the partially specified set of actions in $A?$.
The underlying idea of finding an action model consistent with all the constraints will remain the same, but the model will need to be extended with additional decision variables and constraints. %$\mathsf{is\_condition(p,a)}$, $\mathsf{is\_effect(p,a)}$ to decide whether $p$ is a condition or effect of action $a$.
This will probably lead to the analysis of new heuristics for resolution.

\bibliographystyle{ecai}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
